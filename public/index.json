[{"content":"Introduction Large Transformer models achieve strong generalization and reasoning ability, but their huge size requires massive memory and computation. This makes them hard to train and deploy outside large data centers. To address this, researchers look for efficient architectures that save parameters or computation. Two main directions are parameter efficiency, which reduces or shares model weights, and adaptive computation, which uses more compute only when necessary.\nA common way to improve parameter efficiency is layer tying, where the same set of weights is reused across multiple layers. For adaptive computation, methods like early exiting let the model stop processing simple tokens earlier. However, most existing models treat these two goals separately. A unified design that combines both efficiency types has been missing. Recursive Transformers reuse one set of layers multiple times, naturally achieving weight sharing. Yet, previous recursive models often fix the number of recursions for all tokens. This wastes computation because every token receives the same processing depth, regardless of difficulty. Attempts at dynamic recursion have also faced training and efficiency challenges.\nTo solve these issues, the Mixture-of-Recursions (MoR) framework was proposed. It introduces a router that learns how many recursive steps each token needs. Easy tokens stop early, while complex ones go through more recursions. This provides true token-level adaptive depth. MoR also includes key–value (KV) caching, storing results from previous recursions to reduce memory use and improve throughput.\nOne Transformer layer = Self-Attention + Feed-Forward + Normalization + Residual connections.\nConceptually, MoR provides a model to “think” recursively inside its latent space during decoding of each token. It adjusts its reasoning depth per token instead of using a fixed number of layers. In this way, MoR unifies parameter efficiency and adaptive computation in one efficient Transformer architecture.\nThrough this design, MoR achieves three goals at once:\nWeight sharing reduces model parameters. Dynamic routing saves computation by skipping redundant steps. KV caching lowers memory traffic and speeds up inference. Methods Mixture-of-Recursions (MoR)—a framework that dynamically adjusts recursion step for each token during pretraining and inference.\nThe core of MoR lies in two components:\na routing mechanism that assigns token-specific recursion steps to adaptively concentrate computation on more challenging tokens; a KV caching strategy that defines how KV pairs are stored and selectively utilized for attention at each recursive step. 1. Routing Stategies: Expert-choice vs. Token-choice Expert-choice routing each recursion —select→ top-k tokens\nIn expert-choice routing, each recursion depth is treated as an expert. At every recursion step $r$, only the top-k tokens (with the highest scores) are selected to pass through that expert’s recursion block.\nThe router computes a scalar routing score for each token:\n$$ g_t^r = G(\\theta_r^\\top H_t^r) $$\nwhere $\\mathcal{G}$ is an activation function (e.g., sigmoid or tanh).\nThen, tokens with scores above the $\\beta$-percentile threshold $P_\\beta(G^r)$ are selected to proceed:\n$$ H_t^{r+1} = \\begin{cases} g_t^r f(H_t^r, \\Phi\u0026rsquo;) + Ht^r, \u0026amp; \\text{if } g_t^r \u0026gt; P_\\beta(G^r) \\\\ H_t^r, \u0026amp; otherwise \\end{cases} $$\n$r$: recursion steps $t$: token This selective routing makes each recursion act like a different “expert,” and tokens move deeper only when needed. A mechanism called hierarchical filtering ensures that tokens selected at one step can still be re-evaluated at later steps, supporting early-exit-like behavior while training from scratch.\nToken-choice routing each token —select→ recursion depth\nIn token-choice routing, each token independently chooses which recursion expert it wants to follow for all subsequent steps. Unlike expert-choice, routing happens once per token, not at every recursion step.\nGiven the hidden state $\\mathcal{H}_t^1$ at the first layer, the router computes routing scores for all experts $N$:\n$$ g_t^j = \\mathcal{G}(\\theta_r^\\top \\mathcal{H}_t^1), \\quad j \\in {1, \\dots, N_r} $$\nEach token selects its expert $i$ with the highest score ($i$ = the recursion depth of the token go through):\n$$ i = \\arg\\max_j g_t^j $$\nand applies that recursion block $i$ times.\nThe hidden state is updated recursively as:\n$$ H_t^{r+1} = \\begin{cases} g_t^r f(H_t^r, \\Phi\u0026rsquo;) + H_t^1, \u0026amp; \\text{if } r = i, last~recursion \\\\ g_t^r f(H_t^r, \\Phi\u0026rsquo;), \u0026amp; otherwise \\end{cases} $$\nThis approach avoids information leakage and makes each token commit to a fixed recursion path, though it may cause load imbalance among experts.\nFor Expert-choice:\nHierarchical filtering keeps recursion steps causally ordered in data flow — each step selects tokens only from the previous one.\nHowever, during parallelized training, all recursion steps are computed at once, so gradients from deeper steps can flow backward and influence earlier routers.\nThis introduces training-time causality violation — even though the hierarchical filtering itself maintains structural causality.\n2. KV Caching Strategies: Recursion-wise Caching vs. Recursive sharing Dynamic-depth models face challenges with KV cache consistency during autoregressive decoding. When a token exits early, its keys and values from deeper recursion steps are missing, which causes incomplete context for later tokens. Previous methods tried to reuse or recompute these entries but introduced extra complexity. To address this, MoR proposes two efficient strategies: recursion-wise KV caching and recursive KV sharing.\nIn recursion-wise KV caching, only the tokens selected for a specific recursion step store their key–value pairs at that level. The cache size at each depth depends on how many tokens are routed there. Attention computation is restricted to these locally cached tokens. This makes computation more localized, saving memory and reducing input/output operations.\nIn recursive KV sharing, all tokens share the KV pairs produced at the first recursion block. These cached pairs are reused by all later recursion steps, so each step still has access to the full sequence context. Even though fewer tokens may continue deeper, their keys and values still represent the whole sequence, preventing missing-context problems.\nRecursion-wise caching reduces KV memory and IO usage roughly by a factor of $(N_r+1)/(2N_r)$ across the model and decreases attention FLOPs, making both training and inference more efficient. Recursive sharing saves even more memory by globally reusing context, though it provides less FLOP reduction and still faces IO bottlenecks during decoding. Overall, both strategies improve efficiency but trade off between local memory savings (recursion-wise) and global context reuse (recursive sharing).\nMore Details Parameter-sharing strategies in Recursive Transformers.\nSo an equation such as\n$$ f(h_t^{\\ell}; \\Phi\u0026rsquo;_{\\ell \\bmod (L/N_r)}) $$\nmeans:\ntake the token representation $h_t^{\\ell}$, apply a layer function $f(\\cdot)$ using parameters $\\Phi\u0026rsquo;$, and reuse weights cyclically according to the current recursion index $\\ell$. Results MoR outperforms baselines with fewer parameters under equal train compute. MoR outperforms baselines with less compute at equal data. MoR performance varies with routing and caching strategies. Conclusion Mixture-of-Recursions (MoR) is a Transformer model that combines parameter sharing, adaptive recursion depth, and efficient KV caching. It uses routers to decide how many recursive steps each token needs and stores KV pairs only for active tokens. This design cuts unnecessary computation and memory use while keeping strong performance. Experiments show that MoR achieves lower perplexity, higher few-shot accuracy, and faster inference than standard Transformers or earlier recursive models.\nFuture Work Future work will focus on improving reasoning ability. Since MoR already adapts depth per token, it can be trained to adjust recursion depth based on reasoning difficulty, helping the model handle chain-of-thought tasks better.\nMoR can also be scaled to larger models. Future versions will train models with over 3B parameters and may use depth-specific LoRA, expert modules, or expert parallelism to improve performance without slowing inference. Reusing pre-trained LLMs could further reduce training cost.\nAnother goal is to make MoR more flexible during inference. The current router outputs are too sharp, making it hard to change capacity or top-k values after training. New routing methods are needed for dynamic control.\nMoR can also benefit from sparsity methods like pruning and quantization to skip unnecessary computation, improving efficiency further.\nFinally, MoR’s adaptive recursion is not limited to text. It can be extended to vision, speech, and multimodal models. Adjusting depth for different tokens or segments could make processing long videos or audio more efficient in both memory and speed.\n","permalink":"/notes/mixture-of-recursions_learning_dynamic_recursive_d/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eLarge Transformer models achieve strong generalization and reasoning ability, but their huge size requires massive memory and computation. This makes them hard to train and deploy outside large data centers. To address this, researchers look for \u003cstrong\u003eefficient architectures\u003c/strong\u003e that save parameters or computation. Two main directions are \u003cstrong\u003eparameter efficiency\u003c/strong\u003e, which reduces or shares model weights, and \u003cstrong\u003eadaptive computation\u003c/strong\u003e, which uses more compute only when necessary.\u003c/p\u003e\n\u003cp\u003eA common way to improve parameter efficiency is \u003cstrong\u003elayer tying\u003c/strong\u003e, where the same set of weights is reused across multiple layers. For adaptive computation, methods like \u003cstrong\u003eearly exiting\u003c/strong\u003e let the model stop processing simple tokens earlier. However, most existing models treat these two goals separately. A unified design that combines both efficiency types has been missing. \u003cstrong\u003eRecursive Transformers\u003c/strong\u003e reuse \u003cstrong\u003eone set of layers\u003c/strong\u003e multiple times, naturally achieving weight sharing. Yet, previous recursive models often fix the number of recursions for all tokens. This wastes computation because every token receives the same processing depth, regardless of difficulty. Attempts at dynamic recursion have also faced training and efficiency challenges.\u003c/p\u003e","title":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"},{"content":"Source code: https://github.com/varun-jois/FSRST\nIntroduction Face super-resolution aims to reconstruct high-resolution (HR) facial images from low-resolution (LR) inputs, enhancing fine details. This task is challenging because it is ill-posed: many possible HR outputs can correspond to the same LR image.\nTo reduce ambiguity, Reference-Based Super-Resolution (RefSR) introduces external HR reference images that share similar content (e.g., same person’s other photos). The model can then use these reference textures and shapes to guide reconstruction.\nHowever, RefSR introduces two main challenges:\nAlignment problem – matching facial structures between LR input and HR reference images. Information aggregation problem – determining how much and which parts of each reference to use. Traditional alignment methods (like deformable convolutions) are powerful but unstable and hard to train. To overcome this, the authors propose FSRST, which uses a Spatial Transformer Network (STN) for stable alignment and a distance-based weighted aggregation for effective information fusion.\nMethod The proposed Face Super-Resolution using Spatial Transformer (FSRST) is an end-to-end model with four components:\nFeature Extractor:\nExtracts features from both the LR input and HR references using residual blocks. Reference images are converted to grayscale and reshaped (space-to-depth) to match the LR resolution.\nSpatial Transformer Alignment (STA):\nReplaces unstable deformable convolutions with a Spatial Transformer module. It predicts an affine transformation that aligns each reference’s features with those of the LR image. This alignment is differentiable and stable, ensuring good correspondence between LR and HR feature spaces.\nDistance-Based Weighted Aggregation (DWA):\nAfter alignment, the model computes the L2-distance between each aligned reference feature and the LR feature. A softmax weighting gives higher importance to more similar references, while irrelevant ones are ignored. This allows the model to dynamically use helpful references or fall back to single-image SR when references are poor.\nOutput Constructor:\nCombines aggregated and LR features and passes them through multiple residual blocks and sub-pixel convolution for upsampling. The model predicts a residual image, added to a bicubic-upsampled LR image to produce the final HR output.\nConclusion The FSRST model introduces a stable and efficient alternative to deformable alignment for reference-based face super-resolution.\nIts Spatial Transformer alignment provides consistent and accurate correspondence, while the distance-based aggregation flexibly handles multiple references.\nExperiments on DFD, CelebAMask-HQ, and VoxCeleb2 datasets show that FSRST outperforms previous methods like TTSR, C2-Matching, MRefSR, and HIME — achieving higher PSNR/SSIM with fewer parameters.\nAlthough the STN-based alignment is not fully convolutional (fixed input size), the method is lightweight, effective, and adaptable for real-time applications such as video conferencing.\nFuture work aims to extend this model to video super-resolution and make the alignment module fully convolutional.\n","permalink":"/notes/reference-based_face_super-resolution_using_the_sp/","summary":"\u003cp\u003eSource code: \u003ca href=\"https://github.com/varun-jois/FSRST\"\u003ehttps://github.com/varun-jois/FSRST\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eFace super-resolution aims to reconstruct high-resolution (HR) facial images from low-resolution (LR) inputs, enhancing \u003cstrong\u003efine details\u003c/strong\u003e. This task is challenging because it is \u003cstrong\u003eill-posed\u003c/strong\u003e: many possible HR outputs can correspond to the same LR image.\u003c/p\u003e\n\u003cp\u003eTo reduce ambiguity, \u003cstrong\u003eReference-Based Super-Resolution (RefSR)\u003c/strong\u003e introduces external HR reference images that share similar content (e.g., same person’s other photos). The model can then use these reference textures and shapes to guide reconstruction.\u003c/p\u003e","title":"Reference-Based Face Super-Resolution Using the Spatial Transformer"},{"content":"Introduction Single image super-resolution (SISR) aims to restore a low-resolution (LR) image into a high-resolution (HR) image with realistic textures. With the growth of deep learning, SISR performance has greatly improved in recent years.\nCompared to SISR, reference-based super-resolution (RefSR) makes use of additional high-resolution reference images that share similar textures with the target image. This allows RefSR to generate more detailed and realistic results. Because of these promising results, many researchers have focused on RefSR methods in recent years.\nHowever, all previous RefSR methods are trained with only a single reference image. In practice, there are often multiple reference images available, such as in the CUFED5 dataset, where each LR image has five reference images of varying similarity. Yet, CUFED5’s training set provides only one reference per LR input and contains relatively few and small images. As a result, previous methods cannot effectively use multiple references. To handle multiple references, they often merge them into one large image, which consumes much GPU memory and ignores the relationships between references.\nTherefore, a new dataset and method are needed for multi-reference super-resolution. To address this gap, the authors introduce a large-scale multi-reference dataset named LMR, containing 112,142 groups of 300×300 training images, each with five reference images. This dataset is ten times larger than CUFED5 and supports better generalization.\nBased on LMR, the authors propose a new method called MRefSR. It introduces two key components:\nMulti-Reference Attention Module (MAM) – fuses features from multiple reference images by treating the LR input as the query and aligned reference features as keys and values. Spatial Aware Filtering Module (SAFM) – selects the most relevant fused features to refine the output. Overall, the work contributes (1) the first large-scale multi-reference RefSR dataset, (2) a new baseline method MRefSR designed for multiple references, and (3) experimental results showing strong improvements over existing methods.\nMethod Dataset: Large-scale Multi-reference RefSR dataset LMR\nMethod: MRefSR\nConstruction of LMR The LMR dataset is built based on the MegaDepth dataset, which was originally created for single-view depth prediction. MegaDepth collected over one million Internet photos of landmarks and used COLMAP, a Structure-from-Motion (SfM) and Multi-View Stereo (MVS) system, to reconstruct 3D models and dense depth maps. Each landmark includes many photos taken from different viewpoints, making it suitable for creating image groups with overlapping content, just like reference-based super-resolution (RefSR) requires.\nTo construct the LMR dataset, the authors first preprocessed MegaDepth to form image pairs with controlled similarity. They used three filtering rules:\nThe PSNR between the target and candidate reference images must be lower than 30 dB to remove duplicates. The two images must share similar content, ensured by checking the overlap ratio (Rolp) of matched 3D keypoints. The size ratio (Rs) of the same object in both images must not be too small, so the reference provides enough detailed texture. These ratios were computed using the existing D2-Net code. Based on these measures, each image pair was labeled with a similarity level:\nHigh (H) if Rolp \u0026gt; 30% and Rs \u0026gt; 0.9 Medium (M) if Rolp \u0026gt; 10% and Rs \u0026gt; 0.66 Low (L) otherwise After filtering, the authors obtained large image groups — each with one target image and several reference images. Because training on full images is memory-intensive, they cropped smaller patches. For each group, they first randomly cropped a 300×300 patch from the target image. Then, using 3D keypoints, they located five nearby reference patches from images of different similarity levels (one H, two M, two L).\nFinally, this process produced 112,142 training groups, each containing one target patch and five reference patches. This dataset is about ten times larger than CUFED5 and offers much richer multi-reference diversity. For testing, the authors built another set of 142 groups, each with a target image and 2–6 reference images, with resolutions between 800 and 1600 pixels.\nMulti-Reference RefSR network The authors propose a multi-reference RefSR network, called MRefSR, to effectively use multiple reference images. The model is based on C2-Matching, which provides strong performance and open-source accessibility. Like C2-Matching, a Content Extractor (CE) extracts features $F_{LR}$ from the low-resolution $LR$ image, while a VGG extractor extracts multi-scale features $F_{Ref_i}$ from each reference image. The model also uses a pretrained Contrastive Correspondence Network (CCN) to estimate offsets $O_i$, aligning each reference image with the LR input.\nAfter feature extraction and alignment, the network includes two new modules: the Multi-Reference Attention Module (MAM) and the Spatial Aware Filtering Module (SAFM).\nThe MAM fuses features from multiple reference images. For each spatial location $(x, y)$, attention maps are generated to measure how similar each aligned reference feature $K_i(x, y)$ is to the LR feature $Q(x, y)$. The attention weights are computed with a softmax function, and all aligned reference features are combined into a fused reference feature $F_{fref}$ using a weighted sum. This allows the model to flexibly handle any number of reference images in both training and testing. Next, since not all fused features are reliable, the SAFM selects and refines them. It takes the concatenated features of $F_{LR}$ and $F_{fref}$ as input and generates two masks: a multiplicative mask $M_{mul}$ and an additive mask $M_{add}$. These masks are produced using convolution and Leaky ReLU layers, and $M_{mul}$ is passed through a sigmoid function to keep its values between 0 and 2. The final selected reference feature $F_{sref}$ is obtained by combining the fused feature with these masks. $$ M_{mul} = \\text{sigmoid}(f_1(F_{LR} || F_{fref})) \\cdot 2 $$\n$$ M_{add} = f_2(F_{LR}||F_{fref}) $$\n$$ F_{sref} = F_{fref} \\odot M_{mul} + M_{add} $$\n$$ X_{SR} = \\mathcal{G}(F_{LR}, F_{sref}) $$\nwhere\n$||$ denotes feature concatenation, $\\odot$ denotes element-wise multiplication, $f_1$ and $f_2$ are nonlinear mapping functions (convolutions + LeakyReLU), and $\\mathcal{G}$ is the restoration module that reconstructs the final super-resolved image. Finally, a restoration module $G$ takes both the LR features $F_{LR}$ and the selected reference features $F_{sref}$ to reconstruct the high-resolution output image $X_{SR}$.\nIn summary, MRefSR extends C2-Matching by adding multi-reference attention fusion and spatial-aware filtering, enabling flexible and effective use of multiple reference images for super-resolution.\nConclusion In this paper, the author proposed a large-scale multi-reference RefSR dataset: LMR. Unlike CUFED5, the only training RefSR dataset available before, LMR has 5 reference images for each LR input image. What’s more, LMR contains 112,142 groups of 300×300 training images, 10 times the number of CUFED5, and the image size is also much larger than CUFED5.\nBesides, the author proposed a new multi-reference baseline RefSR method, named MRefSR. A multi- reference attention module (MAM) for feature fusion of an arbitrary number of reference images, and a spatial aware filtering module (SAFM) for the fused feature selection. With LMR enabling multi-reference RefSR training, the method effectively models the relationship among multiple references, thus achieving significant improvements over SOTA approaches on both quantitative and qual- itative evaluations. And the method solves the mismatch problem of previous methods using a single reference image for training but testing with multiple reference images.\n","permalink":"/notes/lmr_a_large-scale_multi-reference_dataset_for_refe/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eSingle image super-resolution\u003c/strong\u003e \u003cstrong\u003e(SISR)\u003c/strong\u003e aims to restore a low-resolution (LR) image into a high-resolution (HR) image with realistic textures. With the growth of deep learning, SISR performance has greatly improved in recent years.\u003c/p\u003e\n\u003cp\u003eCompared to SISR, \u003cstrong\u003ereference-based super-resolution (RefSR)\u003c/strong\u003e makes use of additional high-resolution reference images that share similar textures with the target image. This allows RefSR to generate more detailed and realistic results. Because of these promising results, many researchers have focused on RefSR methods in recent years.\u003c/p\u003e","title":"LMR: A Large-Scale Multi-Reference Dataset for Reference-based Super-Resolution"},{"content":"Introduction Image synthesis has advanced rapidly, with diffusion models now leading in generating complex, high-resolution images. Unlike GANs, which struggle with stability and mode collapse, and autoregressive transformers, which require billions of parameters, diffusion models achieve strong results in class-conditional generation, super-resolution, and inpainting using fewer parameters and more stable training.\nDespite their success, diffusion models are still computationally expensive. Because they model every pixel, including imperceptible details, both training and inference require enormous GPU time and memory. This makes them less accessible and environmentally costly.\nPerceptual compression removes small, imperceptible pixel details but keeps the overall visual appearance. It focuses on what humans can see clearly, not on exact pixel accuracy. Autoencoders or GANs often perform this kind of compression. Semantic compression goes further. It removes even visible low-level details but preserves the meaning or concept of the image. For example, the exact texture of a face may change, but the idea of “a person wearing glasses” remains. Latent diffusion models operate in this stage, learning high-level structure and meaning instead of pixel noise. To solve this, the authors propose training diffusion models in a latent space instead of pixel space. The idea is to first use an autoencoder to compress images into a smaller, perceptually equivalent representation, and then train the diffusion model on this compact latent data. This reduces computation while keeping visual quality.\nThe resulting method, called Latent Diffusion Model (LDM), combines an autoencoder and a diffusion U-Net, and can include transformer-based conditioning for tasks like text-to-image generation. This design makes high-resolution synthesis more efficient and scalable.\nIn summary, LDMs (1) scale better to large data, (2) significantly reduce training and inference cost, (3) produce faithful, detailed reconstructions, and (4) support versatile conditioning for multi-modal tasks such as text- or layout-based image generation.\nMethod The authors propose separating compression and generation into two stages. Instead of training directly in pixel space, they use an autoencoder to learn a latent space that keeps perceptual quality but greatly lowers computational complexity. The U-Net architecture further helps capture spatial structure, so there is no need for strong compression that harms image quality. In addition, the learned latent space can serve as a general-purpose representation for training other generative models or for tasks such as CLIP-guided image synthesis.\nPerceptual Image Compression The perceptual image compression model is an autoencoder trained with both a perceptual loss and a local patch-based adversarial loss to produce realistic, non-blurry reconstructions. The encoder compresses an image $x$ into a latent representation $z = ε(x)$, and the decoder reconstructs it as $\\tilde{x} = D(z)$. The image is downsampled by a factor $f$.\nTo keep the latent space stable, two types of regularization are used: a KL penalty (as in VAEs) or vector quantization (as in VQGANs). Unlike earlier methods that flattened the latent space into one dimension, this model keeps the 2D spatial structure of the latent representation, allowing for milder compression and higher reconstruction quality while preserving fine image details.\nLatent Diffusion Models Latent Diffusion Models (LDMs) are based on diffusion models, which learn to generate data by gradually denoising random noise through a reverse Markov process. In this framework, each step predicts a cleaner version of a noisy input, trained with a simple mean-squared error objective. By applying diffusion in the latent space learned from the perceptual autoencoder, the model focuses on meaningful, semantic image features instead of pixel-level noise. Unlike previous transformer-based methods that used discrete tokens, LDMs use a U-Net built from 2D convolutions to better capture spatial image structure. During generation, latent samples are denoised step by step and then decoded into final images with a single pass through the decoder.\nA reverse Markov process is the step-by-step denoising process that diffusion models learn — it reverses the fixed forward noising process to transform pure noise back into a clean, generated image.\nConditioning Mechanisms Conditioning mechanisms allow diffusion models to generate images based on extra input information $y$, such as text, semantic maps, or other images. To achieve this, the model learns a conditional distribution $p(z|y)$ by adding the condition $y$ to the denoising network. The authors enhance the U-Net backbone of the LDM with a cross-attention mechanism, which lets the network focus on relevant parts of $y$ while generating the image. A separate encoder $τ_θ$ converts the condition into an embedding that interacts with the U-Net through attention layers. This setup allows flexible conditioning from different modalities, meaning the same diffusion model can handle tasks like text-to-image, layout-to-image, or other image-to-image synthesis efficiently.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) \\cdot V $$\n$$ Q = W_Q^{(i)} \\cdot \\varphi_i(z_t), \\quad K = W_K^{(i)} \\cdot \\tau_\\theta(y), \\quad V = W_V^{(i)} \\cdot \\tau_\\theta(y) $$\n$K$ and $V$ come from the conditioning input (e.g., text embeddings). $Q$ comes from the U-Net feature map at the current diffusion step. Use in super-resolution Latent Diffusion Models (LDMs) can perform super-resolution efficiently by conditioning on low-resolution images. The method simply concatenates the LR input with the U-Net input, allowing the model to learn how to reconstruct high-frequency details.\nUsing a pretrained autoencoder with downsampling factor (f=4), the model (LDM-SR) is trained on ImageNet with bicubic 4× downsampling, following SR3’s setup. A user study confirms that LDM-SR produces more visually pleasing results. To further enhance detail, a perceptual loss is added as a guiding mechanism. Finally, since bicubic degradation limits generalization, a more robust version called LDM-BSR is trained using diverse degradations to handle real-world low-quality inputs.\nConclusion Latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and cross-attention conditioning mechanism, the experiments could demonstrate favorable results compared to SOTA methods across a wide range of conditional image synthesis tasks without task-specific architectures.\n","permalink":"/notes/high-resolution_image_synthesis_with_latent_diffus/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eImage synthesis has advanced rapidly, with diffusion models now leading in generating complex, high-resolution images. Unlike GANs, which struggle with stability and mode collapse, and autoregressive transformers, which require billions of parameters, diffusion models achieve strong results in class-conditional generation, super-resolution, and inpainting using f\u003cstrong\u003eewer parameters and more stable training\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eDespite their success, diffusion models are still \u003cstrong\u003ecomputationally expensive\u003c/strong\u003e. Because they model every pixel, including \u003cstrong\u003eimperceptible details\u003c/strong\u003e, both training and inference require enormous GPU time and memory. This makes them less accessible and environmentally costly.\u003c/p\u003e","title":"Latent Diffusion Models"},{"content":"Reinforcement Learning\nIntroduction Recent LLMs are rapidly advancing toward AGI. Post-training has emerged as an important component of the full training pipeline, which enhances accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI’s o1 series models were the first to introduce inference-time scaling by increasing the length of the CoT reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning.\nHowever, the challenge of effective test-time scaling (efficient reasoning at inference) remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models, RL, and search algorithms such as MCTS and Beam Search. However, none of these methods has achieved general reasoning performance comparable to OpenAI’s o1 series models.\nThis paper proposes a RL-only-based approach DeepSeek-R1-Zero which directly applied RL to the base model without relying on supervised fine-tuing (SFT) as a preliminary step. The network use DeepSeek-V3-Base as the foundation and apply GRPO (a reinforcement learning framework). After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks, showing strong reasoning performance, and matching OpenAI o1-0912.\nHowever, DeepSeek-R1-Zero suffers from poor readability and language mixing. To address these issues and further enhance reasoning performance, the DeepSeek introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline.\nDeepSeek-R1-Zero → trained only with RL, no SFT (“pure RL from base model”). DeepSeek-R1 → adds extra cold-start SFT + synthetic data generation + another RL phase. Training pipeline:\nCollect thousands of cold-start data to fine-tune DeepSeek-V3-Base model. Perform reasoning-oriented RL like DeepSeek-R1-Zero. Near convergence in the RL process, Create new SFT(supervised fine-tuning) data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, the obtained checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\nFinally, they perform distillation of DeepSeek-R1 into smaller models (based on Qwen2.5-32B). Even after removing RL, these distilled models retain reasoning skills, showing that large-model reasoning discoveries can be transferred. Notably, the 14B distilled model beats all open-source baselines on reasoning benchmarks.\nContributions Post-Training: Large-Scale Reinforcement Learning on the Base Model\nDeepSeek-R1-Zero: RL applied directly to base model without SFT → achieves self-verification, reflection, long CoT reasoning. DeepSeek-R1: Pipeline with 2 RL + 2 SFT stages → improves reasoning, alignment, and non-reasoning abilities. Distillation: Smaller Models Can Be Powerful Too\nReasoning patterns learned by large models can be distilled into smaller ones. The open-source DeepSeek-R1-Distill family (1.5B – 70B parameters) performs exceptionally well, matching or surpassing strong baselines like OpenAI o1-mini. DeepSeek-R1-Distill-Qwen-7B: 55.5% AIME 2024. DeepSeek-R1-Distill-Qwen-32B: 72.6% AIME 2024, 94.3% MATH-500, 57.2% LiveCodeBench. Summary of Evaluation Results Reasoning Tasks:\nDeepSeek-R1 reaches 79.8 % Pass@1 on AIME 2024 (≈ OpenAI o1-1217). On MATH-500 → 97.3 %, Codeforces → 2 029 Elo (\u0026gt; 96 % human). Performs slightly below DeepSeek-V3 in engineering tasks. Knowledge Tasks:\nOn MMLU (90.8 %), MMLU-Pro (84 %), GPQA Diamond (71.5 %), DeepSeek-R1 beats DeepSeek-V3 and is close to OpenAI o1-1217.\nOther Abilities:\nExcels in writing, summarization, code generation, and instruction following. Achieves 87.6 % length-controlled win-rate (AlpacaEval 2.0) and 92.3 % win-rate (ArenaHard). Especially strong on long-context understanding and non-exam queries. Approach Previous work has heavily relied on large amounts of supervised data to enhance model performance. The DeepSeek team demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. The following sections introduce: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models.\nDeepSeek-R1-Zero DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\nProximal Policy Optimization (PPO) is the standard RL method used in LLM fine-tuning (e.g., RLHF). It involves three models: a policy model that generates responses, a reward(value) model that scores them, and a critic model that estimates a baseline (the expected reward) to compare with the reward. The policy and critic are trained together, using the difference between the actual reward and the baseline to encourage better-than-average outputs while maintaining training stability. But the reward model is fixed which only provides the scores. However, PPO requires a large critic network (often the same size as the policy model), which doubles computational cost.\nTo address this, Group Relative Policy Optimization (GRPO) simplifies the process by removing the critic and computing the baseline directly from the average reward of a group of sampled responses, making reinforcement learning more efficient for large LLMs like DeepSeek-R1-Zero.\nReinforcement Learning Algorithm Goal: Train the policy model $\\pi_\\theta$ to generate above-average answers while keeping training stable and close to a reference model.\nObjective Function:\n$$ J_{GRPO}(\\theta) = E[{q \\sim P(Q), \\{o_i\\}^G_{i=1} \\sim \\pi_{\\theta_{\\text{old}}}(O|q)}] \\\\ \\frac{1}{G} \\sum_{i=1}^{G} \\Big( \\min\\Big( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip}\\Big(\\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1-\\varepsilon, 1+\\varepsilon\\Big)A_i \\Big) - \\beta D_{KL}(\\pi_\\theta||\\pi_{\\text{ref}}) \\Big) $$ $q∼P(Q)$: sample a question/prompt from the dataset. $ {o_i}^G_{i=1} \\sim \\pi_{\\theta_{\\text{old}}}(O|q)$: use the old model to generate G different answers to the same question. $\\frac{1}{G} \\sum_{i=1}^{G}$: averages the result over the G samples (the group). $\\frac{\\pi_\\theta}{\\pi_{\\theta_{old}}}$: probability ratio showing how the model’s belief changes. $A_i$: advantage, computed within the group, measures how much better each answer is than group average ($baseline=mean(r_1​,r_2​,…,r_G​)$): $$ A_i = \\frac{r_i - {mean}(r_1,\\dots,r_G)}{{std}(r_1,\\dots,r_G)} $$\n$r_i$: reward for response $o_i$. clip(·): limits updates to ensure stability (same as PPO). $D_{KL}(\\pi_\\theta||\\pi_{ref})$: KL penalty keeping the model close to a reference policy (e.g., base model). $$ D_{KL}(\\pi_\\theta || \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o_i|q)}{\\pi_\\theta(o_i|q)} -\\log\\frac{\\pi_{\\text{ref}}(o_i|q)}{\\pi_\\theta(o_i|q)} - 1 $$\n$\\varepsilon, \\beta$: hyperparameters controlling clipping and KL weight. Where do the rewards $r_i$ come from? → From Reward Modeling, which defines how to score each generated answer.\n$$ \\boxed{\\text{Total computations} \\propto O \\times G} $$\n$O$ = number of prompts/questions sampled in a batch $G$ = number of responses per prompt Reward Modeling To train DeepSeek-R1-Zero, a rule-based reward system was adopted that mainly consists of two types of rewards:\n$$ r_i=r_i(accuracy)+r_i(format) $$\nAccuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. If the answer matches the correct one, reward = 1; else 0 (or scaled). Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‘’ and ‘’ tags. Reward = 1 if the output format is correct, else 0. Training Template To train DeepSeek-R1-Zero, begining by designing a straightforward template that guides the base model to follow to the specified instructions. This following template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. prompt will be replaced with the specific reasoning question during training. Limit the constraints to this structural format, avoiding any content-specific biases to ensure that the model’s natural progression can be observed accurately during the RL process.\nAlthough DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek R1-Zero struggles with challenges like poor readability, and language mixing(the base model DeepSeek-V3 is multilingual, and RL doesn’t penalize mixing languages). To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data.\nGood readability (_after_SFT):\nTo compute the sum of the first 5 even numbers: 2 + 4 + 6 + 8 + 10 = 30. 30 Poor readability (DeepSeek-R1-Zero):\nsum=2+4+6+8+10=\u0026gt;=30?? yes right 30 correct result final output=30 30\nDeepSeek-R1 DeepSeek-R1: Reinforcement Learning with Cold Start\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows.\nCold Start Construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. The data can include reasoning examples in the prompt (few-shot) or detailed reasoning in the response, ensuring the model learns to generate readable, step-by-step solutions. To collect such data, there are several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, they collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data are readability of responses and better performance by designing a readable pattern that includes a summary at the end of each response.\nReasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on cold-start data, the same GRPO-based RL process as DeepSeek-R1-Zero is applied to enhancing the model’s reasoning capabilities in math, logic, science, and coding tasks. However, RL caused language mixing in Chain-of-Thought reasoning, so they introduced a language-consistency reward, computed as the ratio of target-language words in the reasoning text. The final reward combines accuracy and language consistency: $r_{final} = r_{accuracy} + r_{lang}$Although this slightly reduces task performance, it produces more readable, human-preferred outputs. The model is trained until convergence, resulting in the final DeepSeek-R1 model.\nRejection Sampling and Supervised Fine-Tuning After reasoning-oriented RL training is finished, they use the RL-trained checkpoint to generate new supervised fine-tuning (SFT) data for the next round, aiming to improve model’s capabilities in writing, role-playing, and other general-purpose tasks. They produce two types of data: For reasoning data, they sample many responses (like 10–20 per question) from the RL checkpoint. then they perform rejection sampling. For each prompt, they sample multiple responses and retain only the correct ones. For non-reasoning data, they add general-purpose data reuse or regenerate these using DeepSeek-V3’s SFT data.\nReinforcement Learning for all Scenarios To further align the model with human preferences, they implement a secondary reinforcement learning stage aimed at improving the model’s helpfulness and harmlessness while simultaneously refining its reasoning capabilities.\nFor reasoning data, they follow to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process. For general data, they resort to reward models to capture human preferences $r_i^{(preference)}∈[0,1]$. They build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, they focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, they evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\nDistillation: Empower Small Models with Reasoning Capability For distilled models, the team apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance.\nConclusion This paper shares how to enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achievesperformance comparable to OpenAI-o1-1217 on a range of tasks. They further explore distillation the reasoning capability to small dense models. They use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints.\nIn the future, there are some research directions across the following directions for DeepSeek-R1.\nGeneral Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results. Software Engineering Tasks: Due to the long evaluation times, which impact the efficiency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency. ","permalink":"/notes/deepseek-r1_incentivizing_reasoning_capability_in/","summary":"\u003cp\u003eReinforcement Learning\u003c/p\u003e\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eRecent LLMs are rapidly advancing toward AGI. \u003cstrong\u003ePost-training\u003c/strong\u003e has emerged as an important component of the full training pipeline, which enhances accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against \u003cstrong\u003epre-training\u003c/strong\u003e. In the context of reasoning capabilities, OpenAI’s o1 series models were the first to introduce \u003cstrong\u003einference-time scaling\u003c/strong\u003e by increasing the length of the CoT reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning.\u003c/p\u003e","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"},{"content":"Introduction Self-attention-based architectures, especially Transformers, have become the first choice of model in natural language processing (NLP). The main approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset. Thanks to Transformers’ computational efficiency and scalability, it has become possible to train models of record-breaking size, with over 100B parameters. With the models and datasets growing, there is still no sign of reaching a performance limit .\nHowever, convolutional architectures remain dominant in computer vision. Inspired by the success of Transformers in NLP, many studies have attempted to integrate self-attention mechanisms into CNN-like architectures. A naïve application of self-attention to images would require each pixel to attend to every other pixel, resulting in quadratic computational cost with respect to the number of pixels, which does not scale to realistic image sizes. Later works introduced techniques such as local, sparse, or block attention, or reduced image resolution to reduce this cost. Although these methods make self-attention more scalable for visual data, they often demand complex engineering for efficient implementation on GPUs or TPUs, and are not well-suited for large-scale datasets. Consequently, in large-scale image recognition, classical convolutional architectures such as ResNet still dominate the state of the art.\nHere, a new approach ViT is introduced that applies a standard Transformer directly to images with minimal modifications. To achieve this, the image is divided into patches, and a sequence of linear embeddings of these patches is fed into the Transformer. Each image patch is treated the same way as a token (word) in NLP applications. The model is trained for image classification in supervised learning. When trained on mid-sized datasets such as ImageNet without strong regularization, the performance is moderate; however, the picture changes dramatically when the model is trained on large-scale datasets containing 14M–300M images. In such cases, large-scale training outweighs the need for strong inductive biases. The Vision Transformer (ViT) achieves excellent results when pre-trained at sufficient scale and then fine-tuned on downstream tasks with fewer data points.\nMethods The design of the model follows the original Transformer as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures, and their efficient implementations which can be used almost out of the box.\nVision Transformer (ViT) In its input stage, an image $\\mathbf{x} \\in R^{H \\times W \\times C}$ with height H, width W, and C channels is divided into small, non-overlapping patches of size $(P, P)$. Each patch is flattened into a vector, resulting in a sequence of patch vectors $\\mathbf{x}_p \\in R^{N \\times (P^2 \\cdot C)}$, where $N = \\frac{H W}{P^2}$ represents the total number of patches (also the effective sequence length for the Transformer). These flattened patch vectors are then linearly projected into a latent feature space of dimension $D$ using a trainable weight matrix. The output of this projection forms the patch embeddings, which serve as the Transformer’s token representations, the same as the word embeddings in NLP models.\nSimilar to BERT, ViT introduces an additional learnable [class] token that is placed in the beginning of the sequence of patch embeddings. Its output vector, after passing through all Transformer layers, represents the entire image and is used for classification. The model attaches a classification head to this [class] token output, which is implemented as a multilayer perceptron (MLP) with one hidden layer during pre-training and a single linear layer during fine-tuning.\n$$ output=softmax(Head([class] token)) $$\nPositional Encoding Because the Transformer itself has no spatial concept, ViT adds positional embeddings to the patch embeddings to retain information about the patches’ relative locations. The authors use simple, learnable 1D positional embeddings instead of more complex 2D-aware ones, as they found little improvement from the latter.\nTransformer Encoder The resulting sequence (class token + patch embeddings + positional embeddings) is then fed into a standard Transformer encoder identical to that used in NLP. Each encoder block alternates between multi-head self-attention (MSA) and feed-forward MLP sublayers, with layer normalization (LN) and residual connections applied around both.\nThe key computation within one layer of the model can be summarized by the following equations:\n$$ \\begin{aligned} z_0 \u0026= [\\,x_{class};\\; x_p^1 E;\\; \\ldots;\\; x_p^{N} E\\,] + E_{pos}, \\quad E \\in R^{(P^2 \\cdot C) \\times D}, \\quad E_{pos} \\in R^{(N+1) \\times D} \\\\ z'_{\\ell} \u0026= MSA(LN(z_{\\ell-1})) + z_{\\ell-1}, \\quad \\ell = 1, \\ldots,L \\\\ z_{\\ell} \u0026= MLP(LN(z'_{\\ell})) + z'_{\\ell}, \\quad \\ell = 1, \\ldots,L \\\\ y \u0026= LN(z^{0}_{L}) \\end{aligned} $$ Symbol Meaning P Patch size C Number of color channels N Number of patches D Embedding dimension $x_p^i \\in R^{P^2 \\cdot C}$ is the flattened vector of the i-th image patch. $E \\in R^{(P^2 \\cdot C) \\times D}$ is the linear projection matrix for patch embeddings. $E_{\\text{pos}} \\in R^{(N+1) \\times D}$ provides positional embeddings for all tokens, including the class token. $\\text{MSA}$ denotes the multi-head self-attention mechanism that models global dependencies across all patches. Self-attention computes: $\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$ $\\text{MLP}$ is a two-layer feed-forward network with a GELU (Gaussian Error Linear Unit) activation function to each patch token separately, adding non-linearity. There’s no cross-patch connection. The input and output dimensions are the same $D$. The residual connections (“+” terms) help preserve gradients and stabilize training. Final $LN(z_L^0)$ applied after all Transformer layers (not shown in block diagram). Together, these operations define one forward pass through the ViT encoder, where $L$ is the total number of Transformer layers.\nInductive Bias A defining characteristic of the Vision Transformer is that it possesses much weaker image-specific inductive bias than convolutional neural networks (CNNs). In CNNs, design principles such as local connectivity, two-dimensional neighborhood structure, and translation equivariance are built into the convolution operation at every layer. These biases make CNNs naturally suited for visual data.\nViT, in contrast, uses global self-attention, allowing every image patch to communicate with all other patches at once. This means ViT does not have any built-in understanding of local areas or spatial hierarchy — the model itself doesn’t “know” which patches are next to each other. Inside ViT, only the small MLP layers work separately on each patch (they don’t mix information between patches). The positional embeddings just tell the model roughly where each patch is in the image, but they don’t teach it how nearby regions are related. As a result, ViT must learn all spatial relationships by itself from the training data, instead of having them designed into the architecture like CNNs do. Therefore, ViT depends heavily on large-scale training data to learn these spatial patterns effectively.\nHybrid Architecture The authors also explore a hybrid architecture that combines the strengths of CNNs and Transformers. Instead of feeding the Transformer with raw image patches, they use CNN feature maps as input. In this case, each patch corresponds to a small region (for example, 1×1) of the CNN’s feature map. The later steps are the same. This hybrid approach introduces some of the beneficial inductive biases of CNNs (such as local feature extraction) while maintaining the global modeling capacity and scalability of the Transformer.\nFine-tuning and Higher Resolution During training, the Vision Transformer is first pre-trained on a large dataset and later fine-tuned on a smaller, task-specific dataset. When fine-tuning, the authors remove the old classification head (used in pre-training) and replace it with a new one that matches the number of classes in the new task.\nSometimes, it helps to fine-tune using higher-resolution images than were used during pre-training. In that case, the patch size stays the same, so the model now receives more patches (a longer input sequence). However, the position embeddings learned during pre-training may no longer align with the new number of patches. To fix this, the authors resize the position embeddings matrix in 2D so that they fit the new image resolution.\nThis resolution adjustment and patch extraction are the only points at which an inductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\nExperiments The authors compare Vision Transformer (ViT) with ResNet and hybrid CNN–Transformer models to evaluate its representation learning ability. Models are pre-trained on large datasets, ImageNet (1.3M images), ImageNet-21k (14M images), and JFT-300M (303M images), and then fine-tuned on smaller benchmarks such as CIFAR-10/100, Oxford Pets, Oxford Flowers-102, and the VTAB suite.\nThree ViT variants are tested: Base (86M params), Large (307M), and Huge (632M), corresponding to 12, 24, and 32 Transformer layers. Smaller patch sizes give better resolution but higher computational cost. Models are trained with the Adam optimizer, large batch size, and linear learning-rate warm-up, and fine-tuned at higher image resolutions for better results.\nWhen pre-trained on JFT-300M, ViT-H/14 and ViT-L/16 outperform both Big Transfer (BiT) ResNets and Noisy Student EfficientNet models on nearly all datasets while using less computation. Even with public ImageNet-21k pre-training, ViT achieves competitive accuracy.\nA key result is that dataset size is critical: ViT underperforms ResNets on small data (ImageNet-1k) but surpasses them as the dataset grows. Larger ViT models continue to improve with more data, while ResNets plateau. Hybrid CNN–Transformer models help for smaller data but offer no advantage at large scale.\nIn few-shot evaluation, the Vision Transformer is not fully retrained. Instead, its pre-trained features are frozen, and only a simple linear classifier is trained on top of them using a very small number of labeled examples per class, such as 5 images for each category in ImageNet. This setup tests how well the model’s learned representations can generalize to new tasks when only a few labeled samples are available, showing the quality and transferability of the pre-trained features.\nAlso, the authors explore self-supervised pre-training for Vision Transformers, inspired by how Transformers in NLP achieve strong results through large-scale self-supervised learning rather than supervision alone. They experiment with a masked patch prediction task, similar to BERT’s masked language modeling, where parts of an image are hidden and the model learns to predict them. Using this method, the smaller ViT-B/16 model reaches 79.9% accuracy on ImageNet, which is 2% better than training from scratch but still 4% lower than supervised pre-training. The authors note that while self-supervision improves performance, it does not yet match large-scale supervised results. They suggest that exploring contrastive pre-training methods could be a promising direction for future research.\nConclusion ViT is the direct application of Transformers to image recognition. Unlike prior works using self-attention in computer vision, it do not introduce image-specific inductive biases into the architecture apart from the initial patch extraction step. Instead, it interpret an image as a sequence of patches and process it by a standard Transformer encoder as used in NLP. This simple, yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets. Thus, ViT matches or exceeds the sota on many image classification datasets, meantime being relatively cheap to pre-train.\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other computer vision tasks, such as detection and segmentation. Another challenge is to continue exploring self-supervised pre-training methods. The initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pre-training. Finally, further scaling of ViT would likely lead to improved performance.\n","permalink":"/notes/vit/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eSelf-attention-based architectures, especially \u003cstrong\u003eTransformers\u003c/strong\u003e, have become the first choice of model in \u003cstrong\u003enatural language processing (NLP)\u003c/strong\u003e. The main approach is to \u003cstrong\u003epre-train\u003c/strong\u003e on a large text corpus and then \u003cstrong\u003efine-tune\u003c/strong\u003e on a smaller task-specific dataset. Thanks to Transformers’ computational efficiency and scalability, it has become possible to train models of record-breaking size, with over 100B parameters. With the models and datasets growing, there is still no sign of reaching a performance limit .\u003c/p\u003e","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"content":"Abstraction Bayesian Optimization is a method for finding the best solution when evaluating the objective function is slow or expensive (taking minutes or hours).\nIt is mainly used for problems with fewer than 20 variables and where evaluations may contain noise.\nThe method works by:\nBuilding a surrogate model of the unknown function using a Bayesian machine learning method called Gaussian Process regression, which predicts both the function value and its uncertainty. Using an acquisition function (expected improvement, knowledge gradient or entropy search) to decide where to sample next — balancing exploration and exploitation. The tutorial also extends expected improvement to handle noisy evaluations, supported by a formal decision-theoretic argument, rather than ad hoc fixes.\n1 Introduction Bayesian Optimization (BayesOpt) is a machine learning–based method for optimizing expensive, black-box functions, it’s designed for black-box derivative-free global optimization. It aims to solve\n$$ \\max_{x \\in A} f(x) $$\nwhere $f(x)$ is expensive to evaluate, derivative-free, and continuous.\nTypical Problem Setting\nThe input x lies in a continuous space R^d with small dimensionality (usually d ≤ 20). → search space The feasible set A is simple, such as a box constraint or simplex. → search area The objective function f is: Continuous (needed for Gaussian Process modeling). Expensive — each evaluation might take hours or cost resources. Black-box — no known analytic structure like linearity or convexity. Derivative-free — we can only observe f(x), not gradients. Possibly noisy — measurements may include Gaussian noise. At first, we pretend every time we evaluate f(x), we get the exact same result. Later in the paper, the author adds stochastic noise — meaning repeated evaluations of the same xxx might give slightly different results (like random fluctuations). Concept Meaning Intuition Search space The entire region of possible inputs x, its dimensionality tells us how complex the problem is. it’s the multi-dimensional space that defines where you can look. Feasible set / search area The subset of that space that you actually allow x to take, i.e., with all constraints applied. this is the region inside the search space that satisfies all limits (bounds, rules). BayesOpt is for global optimization of black-box functions. It builds a surrogate model of f(x) using a Bayesian machine learning technique, typically Gaussian Process (GP) regression. It then chooses where to sample next using an acquisition function (e.g., Expected Improvement, Knowledge Gradient, or Entropy Search). This balances exploration (trying uncertain areas) and exploitation (sampling promising areas).\nThe ability to optimize expensive black-box derivative-free functions makes BayesOpt extremely versatile. Recently it has become extremely popular for tuning hyperparameters in machine learning algorithms. And it’s also suitable for engineering design, scientific experiments and reinforcement Learning.\nWhat makes Bayesian Optimization unique compared to general surrogate-based optimization approaches are using surrogates developed using bayesian statistics and choosing new points using a probabilistic acquisition rule instead of heuristics. (reasoning probabilistically about what it already knows and what it’s uncertain about)\n2 Overview of BayesOpt BayesOpt consists of two main components: a Bayesian statistical model for modeling the objective function, and an acquisition function for deciding where to sample next.\nBayesian Statistical Model Models the unknown objective function $f(x)$.\nTypically a Gaussian Process (GP).\nProduces a posterior distribution:\n$$ f(x) \\sim \\mathcal{N}(\\mu_n(x), \\sigma_n^2(x)) $$\nwhere:\n$\\mu_n(x)$: predicted mean (best guess) $\\sigma_n(x)$: predicted uncertainty Updated each time new data (evaluations of f) are observed.\nAcquisition Function Decides where to sample next based on the GP’s posterior. Measures the “value” of sampling a new point x: High $\\mu_n(x)$: promising area. High $\\sigma_n(x)$: uncertain area. Common types: Expected Improvement (EI), Knowledge Gradient (KG), Entropy Search (ES), and Predictive Entropy Search (PES). After evaluating the objective according to an initial space-filling experimental design, often consisting of points chosen uniformly at random, they are used iteratively to allocate the remainder of a budget of N function evaluations.\nAlgorithm 1: Basic Bayesian Optimization Loop\nStart with a Gaussian Process (GP) model that guesses how f might behave. Test f at a few random starting points spread across the space. Repeat until you run out of trials: Update the GP using all results collected so far. Use the acquisition rule to pick the next best point to try. Test the real function at that point to get a new result ( y_n = f(x_n) ). Add this new point to your data. Give the final answer: The point with the best actual value, or The point the GP predicts to be the best. 3 Gaussian Process (GP) Regression Gaussian Process (GP) Regression is a Bayesian way to model an unknown function $f(x)$.\nIt assumes that any collection of function values $[f(x_1), f(x_2), \u0026hellip;, f(x_k)]$ follows a multivariate normal distribution with a specific mean vector and covariance matrix.\nSo instead of guessing one possible curve for $f(x)$, we assume a probability distribution over all possible smooth curves.\n3.1 Initialization Steps Step 1: Define the Prior Before we see any data, we describe our belief about $f(x)$ using:\nA mean function $\\mu_0(x)$ → gives the average expected value, often set to 0 (no bias). A covariance (kernel) function $\\Sigma_0(x_i, x_j)$ → shows how similar two points are. Close points = high correlation (similar f values) Far points = low correlation (independent values) Together, they form the prior:\n$$ f(x_{1:k}) \\sim \\text{Normal}(\\mu_0(x_{1:k}), \\Sigma_0(x_{1:k}, x_{1:k})) $$\nStep 2: Update with Observed Data (Bayes’ Rule) Once we have some known data $(x_1, f(x_1)), \u0026hellip;, (x_n, f(x_n))$, we update our belief to get the posterior distribution, what we now believe about the function after seeing real values.\nFor a new point x:\n$$ f(x)|f(x_{1:n}) \\sim \\text{Normal}(\\mu_n(x), \\sigma_n^2(x)) $$\nwhere:\n$\\mu_n(x)$: the posterior mean — our best prediction at $x$ $\\sigma_n^2(x)$: the posterior variance — how uncertain we are at $x$ The GP uses the kernel to decide how much nearby points influence the prediction:\nIf $x$ is near known points → high confidence, low uncertainty. If $x$ is far from all known points → low confidence, high uncertainty. Step 3: Posterior Formula $$ \\mu_n(x) = \\Sigma_0(x, x_{1:n})\\Sigma_0(x_{1:n}, x_{1:n})^{-1}(f(x_{1:n}) - \\mu_0(x_{1:n})) + \\mu_0(x) $$\n$$ \\sigma_n^2(x) = \\Sigma_0(x, x) - \\Sigma_0(x, x_{1:n})\\Sigma_0(x_{1:n}, x_{1:n})^{-1}\\Sigma_0(x_{1:n}, x) $$\nMeaning:\nThe new prediction $\\mu_n(x)$ = weighted average of nearby known values (old belief + weighted correction from known data) The uncertainty $\\sigma_n^2(x)$ = initial uncertainty minus what we’ve learned (reduce uncertainty) Step 4: Practical Notes Instead of directly inverting large matrices, use Cholesky decomposition for stability and speed. Add a tiny number (e.g., $10^{-6}$) to the diagonal of the covariance matrix to prevent numerical errors. The same formulas work for: Many new points at once (matrices) Continuous domains (theoretically infinite points) 3.2 Choosing a Mean Function and Kernel Kernel choice The kernel (covariance function) defines how much two inputs (x) and (x\u0026rsquo;) are correlated. Points that are close in the input space are more strongly correlated, the kernel must be positive semi-definite (it cannot give negative variances):\n$$ if (||x - x\u0026rsquo;|| \u0026lt; ||x - x\u0026rsquo;\u0026rsquo;||), then (Σ_0(x, x\u0026rsquo;) \u0026gt; Σ_0(x, x\u0026rsquo;\u0026rsquo;)). $$\nIf you combine things in any way, the total uncertainty you calculate will never be negative.\nPower Exponential (Gaussian) Kernel $$ Σ_0(x, x\u0026rsquo;) = α_0 \\exp(-||x - x\u0026rsquo;||^2) $$\nThe is the most common kernel and produces very smooth functions.\n$α_0$ controls overall variance (how much $f(x)$ can vary). $α_i$ inside $||x - x\u0026rsquo;||^2 = \\sum_i α_i (x_i - x\u0026rsquo;_i)^2$ control how quickly correlation decreases as inputs differ. Matérn Kernel $$ Σ_0(x, x\u0026rsquo;) = α_0 \\frac{2^{1-ν}}{Γ(ν)} (\\sqrt{2ν}||x - x\u0026rsquo;||)^{ν} K_ν(\\sqrt{2ν}||x - x\u0026rsquo;||) $$\nAdds a parameter $ν$ that controls smoothness. Smaller $ν$ produces rougher functions, larger $ν$ gives smoother ones. $K_ν$ is the modified Bessel function. Mean function The mean function expresses the expected trend of $f(x)$ before seeing data. The most common choice is a constant mean: $μ_0(x) = μ$. And If $f(x)$ is believed to have a trend, a parametric mean can be used:\n$$ μ_0(x) = μ + \\sum_{i=1}^{P} β_i Ψ_i(x) $$\nwhere $Ψ_i(x)$ are basis functions, often low-order polynomials.\nFor example, $Ψ(x) = x$ gives a linear trend, $Ψ(x) = [1, x, x^2]$ gives a quadratic trend.\n3.3 Choosing Hyperparameters The mean and kernel functions contain parameters (like $\\alpha_0, \\nu, \\mu$) called hyperparameters, grouped in a vector $\\eta$. These control how the Gaussian Process behaves (for example, how smooth it is or what its average level is).\nMaximum Likelihood Estimation (MLE) $$ \\hat{\\eta} = \\arg\\max_{\\eta} P(f(x_{1:n}) | \\eta) $$\nChoose hyperparameters that make the observed data most likely under the GP model. It’s simple and widely used. But it can give unreasonable results if the model overfits (e.g., too smooth or too wiggly).\nMaximum a Posteriori (MAP) $$ \\hat{\\eta} = \\arg\\max_{\\eta} P(\\eta | f(x_{1:n})) = \\arg\\max_{\\eta} P(f(x_{1:n}) | \\eta) P(\\eta) $$\nSimilar to MLE, but adds a prior $P(\\eta)$ on the hyperparameters. This prior prevents extreme or unrealistic parameter values. The MLE is a special case of MAP when $P(\\eta)$ is constant (flat). Common priors include uniform, normal, or log-normal distributions.\nFully Bayesian Approach $$ P(f(x) = y| f(x_{1:n})) = \\int P(f(x) = y| f(x_{1:n}), \\eta) P(\\eta | f(x_{1:n})) d\\eta $$\nInstead of choosing a single best $\\eta$, it integrates over all possible values of the hyperparameters. It produces more robust uncertainty estimates but is computationally expensive. In practice, it’s approximated using sampling methods (e.g., MCMC). MAP can be viewed as an approximation to this full Bayesian inference.\nDon’t fix η; instead, consider all possible η, weighted by how likely each one is.\nBut this high-dimensional and usually cannot be computed exactly, so in practice, we approximate it by sampling:\n$$ P(f(x) = y |f(x_{1:n})) \\approx \\frac{1}{J} \\sum_{j=1}^{J} P(f(x) = y |f(x_{1:n}), \\eta = \\eta_{j}) $$\nwhere the samples $\\eta_j$ are drawn from $P(\\eta | f(x_{1:n}))$. This is typically done using MCMC (Markov Chain Monte Carlo).\n3. Summary table Method In short Pros Cons MLE Fit the data best Finds hyperparams that best fit data Simple MAP Fit the data but stay reasonable Adds prior to control extremes More stable Fully Bayesian Consider all possible fits, weighted by probability computationally expensive Integrates over all possible scenarios 4 Acquisition Functions Expected Imrpvement(EI), Knowledge Gradient(KG), Entropy Search(ES)\n4.1 Expeced Improvement Goal: Decide where to sample next so that we are likely to improve our current best result.\nSuppose we have already tested n points. The best value so far is\n$$ f_n^* = \\max_{m \\le n} f(x_m) $$\nParameters:\nn is the number of points we have already evaluated so far. m is just an index variable If we evaluate a new point x, its value f(x) is uncertain. The improvement is how much better it is than the current best:\n$$ I(x) = [f(x) - f_n^*]^+ = \\max(f(x) - f_n^*, 0) $$\nSince f(x) is a random variable under the Gaussian Process model, we take the expected value of this improvement:\n$$ EI_n(x) = E_n[[f(x) - f_n^*]^+] $$\nBecause $f(x) \\sim \\mathcal{N}(\\mu_n(x), \\sigma_n^2(x))$, EI can be computed in closed form:\n$$ EI_n(x) = (\\mu_n(x) - f_n^*)\\Phi(z) + \\sigma_n(x)\\phi(z), \\quad z = \\frac{\\mu_n(x) - f_n^*}{\\sigma_n(x)} $$\nwhere $\\Phi$ is the normal CDF and $\\phi$ is the normal PDF.\n$Φ(z)$ = the cumulative distribution function (CDF) of the standard normal.\n→ It gives the probability that a standard normal variable is ≤ z.\n$ϕ(z)$ = the probability density function (PDF) of the standard normal.\n→ It gives the height of the bell curve at z.\nGoal → How much do I expect to improve the best result I’ve found so far if I test at this new point x?\n$EI_n(x)$ = predicted gain × chance it’s true + uncertainty × possible surprise\nEI =（平均能提升多少 × 提升的可能性） + （不确定性 × 由不确定性带来的潜在收益）\nFirst term: expected improvement if you trust the mean. Second term: extra improvement that might happen because the model is uncertain. The next sampling point is chosen by maximizing EI:\n$$ x_{n+1} = \\arg\\max_x EI_n(x) $$\nInterpretation:\nEI balances two goals:\nExploitation: sampling where the predicted mean $\\mu_n(x)$ is high. Exploration: sampling where uncertainty $\\sigma_n(x)$ is high. This trade-off helps the algorithm explore new areas and improve known good ones efficiently.\n4.2 Knowledge Gradient The Knowledge Gradient (KG) acquisition function measures the expected value of information gained from sampling a new point.\nUnlike Expected Improvement (EI), which focuses on immediate improvement at the sampled point, KG evaluates how much better our overall knowledge about the objective becomes after sampling.\nEI assumes the final solution must be one of the evaluated points. KG relaxes this: after we take one more sample, we can still choose any point (evaluated or not) as our final decision. Therefore, the value of sampling comes not just from finding a better local result, but from improving the entire model’s understanding of the objective surface. Mathematical Form:\n$$ KG_n(x) = E_n[\\mu_{n+1}^* - \\mu_n^*] $$\nwhere\n$\\mu_n^* = \\max_{x\u0026rsquo;} \\mu_n(x\u0026rsquo;)$: current predicted maximum, $\\mu_{n+1}^* = \\max_{x\u0026rsquo;} \\mu_{n+1}(x\u0026rsquo;)$: predicted maximum after taking a new sample at (x), The expectation $\\mathbb{E}_n[\\cdot]$ averages over possible outcomes of the new observation. Interpretation:\nKG measures the expected increase in the best achievable posterior mean after taking one new sample.\nAlgorithm 2 Simulation-based computation Purpost: estimate how much the best mean prediction might improve if we sample at x.\nSteps:\nFind the current best mean value\n$$ \\mu_n^* = \\max_{x\u0026rsquo;} \\mu_n(x\u0026rsquo;) $$\nThis is the best prediction under the current Gaussian Process (GP).\nSimulate what could happen if we sample at x\nRepeat J times (Monte Carlo simulation):\nDraw a random possible observation\n$$ y_{n+1} \\sim \\mathcal{N}(\\mu_n(x), \\sigma_n^2(x)) $$\n(equivalently, $y_{n+1} = \\mu_n(x) + \\sigma_n(x)Z ; Z\\sim\\mathcal{N}(0,1)$).\nUpdate the GP posterior using this “imagined” observation $(x, y_{n+1})$, obtaining a new mean function $\\mu_{n+1}(\\cdot)$.\nCompute the new best mean value\n$$ \\mu_{n+1}^* = \\max_{x\u0026rsquo;} \\mu_{n+1}(x\u0026rsquo;) $$\nCompute the gain for this scenario\n$$ \\Delta^{(j)} = \\mu_{n+1}^* - \\mu_n^* $$\nAverage over all J simulations\nThe Knowledge Gradient estimate is\n$$ KG_n(x) = \\frac{1}{J}\\sum_{j=1}^J \\Delta^{(j)} $$\nAlgorithm 3: Multi-start Stochastic Gradient Ascent Goal: Find the best next sampling point $x$ that maximizes $KG_n(x)$.\nProcess:\nStart from multiple random initial points $x_0^{(r)}$ (r = 1,…,R).\nFor each start, perform T stochastic gradient ascent steps:\nCompute stochastic gradient $G$ (estimated using Algorithm 4).\nUpdate $x_t^{(r)} = x_{t-1}^{(r)} + \\alpha_t G$,\nwhere $\\alpha_t = a / (a + t)$ is a decreasing step size.\nAfter T steps, estimate $KG_n(x_T^{(r)})$ using simulation (Algorithm 2).\nReturn the point with the largest estimated KG value.\nNotes:\nUsing multiple random starts helps avoid local optima. This method converges to a local maximum of the KG function. Algorithm 4 — Simulation of Stochastic Gradients Purpose: Compute an unbiased estimate of the gradient $\\nabla KG_n(x)$.\nSteps:\nFor each of J simulations: Sample a random variable $Z \\sim \\mathcal{N}(0,1)$. Generate a possible observation $y_{n+1} = \\mu_n(x) + \\sigma_n(x)Z$. Update the GP posterior using $(x, y_{n+1})$ to obtain new mean $\\mu_{n+1}$. Compute the new best posterior mean value $\\mu^*{n+1} = \\max{x\u0026rsquo;} \\mu{n+1}(x\u0026rsquo;)$. Evaluate its gradient w.r.t. the sampled point x. Average over all J samples to estimate $\\nabla KG_n(x)$. 4.3 Entropy Search and Predictive Entropy Search Entropy Search (ES) and Predictive Entropy Search (PES) are acquisition functions in Bayesian optimization that try to reduce uncertainty about the position of the global optimum $x^*$.\nInstead of asking “which point will improve the function value most,” they ask “which point will tell me the most about where the best value is.”\nEntropy measures uncertainty. If the posterior over the location of the global optimum x has high entropy, we are unsure where the best point is. A good new observation is one that most reduces this entropy.\nEntropy Search (ES) Purpose: Measures how much uncertainty (entropy) about the true optimum $x^*$ will go down if we sample at point x.\n$P_n(x^*)$: the current posterior belief over where the global optimum lies. $H(P_n(x^*))$: its entropy, representing uncertainty. After sampling at a candidate point x, the posterior changes to $P_n(x^*|f(x))$. The expected reduction in entropy is $$ ES_n(x) = H(P_n(x^*)) - E_{f(x)}[H(P_n(x^*|f(x)))] $$\nThis means we prefer points x where observing $f(x)$ is expected to most reduce our uncertainty about $x^*$. Computing ES directly is difficult, because it requires calculating entropy over many possible function outcomes.\nPredictive Entropy Search (PES) Purpose: Measures how much uncertainty (entropy) about the true optimum $x^*$ will go down if we sample at point x.\nPES reformulates the same idea in a simpler way. Instead of measuring the entropy of the optimum location directly, it measures the mutual information between $f(x)$ and $x^*$:\n$$ PES_n(x) = H(P_n(f(x))) - \\mathbb{E}_{x^*}[H(P_n(f(x)|x^*))] $$\nThis is mathematically equivalent to ES but easier to approximate in practice. PES estimates how much knowing the value of $f(x)$ would reduce uncertainty about where the optimum is.\nIntuitive difference from EI and KG\nExpected Improvement (EI) focuses on increasing the best function value so far. Knowledge Gradient (KG) focuses on improving the overall model prediction. ES and PES focus on learning information that narrows down the true location of the optimum. Entropy Search and Predictive Entropy Search choose sampling points that give the most information about the global optimum.\nThey are more global and information-driven than EI or KG but are computationally more complex.\n4.4 Multi-Step Optimal Acquisition Functions Bayesian optimization can be viewed as a sequential decision process: each sample depends on past results. Standard methods like EI, KG, ES, and PES are one-step optimal, choosing the next point assuming only one evaluation remains.\nA multi-step optimal strategy would plan several future evaluations ahead, maximizing total expected reward. However, computing it is extremely hard due to the dimensionality.\nRecent studies have tried approximate multi-step methods using reinforcement learning and dynamic programming, but they are not yet practical. Experiments show that one-step methods already perform nearly as well, so they remain the preferred approach in practice.\n5 Exotic Bayesian Optimization Noisy Evaluations Gaussian Process (GP) regression can handle noisy observations by adding noise variance to the covariance matrix. In practice, the noise variance is often unknown and treated as a hyperparameter. If noise varies across the domain, it can be modeled with another GP.\nAcquisition functions like EI, KG, ES, and PES naturally extend to noisy settings, but EI becomes less straightforward since the “improvement” is not directly observable. The KG approach is more robust under noise.\nParallel Evaluations Parallel Bayesian optimization allows evaluating several points simultaneously using multiple computing resources. Expected Improvement (EI) is extended to parallel EI, where several points $(x^{(1)}, \\dots, x^{(q)})$ are selected jointly to maximize expected improvement.\nVariants like multipoint EI and Constant Liar approximations simplify optimization. Similar extensions exist for KG, ES, and PES. Parallel versions are computationally harder but useful for speeding up optimization on modern systems.\nConstraints In real problems, sampling may be limited by constraints $g_i(x) \\ge 0$ (g is the constraint). These constraints can be as expensive to evaluate as $f(x)$. EI can be extended to check improvement only among feasible points, i.e., points that satisfy all $g_i(x) \\ge 0$.\nRecent work also studied constrained Bayesian optimization under noisy or uncertain feasibility.\nMulti-Fidelity and Multi-Information Source Evaluations Sometimes there are multiple ways to estimate the objective, each with different accuracy and cost (called fidelities). For example, $f(x, s)$ may represent evaluating $x$ with fidelity level $s$:\nlow fidelity is cheap but inaccurate high fidelity is expensive but precise The goal is to allocate a limited total budget among fidelities to maximize information gain. Methods like KG, ES, and PES can handle this setting, but EI does not generalize well because evaluating $f(x, s)$ for $s ≠ 0$ never provides an improvement in the best objective function value seen.\nRandom Environmental Conditions and Multi-Task Bayesian Optimization Here, the objective $f(x, w)$ depends on both design variables x and random environmental variables w (e.g., weather, test fold, etc.). The aim is to optimize either the expected value $\\int f(x,w)p(w)dw$ or the sum over tasks $\\sum f(x,w)p(w)$.\nBy observing performance under different w, we can infer information about nearby conditions, reducing the need for full evaluations. This setup is widely used in engineering, machine learning (cross-validation folds), and reinforcement learning. Modified EI, KG, and PES methods apply here.\nDerivative Observations Sometimes gradient (derivative) information is available along with function values. Gradients can be incorporated into GP models to improve predictions and optimization speed. While EI does not directly benefit from derivatives, KG can use them effectively.\nGradient-based updates improve convergence and numerical stability, especially in regions where function evaluations are expensive.\n6 Software 7 Conclusion and Research Directions 7.1 Conclusion The paper reviews Bayesian Optimization (BO) including Gaussian Process (GP) regression, and key acquisition functions such as expected improvement (EI), knowledge gradient (KG), entropy search (ES), and predictive entropy search (PES). And paper extends discussion to more complex cases (noise, constraints, multi-fidelity, multi-task, etc.).\n7.2 Future Research Directions Theory and Convergence: There is a need for a deeper theoretical understanding of BO. Multi-step optimal algorithms are known to exist but are hard to compute. We lack finite-time performance guarantees and full understanding of convergence rates. Beyond Gaussian Processes: Most BO methods use GPs, but new statistical models may better capture some types of problems. Research should aim to develop alternative models suited for specific applications. High-Dimensional Optimization: Current BO struggles when the number of parameters is large. New methods should leverage structure in high-dimensional problems. Exotic Problem Structures: BO should handle more complex, real-world conditions (multi-fidelity data, environmental randomness, derivative information). Combining method development with practical applications can reveal new challenges and innovations. Real-World Impact: BO has strong potential in chemistry, materials science, and drug discovery, where experiments are expensive and slow. However, few researchers in these fields currently use BO — so expanding awareness and applications is important. ","permalink":"/notes/a_tutorial_on_bayesian_optimization/","summary":"\u003ch1 id=\"abstraction\"\u003eAbstraction\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eBayesian Optimization\u003c/strong\u003e is a method for finding the best solution when evaluating the objective function is \u003cstrong\u003eslow or expensive\u003c/strong\u003e (taking minutes or hours).\u003c/p\u003e\n\u003cp\u003eIt is mainly used for problems with \u003cstrong\u003efewer than 20 variables\u003c/strong\u003e and where evaluations may contain \u003cstrong\u003enoise\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThe method works by:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBuilding a \u003cstrong\u003esurrogate model\u003c/strong\u003e of the unknown function using a \u003cstrong\u003eBayesian machine learning method called Gaussian Process regression\u003c/strong\u003e, which predicts both the function \u003cstrong\u003evalue\u003c/strong\u003e and its \u003cstrong\u003euncertainty\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eUsing an \u003cstrong\u003eacquisition function\u003c/strong\u003e (expected improvement, knowledge gradient or entropy search) to decide \u003cstrong\u003ewhere to sample next\u003c/strong\u003e — balancing exploration and exploitation.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe tutorial also extends expected improvement to handle \u003cstrong\u003enoisy evaluations\u003c/strong\u003e, supported by a \u003cstrong\u003eformal decision-theoretic argument\u003c/strong\u003e, rather than ad hoc fixes.\u003c/p\u003e","title":"A Tutorial on Bayesian Optimization"},{"content":" CrossNet and CrossNet++ Both are for Reference-Based Super-Resolution (RefSR), using a low-resolution (LR) image and a high-resolution (HR reference) image to make a sharper, high-quality output.\nThe performance of CrossNet drops with the increasing of perspective parallax, the improvement of CrossNet++:\nTwo-stage warping → improves alignment Self-supervised flow estimation → uses FlowNet to estimate motion between LR and Ref images Cross-scale alignment → Aligns features at multiple resolutions Hybrid loss functions → warping + landmark + super-resolution loss Real-world performance → produces smoother, sharper, and more realistic results, suitable for a variety of scenarios Abstraction CrossNet++ focuses on reference-based super-resolution (RefSR), improving a low-resolution image using a high-resolution reference from another camera. This task is hard because of large scale differences (8×) and big parallax (~10%) between the two views.\nTo solve this, CrossNet++ introduces an end-to-end two-stage network with:\nCross-scale warping modules, align images at multiple zoom levels to narrow down parallax, handle scale and parallax differences. Image encoder and fusion decoder, extract multi-scale features and combine them to reconstruct a high-quality super-resolved image. It uses new hybrid loss functions comprising warping loss, landmark loss and super-resolution loss to improve accuracy and stability by stabilizing the training of alignment module and helps to improve super-resolution performance.\ntwo-stage wrapping, hybrid loss\n1 Introduction The development of method:\npatch-matching + patch-synthesis + iteratively applying non-uniform warping Causes grid artifacts, incapable of handling the non-rigid image deformation Directly warping between the low and high-resolution images is inaccurate. Such iterative combination of patch matching and warping introduces heavy computational burden. The difference between rigid deformation and non-rigid deformation:\nRigid deformation = viewpoint change, like camera movement. Non-rigid deformation = object itself changes shape (face expression, fabric fold, petal bending). Grid artifacts = tiny square patterns caused by wrong image enlargement or alignment.\nwarping + synthesis It cannot effectively handle large-parallax cases that widely existed in real-world data. pre-warping + re-warping + synthesis CrossNet++ is a unified framework enabling fully end-to-end training which does not require pretraining the flow estimator. Two-stage pipeline: Two-stage cross-scale warping module. stage 1: Uses FlowNet to estimate motion (optical flow) between the low-resolution (LR) and reference (Ref) images without needing ground-truth flow (self-supervised). This produces a roughly aligned “warped-Ref” image. stage 2: Further refines alignment between the warped-Ref and LR image for more accurate warping. Hybrid loss: warping loss, landmark loss and super-resolution loss. warping loss: supervise the flow estimation implicitly. landmark loss: supervise the flow estimation explicitly. Without ground-truth flow = the model learns to estimate motion on its own, using only the images, not any pre-labeled motion data.\nInterpolation = predict inside known area Extrapolation = predict outside known area 2 Related Work 3 Preliminary of CrossNet 3.3 Network Structure 3.3.1 Alignment Module The alignment module aims to align the reference image $I_{REF}$ with the low-resolution image $I_{LR}$. CrossNet++ adopts a warping-based alignment using two-stage optical flow estimation.\nIn the first stage, a modified FlowNet (denoted as $Flow_1$) predicts the flow field between an upsampled LR image $I_{LR↑}$ and the reference image $I_{REF}$:\n$$ V_1^0 = Flow_1(I_{LR↑}, I_{REF}) $$ where $V_1^0$ represents the flow at scale 0 (the original image scale). The upsampled LR image $I_{LR↑}$ is obtained via a single-image SR method:\n$$ I_{LR↑} = SISR(I_{LR}) $$ Then, the reference image is spatially warped using this flow to produce the pre-aligned reference:\n$$ \\hat{I}_{REF} = Warp(I_{REF}, V_1^0) $$ In the second stage, the pre-aligned reference \\( \\hat{I}_{\\mathrm{REF}} \\) and the upsampled LR image \\( I_{LR}\\uparrow \\) are again input to another flow estimator \\( Flow_2 \\) to compute multi-scale flow fields:\n$$ {V_2^3, V_2^2, V_2^1, V_2^0} = Flow_2(I_{LR↑}, \\hat{I}_{REF}) $$ These multi-scale flows are used later in the synthesis network to refine alignment and reconstruct the final super-resolved image.\nthis two-stage alignment, coarse warping followed by multi-scale refinement—allows CrossNet++ to handle large parallax and depth variations, achieving more accurate correspondence and better alignment quality than the original CrossNet.\n3.3.2 Encoder Through the alignment module, we obtain four flow fields at different scales. The encoder receives the pre-aligned reference image $\\hat I_{REF}$ and the upsampled LR image $I_{LR↑}$, then extracts their feature maps at four different scales.\nThe encoder has five convolutional layers with 64 filters of size ( 5 $\\times$ 5 ).\nThe first two layers (stride = 1) extract the feature map at scale 0. The next three layers (stride = 2) produce lower-resolution feature maps for scales 1 to 3. These operations are defined as: where $\\sigma$ is the ReLU activation, $*_1$ and $*_2$ denote convolutions with strides 1 and 2 respectively, and $F^i$ is the feature map at scale $i$.\n$$ F^0 = \\sigma(W^0 *_{1} I) $$ $$ F^i = \\sigma(W^i *_{2} F^{i-1}), \\\\ \\quad i = 1, 2, 3, $$ Unlike the original CrossNet, CrossNet++ uses a shared encoder for both $\\hat I_{REF}$ and $I_{LR↑}$ instead of two separate encoders, which reduces about 0.41 M parameters while maintaining accuracy.\nThe resulting feature sets are:\n$$ {F^0_{LR}, F^1_{LR}, F^2_{LR}, F^3_{LR}} \\quad \\text{and} \\quad {F^0_{REF}, F^1_{REF}, F^2_{REF}, F^3_{REF}}. $$ Finally, each reference feature map $F^i_{REF}$ is warped using the multi-scale flow fields $V^i_2$ from to produce the aligned feature maps:\n$$ \\hat{F}^i_{REF} = Warp(F^i_{REF}, V^i_2), \\\\ \\quad i = 0, 1, 2, 3. $$ In short, the encoder extracts multi-scale feature maps for both LR and reference images using shared convolutional layers, then aligns the reference features to the LR features through warping with multi-scale flow fields, which provides precise, scale-consistent alignment for the next fusion step.\n3.3.3 Decoder After feature extraction and alignment, the decoder fuses the LR and reference feature maps and generates the final super-resolved image.\nIt follows a U-Net-like structure, which progressively upsamples the feature maps from coarse to fine scales.\nTo create the decoder features at scale $i$, the model concatenates:\nthe warped reference features $\\hat{F}^i_{REF}$, the LR image features $F^i_{LR}$ , and the decoder feature from the next coarser scale $F^{i+1}_{D}$ (if available). Then a deconvolution layer (stride 2, filter size 4 $\\times$ 4) is applied:\n$$ F^3_{D} = \\sigma(W^3_{D} *_{2} (F^3_{LR}, \\hat{F}^3_{REF})) $$ where $*_2$ is deconvolution with stride 2 and $\\sigma$ is the activation (ReLU).\n$$ F^i_{D} = \\sigma(W^i_{D} *_{2} (F^i_{LR}, \\hat{F}^i_{REF}, F^{i+1}_{D})), \\\\quad i = 2, 1, $$ After that, three more convolutional layers (filter sizes (5 $\\times$ 5), channels {64, 64, 3}) perform post-fusion to synthesize the final image $I_p$:\n$$ F^0_{D} / F_1 = \\sigma(W_1 *_{1} (F^0_{LR}, \\hat{F}^0_{REF}, F^1_{D})) $$ $$ F_2 = \\sigma(W_2 *_{1} F_1) $$ $$ I_p = \\sigma(W_p *_{1} F_2), $$ where $*_{1}$ means convolution with stride 1.\nThe decoder takes aligned multi-scale features from LR and reference images, fuses them step by step through deconvolutions and convolutions, and finally reconstructs the high-resolution output image $I_p$, the sharp, super-resolved result.\n3.4 Loss Function warping loss, landmark loss → encourage flow estimator to generate precise flow.\nsuper-resolution loss → is responsible for the final synthesized image.\n3.4.1 Warping Loss Used in the first-stage Flow Estimator to regularize the generated optical flow.\nIt ensures that the warped reference image $\\hat I_{REF}$ is close to the ground-truth HR image $I_{HR}$, assuming both share a similar viewpoint.\nThe loss minimizes pixel-wise intensity differences:\n$$ L_{warp} = \\frac{1}{2N} \\sum_{i,s,c} (\\hat{I}_{REF}(s, c) - I_{HR}(s, c))^2 $$ where $N$ is the total number of samples, $i$, $s$, and $c$ iterate over training samples, spatial locations and color channels respectively.\n3.4.2 Landmark Loss This loss provides directional geometric guidance for large-parallax cases.\nIt uses SIFT feature matching to find corresponding landmark pairs $(p, q)$ between the HR and reference images, and applies the flow field $V^0_1$ to warp these landmarks.\nThe warped landmark $\\hat{p}^j$ is computed as:\n$$ \\hat{p}^j = p^j + V^0_1[p^j] $$ and the landmark loss penalizes the distance between warped and target landmarks:\n$$ L_{lm} = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{m_i} | \\hat{p}^j - q^j |_2^2 $$ where $m_i$ is the number of landmark pairs in image $i$.\nThis term helps the flow estimator predict more accurate motion fields, especially when viewpoints differ greatly.\n3.4.3 Super-Resolution Loss This loss directly trains the model to synthesize the final super-resolved image $I_p$, comparing it with the ground-truth high-resolution image $I_{HR}$ using the Charbonnier penalty (a smooth $L_1$ loss):\n$$ L_{sr} = \\frac{1}{N} \\sum_{i,s,c} \\rho(I_{HR}(s, c) - I_p(s, c)) $$ $$ \\rho(x) = \\sqrt{x^2 + 0.001^2}. $$ 4 Experiment Flower dataset and LFVideo dataset\n14 $\\times$ 14 angular samples of size 376 $\\times$ 541. training and testing: central 8 $\\times$ 8 grid of angular samples top-left 320 $\\times$ 512 for training and testing training: 3243 images from Flower and 1080 images from LFVideo testing: 100 images from Flower and 270 images from LFVideo ","permalink":"/notes/crossnet++_cross-scale_large-parallax_warping_for/","summary":"\u003caside\u003e\n\u003ch2 id=\"crossnet-and-crossnet\"\u003e\u003cstrong\u003eCrossNet\u003c/strong\u003e and \u003cstrong\u003eCrossNet++\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eBoth are for \u003cstrong\u003eReference-Based Super-Resolution (RefSR),\u003c/strong\u003e using a \u003cstrong\u003elow-resolution (LR)\u003c/strong\u003e image and a \u003cstrong\u003ehigh-resolution (HR reference)\u003c/strong\u003e image to make a sharper, high-quality output.\u003c/p\u003e\n\u003cp\u003eThe performance of \u003cstrong\u003eCrossNet\u003c/strong\u003e drops with the increasing of perspective parallax, the improvement of \u003cstrong\u003eCrossNet++:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTwo-stage warping\u003c/strong\u003e → improves alignment\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-supervised flow estimation\u003c/strong\u003e → uses \u003cstrong\u003eFlowNet\u003c/strong\u003e to estimate motion between LR and Ref images\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCross-scale alignment\u003c/strong\u003e → Aligns features at \u003cstrong\u003emultiple resolutions\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHybrid loss functions\u003c/strong\u003e → warping + landmark + super-resolution loss\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReal-world performance\u003c/strong\u003e → produces smoother, \u003cstrong\u003esharper\u003c/strong\u003e, and more realistic results, suitable for a variety of scenarios\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003ch1 id=\"abstraction\"\u003eAbstraction\u003c/h1\u003e\n\u003cp\u003eCrossNet++ focuses on \u003cstrong\u003ereference-based super-resolution (RefSR),\u003c/strong\u003e improving a low-resolution image using a high-resolution reference from another camera. This task is hard because of \u003cstrong\u003elarge scale differences (8×)\u003c/strong\u003e and \u003cstrong\u003ebig parallax (~10%)\u003c/strong\u003e between the two views.\u003c/p\u003e","title":"CrossNet++: Cross-Scale Large-Parallax Warping for Reference-Based Super-Resolution"},{"content":"paper resource\nRWKV = a bridge between LSTM and Transformer (less computation and memory, parallel in training). Replace self-attention with a time-mixing mechanism that behaves like an LSTM in time but a Transformer in training and inference xLSTM = scale up the LSTM family to match or even surpass Transformer-level performance. Keep the recurrent spirit of LSTM, but redesign gates and memory (mLSTM) so it can train and scale efficiently like Transformers. Speciality: memory mixing, RWKV Abstract The paper revisits LSTMs, whose key innovations are the constant error carousel and gating mechanisms — originally solved the vanishing-gradient problem and made long-term memory possible. Although Transformers later surpassed LSTMs thanks to their parallelizable self-attention, the authors ask whether LSTMs can be scaled up, like modern LLMs, to billions of parameters while overcoming their known limits.\nTo achieve this, they introduce:\nExponential gating — a new gating function with improved normalization and stability. Modified memory structures: sLSTM — uses scalar memory and scalar updates with new “memory mixing.” → memory mixing mLSTM — introduces matrix-based memory that supports full parallelization and a covariance-based update rule. A new memory architecture → parallelization By stacking these enhanced cells into residual xLSTM blocks, they create architectures that combine the strengths of LSTMs and Transformers.\nExperiments show that xLSTMs can match or even outperform Transformers and State Space Models in both performance and scaling.\n👉 Code: github.com/NX-AI/xlstm\n1 Introduction 1.1 LSTM $$ c_t = f_t c_{t-1} + i_t z_t, \\quad h_t = o_t \\psi(c_t) $$\nUpdate the cell state / long-term memory: $$ c_t = f_t c_{t-1} + i_t z_t $$\n$c_t$: Cell state, real-valued vector, the internal long-term memory after update $f_t$: Forget gate, values in (0, 1), decide how much of $c_{t-1}$ to keep $c_{t-1}$: Previous cell state, vector, carries long-term memory $i_t$: Input gate, values in (0, 1), decides how much new info to write $z_t$: Cell input / candidate memory, usually $\\tanh(\\cdot)$ output, the new content that could be added Produce the output / hidden state / short-term memory: $$ \\quad h_t = o_t \\psi(c_t) $$\n$h_t$: Hidden state, vector, output of the cell (short-term memory) $o_t$: Output gate, values in (0, 1), controls what part of memory is shown outside $\\psi(c_t)$: Activation function (often $\\tanh(c_t$)), squashes memory to bounded range Three Main Limitations of LSTMs Can’t revise stored information Once an LSTM stores something in its cell state, it struggles to update or replace it later. xLSTM fix: introduces exponential gating, allowing flexible updating of stored values. Limited storage capacity Traditional LSTMs store information in a single scalar cell state, forcing compression and loss of details. xLSTM fix: uses a matrix memory, which can hold richer, multi-dimensional information. No parallelization LSTM depends on sequential hidden-to-hidden connections, meaning each step waits for the previous one. xLSTM fix: changes the memory mixing structure to make computation parallelizable across time steps. 2 Extended LSTM Two main modifications: exponential gating and novel memory structures. Two variants mombined into xLSTM blocks, stacked with residual connections to build xLSTM architectures, both can have multiple memory cells and heads: sLSTM – scalar memory, scalar update, memory mixing across cells. mLSTM – matrix memory, covariance (outer product) update, fully parallelizable. 2.2 sLSTM sLSTM = LSTM + exponential gates + normalization state + stabilizer state + multiple memory cells.\nThe exponential gates $i_t$ and $f_t$ make it easier to amplify or reduce memory dynamically.\n→ Helps sLSTM revise stored information better (a key limitation of classical LSTM).\nThe normalizer state $n_t$ keeps things numerically stable, so exponential growth doesn’t blow up.\nThe stabilizer state $m_t$ keeps their scale controlled, prevents numerical overflow during training.\nThe Multiple heads each with its own LSTM-like structure to compute its own $h_t$, then combined, just like multi-head attention in Transformers, allowing the network to learn different kinds of temporal patterns in parallel.\nNew Memory Mixing: In an sLSTM, each time step has multiple memory cells → a vector computed by recurrent matrices R, each cell stores part of the long-term memory, we allow these memory cells to talk to each other.\nMemory mixing = different parts (dimensions) of the memory cells communicate and influence each other.\n2.3 mLSTM mLSTM = LSTM + exponential gates + normalization state + stabilizer state + multiple memory cells.\nmLSTM replaces the small one-number memory $c_t$ of LSTM with a key–value memory matrix, so it can store, search, and update information like attention, but still works as a recurrent network (RNN).\n$q_k, k_t, v_t$ → same like query, key, value in transformer… uses a matrix memory because it wants to store relationships between features (keys and values), not just single values like traditional LSTM. The normalizer state $n_t$ is the weighted sum of key vectors, keeps record of the strength of the gates. Multiple heads and multiple cells are equivalent as there is no memory mixing. 2.4 xLSTM Architecture 2.4.1 xLSTM Blocks Each block takes an input (sequence or features), passes it through an sLSTM or mLSTM cell, adds some non-linear layers (MLPs) and residual/skip connections, finally outputs a transformed sequence representation.\nPatterns are easier to separate after mapping into a higher-dimensional space. Like for better points classification, we can map each point from 2D → 3D.\nWhen an xLSTM processes a sequence, it wants to distinguish different histories, for example:\n“The dog chased the cat” vs “The cat chased the dog.” These sequences may look similar in lower dimensions (both use same words), but when we map them into a higher-dimensional representation, the model can more easily tell them apart.\nSo each xLSTM block:\nexpands data into a higher space (“up-projection”), applies non-linear transformations, and then compresses back (“down-projection”). That makes it easier for the model to separate different contexts or meanings.\nType Memory type Recurrent connections Parallelization Up-proj position Storage capacity sLSTM Post up-projection Scalar memory (vector) ✅ via matrices R ❌ Sequential after LSTM Smaller mLSTM Pre up-projection Matrix memory ❌ No recurrent matrices ✅ parallelizable before LSTM Much larger 2.4.2 xLSTM Architecture Figure 1: The extended LSTM (xLSTM) family.\nFrom left to right:\nThe original LSTM memory cell with constant error carousel and gating. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule. mLSTM and sLSTM in residual blocks yield xLSTM blocks. Stacked xLSTM blocks give an xLSTM architecture. The constant error carousel is the additive update of the cell state $c_{t−1}$ (green) by cell inputs $z_t$ and moderated by sigmoid gates (blue).\nThe gating mechanisms:\nForget gate decides what to erase. Input gate decides what to add. Output gate decides what to show. 4 Experiments LSTM and xLSTM models far outperform Transformers and State Space Models on tasks that need long-term memory and state tracking; xLSTM, especially when combining sLSTM + mLSTM, achieves the best all-around performance, showing that recurrent memory architectures still beat attention models for logical and structured reasoning.\nThe paper uses perplexity (ppl) as the main evaluation metric for language modeling. It measures how well the model predicts the next token in a text sequence.\nThe model is confident and accurate → the model gives high probability to the correct next word → it’s confident → low perplexity. The model is confused and often wrong → the model gives low probability → it’s uncertain or wrong → high perplexity. Scaling Laws Next token prediction perplexity of xLSTM, RWKV-4, Llama, and Mamba on the SlimPajama validation set when trained on 300B tokens from SlimPajama. Model sizes are 125M, 350M, 760M, and 1.3B. The scaling laws indicate that for larger models xLSTM will perform well too.\n5 Limitations sLSTM not parallelizable:\nIts memory mixing prevents full parallel execution. Custom CUDA version is faster, but still ~2× slower than mLSTM. mLSTM kernels not optimized:\nCurrent CUDA implementation is ~4× slower than FlashAttention. Could be improved with better GPU kernels. High computation cost:\nmLSTM processes (d \\times d) matrices, which increases compute load,\nthough it can still be parallelized using standard matrix ops.\nGate initialization sensitivity:\nForget-gate parameters must be tuned carefully for stability. Memory limits at long contexts:\nLarge matrix memory may overload at very long sequence lengths,\nbut works fine up to ~16k tokens.\nNot fully optimized yet:\nArchitecture and hyperparameters weren’t exhaustively tuned due to cost. More optimization could further boost performance. ","permalink":"/notes/xlstm_extended_long_short-term_memory/","summary":"\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2405.04517\"\u003epaper resource\u003c/a\u003e\u003c/p\u003e\n\u003caside\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRWKV\u003c/strong\u003e = a \u003cem\u003ebridge\u003c/em\u003e between LSTM and Transformer (less computation and memory, parallel in training). Replace self-attention with a time-mixing mechanism that behaves like an LSTM in time but a Transformer in training and inference\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003exLSTM\u003c/strong\u003e = s\u003cem\u003ecale up\u003c/em\u003e the LSTM family to match or even surpass Transformer-level performance. Keep the recurrent spirit of LSTM, but redesign gates and memory (mLSTM) so it can train and scale efficiently like Transformers.\n\u003cul\u003e\n\u003cli\u003eSpeciality: memory mixing, \u003cdel\u003eRWKV\u003c/del\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003eThe paper revisits \u003cstrong\u003eLSTMs\u003c/strong\u003e, whose key innovations are the \u003cstrong\u003econstant error carousel\u003c/strong\u003e and \u003cstrong\u003egating mechanisms\u003c/strong\u003e — originally solved the vanishing-gradient problem and made long-term memory possible. Although \u003cstrong\u003eTransformers\u003c/strong\u003e later surpassed LSTMs thanks to their \u003cstrong\u003eparallelizable self-attention\u003c/strong\u003e, the authors ask whether LSTMs can be scaled up, like modern LLMs, to \u003cstrong\u003ebillions of parameters\u003c/strong\u003e while overcoming their known limits.\u003c/p\u003e","title":"xLSTM: Extended Long Short-Term Memory"},{"content":"Abstract Transformers are very powerful for language tasks and training is ****faster on GPUs because of parallization, but they use a lot of memory and computing power, especially when processing long text, their cost grows very fast (quadratically).\nRNNs, on the other hand, use less memory and computation cause they grow linearly but are slower to train and not as good at handling long sentences.\nThe new model RWKV mixes the good parts of both, it trains quickly and gets good performance like a Transformer and also runs efficiently like an RNN.\nMemory and computation:\nIn a Transformer, each token looks at every other token through self-attention. If you have N tokens in a sentence, it compares every pair, so total comparisons = N × N = N². That’s why Memory and computation both grow quadratically. In an RNN, the model reads tokens one by one, passing information step by step. So for N tokens, it just does N steps, the total cost = N (linear). 1 Introduction RWKV reformulates the attention mechanism with a variant of linear attention, replacing traditional dot-product token interaction with more effective channel-directed attention. This implementation, without approximation, offers the lowest computational and memory complexity.\n2 Background 2.1 Recurrent Neural Networks (RNNs) Popular RNN architectures such as LSTM and GRU. Although these RNNs can be factored into two linear blocks (W and U) and an RNN-specific block, the data dependency relying on previous time steps prohibits parallelizing these typical RNNs.\n$$ \\begin{aligned} f_t \u0026= \\sigma_g\\left(W_f x_t + U_f h_{t-1} + b_f\\right), \\\\ i_t \u0026= \\sigma_g\\left(W_i x_t + U_i h_{t-1} + b_i\\right), \\\\ o_t \u0026= \\sigma_g\\left(W_o x_t + U_o h_{t-1} + b_o\\right), \\\\ \\tilde{c}_t \u0026= \\sigma_c\\left(W_c x_t + U_c h_{t-1} + b_c\\right), \\\\ c_t \u0026= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, \\\\ h_t \u0026= o_t \\odot \\sigma_h(c_t). \\end{aligned} $$ Model Depends on Parallelizable? RNN computed previous state ❌ No RWKV raw previous input ✅ Yes (for training) 2.2 Transformers and AFT Standard Transformer self-attention Matrix form\n$$ \\text{Attn}{Attn}(Q,K,V)=\\text{softmax}(QK^\\top)V $$ Per token (t)\n$$ \\text{Attn}{Attn}(Q,K,V)_t=\\frac{\\sum_{i=1}^{T}\\exp(q_t^\\top k_i) \\odot v_i}{\\sum_{i=1}^{T}\\exp(q_t^\\top k_i)} $$ The weighted value equation in multi-head attention. Weight of token $i$ for query $t$ is $\\exp(q_t^\\top k_i)$. Output is a weighted average of $v_i$ AFT (Attention-Free Transformer) variant $$ \\text{Attn}{Attn}^{+}(W,K,V)_t=\\frac{\\sum_{i=1}^{t}\\exp(w_{t,i}+k_i)\\odot v_i}{\\sum_{i=1}^{t}\\exp(w_{t,i}+k_i)} $$ Replace $q_t^\\top k_i$ with (learned) position bias $w_{t,i}$ + a key score $k_i$. Causal: sum only $i\\le t$. Still a normalized weighted average, but weights depend on position via $w_{t,i}$. Variables:\ni: past token index, what we’re reading from memory t: current token index, what we’re generating now RWKV’s simplification Define the bias as a decay with distance:\n$$ w_{t,i}=-(t-i)w \\qquad w\\in(\\mathbb{R}{\\ge 0})^d $$ Per-channel vector $w$ ⇒ older tokens get weight $e^{-(t-i)w}\\le 1$. Now weights depend only on how far back a token is, not on $q_t$. This structure lets us keep running sums, so we don’t need all pairwise scores. Result:\nSame attention-like effect, but computed with O(T) time and O(1) memory per step (like an RNN), while still trainable in parallel across tokens (like a Transformer) by prefix-scan tricks.\nModel Has Query? How it computes weights Complexity Key idea Transformer ✅ Yes all tokens attend to all O(N²) Full pairwise attention, strong but heavy AFT ❌ No Uses position bias O(N) Removes query, uses learned positional weights RWKV ❌ No Adds time-decay weights O(N) AFT idea + RNN-style update (linear, efficient) 3 RWKV Four fundamental elements:\nR: The Receptance vector acts as the receiver of past information. W: The Weight signifies the positional weight decay vector, a trainable parameter within the model. K: The Key vector performs a role analogous to K in traditional attention mechanisms. V: The Value vector functions similarly to V in conventional attention processes. The difference between Time Mix and Channel Mix:\nTime Mix: Builds each token’s vector using time order and previous token info Channel Mix: Refines each token’s vector by mixing internal dimensions (features). It processes each token separately to mix and refine its feature channels. 3.1 Architecture The RWKV model is composed of stacked residual blocks. Each block consists of a time-mixing and a channel-mixing sub-block, embodying recurrent structures to leverage past information.\nThis model uses a unique attention-like score update process, which includes a time-dependent softmax operation improving numerical stability and mitigating vanishing gradients. It ensures that the gradient is propagated along the most relevant path. Additionally, layer normalization incorporated within the architecture aids in stabilizing the gradients, effectively addressing both vanishing and exploding gradient issues.\n3.1.1 Token Shift In this architecture, all linear projection vectors (R, K, V in time-mixing, and R′, K′ in channel- mixing) involved in computations are produced by linear interpolation between current and previous timestep inputs, facilitating a token shift.\nThe vectors for time-mixing computation are linear projections of linear combinations of the current and previous inputs of the block:\n$$ \\begin{aligned} r_t \u0026= W_r \\cdot \\left(\\mu_r \\odot x_t + (1 - \\mu_r) \\odot x_{t-1}\\right), \\\\ k_t \u0026= W_k \\cdot \\left(\\mu_k \\odot x_t + (1 - \\mu_k) \\odot x_{t-1}\\right), \\\\ v_t \u0026= W_v \\cdot \\left(\\mu_v \\odot x_t + (1 - \\mu_v) \\odot x_{t-1}\\right). \\end{aligned} $$ The channel-mixing inputs:\n$$ \\begin{aligned} r'_t \u0026= W'_{r} \\cdot \\left(\\mu'_{r} \\odot x_t + (1 - \\mu'_{r}) \\odot x_{t-1}\\right), \\\\ k'_t \u0026= W'_{k} \\cdot \\left(\\mu'_{k} \\odot x_t + (1 - \\mu'_{k}) \\odot x_{t-1}\\right). \\end{aligned} $$ 3.1.2 WKV Operator $$ wkv_t = \\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} \\odot v_i + e^{u + k_t} \\odot v_t}{\\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} + e^{u + k_t}} $$ The difference of treating W between AFT and RWKV:\nPairwise matrix: Each token pair has its own weight Channel-wise vector: One decay weight per feature channel 3.1.3 Output Gating Time Mixing:\n$$ o_t = W_o \\cdot (\\sigma(r_t) \\odot wkv_t) $$ Channel Mixing:\n$$ o'_t = \\sigma(r'_t) \\odot (W'_v \\cdot \\max(k'_t, 0)^2) $$ 4 Trained Models and Computing Costs Adam optimizer, bfloat16 precision, the auxiliary loss introduced by PaLM…\n4.2 Scaling Laws Scaling laws in language models refer to the mathematical relationships that describe how the performance of a language model changes with respect to various factors.\nScaling laws are important for two primary reasons:\nthey allow us to make predictions and plans regarding the costs and performance of large models before they are trained via interpolation and extrapolation. the contexts in which they fail provides rich feedback on important areas for future research. Explain Interpolation and Extrapolation:\nInterpolation: predicting within the range of data you already know. For example, you trained models with 1B, 5B, and 10B parameters, then you interpolate to guess how a 7B model will perform. Extrapolation: predicting beyond the known range. For example, you trained up to 10B, and now you try to estimate performance of a 100B model. 5 Evaluation Evaluation direction and questions:\nCompetitiveness: Tests if RWKV performs as well as Transformers when both use the same computing power.\nLong Context: Tests if RWKV can handle very long texts better than Transformers,\nespecially when Transformers become too slow or costly for long sequences.\n6 Inference Experiments float32 precision, the HuggingFace Transformers,\n7 Future Work 1. Increase Model Expressivity\nImprove time-decay formulas. Explore better initialization of model states. Goal: more powerful representations without losing efficiency. 2. Improve Computational Efficiency\nUse parallel scan in $wkv_t$ step. Target complexity: $O(B \\log(T)d)$. 3. Apply to Encoder–Decoder Architectures\nReplace cross-attention with RWKV mechanism. Useful for seq2seq and multimodal models. Boosts efficiency in both training and inference. 4. Use of Model State (Context)\nUsed for interpretability and predictability. Could enhance safety and control via prompt tuning. Modify hidden states to guide model behavior. 5. Larger Internal States\nImprove long-term memory and context understanding. Increase performance across various tasks. 8 Conclusion We introduced RWKV, a new approach to RNN models exploiting the potential of time-based mixing components. RWKV introduces several key strategies that allow it to capture locality and long-range dependencies while addressing limitations of current architectures by: (1) replacing the quadratic QK attention with a scalar formulation at linear cost, (2) reformulating recurrence and sequential inductive biases to enable efficient training parallelization and efficient inference, and (3) enhancing training dynamics using custom initializations.\nWe benchmark the proposed architecture in a wide variety of NLP tasks and show comparable performance to SoTA with reduced cost. Further experiments on expressivity, interpretability, and scaling showcase the model capabilities and draw parallels in behavior between RWKV and other LLMs.\nRWKV opens a new route for scalable and efficient architectures to model complex relationships in sequential data. While many alternatives to Transformers have been proposed with similar claims, ours is the first to back up those claims with pretrained models with tens of billions of parameters.\n9 Limitations 1. Performance Limitation (Memory Loss over Long Contexts)\nLinear attention is efficient but may lose fine details over long sequences. RWKV compresses history into a single vector, unlike Transformers that keep all token interactions. Its recurrent design limits ability to fully “look back” at distant tokens. 2. Dependence on Prompt Engineering\nRWKV relies more on well-designed prompts than Transformers. Poor prompts may cause information loss between prompt and continuation. ","permalink":"/notes/rwkv/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003eTransformers are very powerful for language tasks and training is ****faster on GPUs because of \u003cstrong\u003eparallization\u003c/strong\u003e, but they use \u003cstrong\u003ea lot of memory and computing power\u003c/strong\u003e, especially when processing long text, their cost grows \u003cstrong\u003every fast\u003c/strong\u003e (quadratically).\u003c/p\u003e\n\u003cp\u003eRNNs, on the other hand, use \u003cstrong\u003eless memory and computation\u003c/strong\u003e cause they grow linearly but are \u003cstrong\u003eslower to train\u003c/strong\u003e and not as good at handling long sentences.\u003c/p\u003e\n\u003cp\u003eThe new model \u003cstrong\u003eRWKV\u003c/strong\u003e mixes the good parts of both, it trains quickly and gets good performance like a Transformer and also runs efficiently like an RNN.\u003c/p\u003e","title":"RWKV: Reinventing RNNs for the Transformer Era"},{"content":"Abstract The game of Go:\nThe most challenging of classic games for AI, because:\nEnormous search space The difficulty of evaluating board positions and moves Concept Meaning Example in Go AI Solution Enormous search space Too many possible moves and future paths → impossible to explore all At every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities Policy network narrows down the choices (reduces breadth of search) Hard-to-evaluate positions Even if you know the board, it’s hard to know who’s winning Humans can’t easily assign a numeric score to a mid-game position Value network predicts win probability (reduces depth of search) AlphaGo Imagine AlphaGo is a smart player who has:\nintuition → from the policy network judgment → from the value network planning ability → from MCTS Integrating deep neural networks with Monte Carlo Tree Search (MCTS).\nThe main innovations include:\nTwo Neural Networks: Policy Network: Selects promising moves → the probability of each move. Value Network: Evaluates board positions → the likelihood of winning. Training Pipeline: Supervised Learning (SL) from expert human games to imitate professional play. Reinforcement Learning (RL) through self-play, improving beyond human strategies. Integration with MCTS: Combines the predictive power of neural networks with efficient search. Reduces: breadth (number of moves to consider) depth (number of steps to simulate) of search. MCTS It first adds all legal moves (children) under the current position in the tree. In every simulation, AlphaGo chooses one branch to go deeper into the tree (not all of them). It decides which one based on three main metrics: Policy Prior (P) → the probability of the move from policy network Visit Count (N) → how many times we’ve already explored this move during simulations. Q-Value (Q) → average win rate from past simulations Symbol Meaning Source Role in decision P(s,a) Policy prior (initial move probability) From policy network Guides initial exploration N(s,a) Number of times this move was explored Counted during simulations Balances exploration vs exploitation Q(s,a) Average predicted win rate (past experience) From value network results of simulations Exploitation: “keep doing what worked” Results:\nWithout search, AlphaGo already played at the level of strong Go programs. With the neural-network-guided MCTS, AlphaGo achieved a 99.8% win rate against other programs. It became the first program ever to defeat a human professional Go player (Fan Hui, European champion) by 5–0. Introduction Optimal value function $v^*(s)$ For any game state $s$, there exists a theoretical function that tells who will win if both players play perfectly. Computing this function exactly means searching through all possible sequences of moves. Search space explosion Total possibilities ≈ $b^d$, where $b$: number of legal moves (breadth), $d$: game length (depth). For Go: ( $b$ ≈ 250, $d$ ≈ 150) → bigggg number → impossible to compute exhaustively. Reducing the search space — two key principles: (1) Reduce depth using an approximate value function $v(s)$: Stop (truncate) deep search early. Use an approximate evaluator to predict how good a position is instead of exploring all future moves. This worked in chess, checkers, and Othello, but was believed to be impossible (“intractable”) for Go because Go’s positions are much more complex. (2) Reduce breadth using a policy $p(a|s)$: Instead of exploring all moves, only sample the most likely or promising ones. This narrows down which actions/moves to consider, saving enormous computation. Example: Monte Carlo rollouts: Simulate random games (using the policy) to estimate how good a position is. Maximum depth without branching at all, by sampling long sequences of actions for both players from a policy $p$. This method led to strong results in simpler games (backgammon, Scrabble), and weak amateur level in Go before AlphaGo. “Simulate” and “Roll out” basically mean the same thing in this context.\n“Simulate” → a general word: to play out an imaginary game in your head or computer. “Roll out” → a more specific term from Monte Carlo methods, meaning “play random moves from the current position until the game ends.” So → every rollout is one simulation of a complete (or partial) game.\nrollout = one simulated playthrough. Monte Carlo Rollout Monte Carlo rollout estimates how good a position is by:\nStarting from a given board position (s). Playing many simulated games to the end (using policy-guided moves → reduce breadth). Recording each game’s result (+1 for win, −1 for loss). Averaging all outcomes to estimate the win probability for that position. $$ v(s) \\approx \\text{average(win/loss results from rollouts)} $$\nGoal:\nApproximate the value function $v(s)$, the expected chance of winning from position $s$.\nIt’s simple but inefficient — great for small games, too slow and noisy for Go.\nMonte Carlo tree search MCTS uses Monte Carlo rollouts to estimate the value of each state. As more simulations are done, the search tree grows and values become more accurate. It can theoretically reach optimal play, but earlier Go programs used shallow trees and simple, hand-crafted policies or linear value functions (not deep learning). These older methods were limited because Go’s search space was too large. Training pipeline of AlphaGo Deep convolutional neural networks (CNNs) can represent board positions much better. So AlphaGo uses CNNs to reduce the search complexity in two ways: Evaluating positions with a value network → replaces long rollouts (reduces search depth). Sampling moves with a policy network → focuses on likely moves (reduces search breadth). Together, this lets AlphaGo explore much more efficiently than traditional MCTS. Supervised Learning (SL) Policy Network $p_\\sigma$: trained from human expert games. Fast Policy Network $p_\\pi$: used to quickly generate moves during rollouts. Reinforcement Learning (RL) Policy Network $p_\\rho$: improves the SL policy through self-play, optimizing for winning instead of just imitating humans. Value Network $v_\\theta$: predicts the winner from any board position based on self-play outcomes. Final AlphaGo system = combines policy + value networks inside MCTS for strong decision-making. Supervised learning of policy networks Panel(a): Better policy-network accuracy in predicting expert moves → stronger actual gameplay performance.\nThis proves that imitation learning (supervised policy $p_σ$) already provides meaningful playing ability before any reinforcement learning or MCTS.\nFast Rollout Policy networks $p_\\pi(a|s)$\nA simpler and faster version of the policy network used during rollouts in MCTS. Uses a linear softmax model on small board-pattern features (not deep CNN). Much lower accuracy (24.2 %) but extremely fast takes only 2 µs per move (vs. 3 ms for the full SL policy). Trained with the same supervised learning principle on human moves. Reinforcement learning of policy networks Structure of the policy network = SL policy network initial weights ρ = σ Step What happens What’s learned Initialize Copy weights from SL policy (ρ = σ) Start with human-like play Self-play Pick current p and an older version p Generate thousands of full games (self-play) Reward +1 for win, −1 for loss Label each move sequence, and collect experience (state, action, final reward) Update Update weights ρ by SGD Policy network Repeat Thousands of games Stronger, self-improving policy Reinforcement learning of value networks Step What happens What’s learned Initialize Start from the trained RL policy network; use it to generate self-play games Provides realistic, high-level gameplay data Self-play RL policy network plays millions of games against itself Produce diverse board positions and their final outcomes (+1 win / −1 loss) Sampling Randomly select one position per game to form 30 M independent (state, outcome) pairs Avoids correlation between similar positions Labeling Each position (s) labeled with the final game result (z) Links every board state to its real win/loss outcome Training Train the value network (v_θ(s)) by minimizing MSE Learns to predict winning probability directly from a position Evaluation Compare against Monte Carlo rollouts (pπ, pρ) Matches rollout accuracy with 15 000× less computation Result MSE ≈ 0.23 (train/test), strong generalization Reliable position evaluation for use in MCTS Problem of naive approach of predicting game outcomes from data consisting of complete games:\nThe value network was first trained on all positions from the same human games. Consecutive positions were almost identical and had the same win/loss label. The network memorized whole games instead of learning real position evaluation → overfitting (MSE = 0.37 test). Solution\nGenerate a new dataset: 30 million self-play games, take only one random position per game. Each sample is independent, so the network must learn general Go patterns, not memorize. Result: good generalization (MSE ≈ 0.23) and accurate position evaluation. Searching with policy and value networks (MCTS) Panel Step What happens Which network helps a Selection Traverse the tree from root to a leaf by selecting the move with the highest combined score Q + u(P). Uses Q-values (average win) and policy priors P (from policy network). b Expansion When reaching a leaf (a position never explored before), expand it: generate its possible moves and initialize their prior probabilities using the policy network RL policy network c Evaluation Evaluate this new position in two ways: ① Value network (v_θ(s)): predicts win probability instantly. ② Rollout with fast policy p_π: quickly play random moves to the end, get final result (r). Value net + Fast policy d Backup Send the evaluation result (average of (v_θ(s)) and (r)) back up the tree — update each parent node’s Q-value (mean of all results from that branch). None directly (update step) The core idea Each possible move/edge (s, a) in the MCTS tree stores 3 key values:\nSymbol Meaning Source P(s,a) Prior probability — how promising this move looks before searching From the policy network N(s,a) How many times this move has been tried From search statistics Q(s,a) Average win rate from playing move a at state s From past simulations Step 1: Selection — choose which move to explore next At each step of a simulation, AlphaGo selects the move $a_t$ that maximizes:\n$$ a_t = \\arg\\max_a [Q(s_t, a) + u(s_t, a)] $$\nwhere the bonus term $u(s,a)$ encourages exploration:\n$$ u(s,a) \\propto \\frac{P(s,a)}{1 + N(s,a)} $$\n$Q(s,a)$: “How good this move has proven so far.” $u(s,a)$: “How much we should still explore this move.” → Moves that are both good (high Q) and underexplored (low N) get priority.\nAs N increases, the bonus term shrinks — the search gradually focuses on the best moves.\nStep 2: Expansion When the search reaches a leaf (a position not yet in the tree):\nThe policy network $p_\\sigma(a|s)$ outputs a probability for each legal move. Those values are stored as new P(s,a) priors for the new node. Initially $N(s,a) = 0$ $Q(s,a) = 0$ Now the tree has grown — this new node represents a new possible future board.\nStep 3: Evaluation — estimate how good the leaf is Each leaf position $s_L$ is evaluated in two ways:\nValue network $v_θ(s_L)$: directly predicts win probability. Rollout result $z_L$: fast simulation (using the fast rollout policy $p_π$) until the game ends +1 if win −1 if loss. Then AlphaGo combines the two results:\n$$ V(s_L) = (1 - λ)v_θ(s_L) + λz_L $$\n$λ$ = mixing parameter (balances between value net and rollout). If $λ$ = 0.5, both count equally. Step 4: Backup — update the tree statistics The leaf’s evaluation $V(s_L)$ is propagated back up the tree:\nEvery move (edge) $(s, a)$ that was used to reach that leaf gets updated:\n$$ N(s,a) = \\sum_{i=1}^{n} 1(s,a,i) $$\n$$ Q(s,a) = \\frac{1}{N(s,a)} \\sum_{i=1}^{n} 1(s,a,i) V(s_L^i) $$\n$1(s,a,i)$ = 1 if that move was part of the i-th simulation, else 0. $V(s_L^i)$ = evaluation result from that simulation’s leaf. So, Q(s,a) becomes the average value of all evaluations ( $r$ and $vθ$) in its subtree.\nStep 5: Final move decision After thousands of simulations, the root node has a set of moves with:\n$P(s_0, a)$: from policy network, $Q(s_0, a)$: average win rate, $N(s_0, a)$: visit counts. AlphaGo chooses the move with the highest visit count (N) — the most explored and trusted move.\nWhy SL policy network performed better than RL policy network for MCTS?\nPolicy Behavior Effect in MCTS SL policy Mimics human experts → gives a diverse set of good moves MCTS can explore several promising branches efficiently RL policy Optimized for winning → focuses too narrowly on top 1–2 moves MCTS loses diversity → gets less exploration benefit So, for MCTS’s exploration stage, a broader prior (SL policy) performs better.\nBut for value estimation, the RL value network is superior — because it predicts winning chances more accurately.\nImplementation detail Evaluating policy \u0026amp; value networks takes much more compute than classical search. AlphaGo used: 40 search threads, 48 CPUs, 8 GPUs for parallel evaluation. The final system ran asynchronous multi-threaded search: CPUs handle the tree search logic, GPUs compute policy and value network evaluations in parallel. This allowed AlphaGo to efficiently combine deep learning with massive search.\nAll programs were allowed 5 s of computation time per move.\nDiscussion In this work we have developed a Go program, based on a combination of deep neural networks and tree search. We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine.\nSelect those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network. Policy network → “probability of choosing a move”\nValue network → “probability of winning from a position”\n","permalink":"/notes/mastering-go-mcts/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eThe game of Go:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most challenging of classic games for AI, because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnormous search space\u003c/li\u003e\n\u003cli\u003eThe difficulty of evaluating board positions and moves\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConcept\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample in Go\u003c/th\u003e\n          \u003cth\u003eAI Solution\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eEnormous search space\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eToo many possible moves and future paths → impossible to explore all\u003c/td\u003e\n          \u003ctd\u003eAt every turn, Go has ~250 legal moves; across 150 moves → (250^{150}) possibilities\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003ePolicy network\u003c/strong\u003e narrows down the choices (reduces \u003cem\u003ebreadth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eHard-to-evaluate positions\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eEven if you know the board, it’s hard to know who’s winning\u003c/td\u003e\n          \u003ctd\u003eHumans can’t easily assign a numeric score to a mid-game position\u003c/td\u003e\n          \u003ctd\u003e\u003cstrong\u003eValue network\u003c/strong\u003e predicts win probability (reduces \u003cem\u003edepth\u003c/em\u003e of search)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"alphago\"\u003e\u003cstrong\u003eAlphaGo\u003c/strong\u003e\u003c/h2\u003e\n\u003caside\u003e\n\u003cp\u003eImagine AlphaGo is a \u003cem\u003esmart player\u003c/em\u003e who has:\u003c/p\u003e","title":"Mastering the game of Go with MCTS and Deep Neural Networks"},{"content":"Abstract What’s the Reference-based Super-resolution (RefSR) Network:\nSuper-resolves a low-resolution (LR) image given an external high-resolution (HR) reference image The reference image and LR image share similar viewpoint but with significant resolution gap (8×). Solve the problem Existing RefSR methods work in a cascaded way such as patch matching followed by synthesis pipeline with two independently defined objective functions Divide the image into many small patches (like tiny squares), each patch is compared with a reference image to find its most similar region. But every patch makes its decision independently. → Inter-patch misalignment Because of the small misalignments, the grid of patch boundaries in the final image shows. → Grid artifacts Old methods trained two steps separately → Inefficient training The challenge large-scale (8×) super-resolution problem the spatial resolution is increased by 8 times in each dimension (width and height). So the total number of pixels increases from 8×8 to 64×64. patch matching → warping Structure:\nimage encoders extract multi-scale features from both the LR and the reference images cross-scale warping layers spatially aligns the reference feature map with the LR feature map warping module originated from spatial transformer network (STN) fusion decoder aggregates feature maps from both domains to synthesize the HR output Scale Resolution (relative) Example size What it focuses on Scale 0 ×1 (full resolution) 512×512 Fine details (small shifts) Scale 1 ×2 smaller 256×256 Medium motions Scale 2 ×4 smaller 128×128 Larger motions Scale 3 ×8 smaller 64×64 Very large motions Scale 4–5 ×16, ×32 smaller 32×32, 16×16 Extremely coarse view (too little detail) Result Using cross-scale warping, our network is able to perform spatial alignment at pixel-level in an end-to-end fashion, which improves the existing schemes both in precision (around 2dB-4dB) and efficiency (more than 100 times faster).\nspatial alignment at pixel-level → precision and efficiency precision efficiency 1. Introduction The two critical issues in RefSR:\nImage correspondence between the two input images High resolution synthesis of the LR image. The ‘patch maching + synthesis’ pipeline → the end-to-end CrossNet → results comparisons.\n💡 Flow estimator Input: feature maps from LR and Ref encoders.\nComputation:\nThe module compares these features (using convolutions) and learns to predict displacement vectors (optical flow).\nOutput: a flow map $F(x, y) = (\\Delta x, \\Delta y)$.\nUse: the warping layer applies this flow map to the reference feature map:\n$$ \\tilde{R}(x, y) = R(x + \\Delta x, y + \\Delta y) $$\nso the warped reference aligns with the LR image.\nNon-rigid deformation: when an object changes its shape or structure in the image (for example, bending, twisting, or changing due to different camera angles).\nRigid = only simple shifts, rotation, or scaling. Non-rigid = more complex distortions — like bending, stretching, or perspective change. Grid artifacts: visible blocky or checker-like patterns that appear because the image was reconstructed from many small, rigid square patches that don’t align smoothly.\nGrid artifacts occur when an image is reconstructed from many small square patches that don’t align perfectly at their borders. The Laplacian is a mathematical operator that measures how much a pixel value differs from its surroundings.\nIn other words, it tells you where the image changes quickly — that’s usually at edges or texture details.\n2. Related Work Multi-scale deep super resolution we employ MDSR as a sub-module for LR images feature extraction and RefSR synthesis.\nMDSR stands for Multi-scale Deep Super-Resolution Network Used for Feature extraction → understanding what’s in the LR image RefSR synthesis → combining LR and reference features to output the high-resolution result Warping and synthesis We follow such “warping and synthesis” pipeline. However, our approach is different from existing works in the following ways:\nOur approach performs multi-scale warping on feature domain at pixel-scale which accelerates the model convergence by allowing flow to be globally updated at higher scales. a novel fusion scheme is proposed for image synthesis. concatenation, linearly combining images 3. Approach 3.1 Fully Conv Cross-scale Alignment Module It is necessary to perform spatial alignment for reference image, since it is captured at different view points from LR image.\nCross-scale warping We propose cross-scale warping to perform non-rigid image transformation.\nOur proposed cross-scale warping operation considers a pixel-wise shift vector ( V ):\n$$ I_o = warp(y_{Ref}, V) $$\nwhich assigns a specific shift vector for each pixel location, so that it avoids the blocky and blurry artifacts.\n💡 Pixel-wise shift vector (V) → patch matching\n( V ) represents a flow field, where each pixel gets its own small movement vector (Δx, Δy). A flow field is a map (like a vector field) that assigns a motion vector to every pixel in the image. So instead of moving the entire image or patch, CrossNet can move each pixel individually — very flexible. The equation:\n$$ I_o = warp(y_{Ref}, V) $$\nmeans:\nThe output image ( $I_o$ ) is generated by warping the reference image ( $y_{Ref}$ ) according to the flow field ( $V$ ).\nEach pixel in ( $y_{Ref}$ ) is shifted by its corresponding vector in ( $V$ ).\nCross-scale flow estimator Purpose: Predict pixel-wise flow fields to align the upsampled LR image with the HR reference.\nModel: Based on FlowNetS, adapted for multi-scale correspondence.\nInputs:\n$I_{LR↑}$ : LR image upsampled by MDSR (SISR) $I_{REF}$ : reference image Outputs: Multi-scale flow fields\n${V^{(3)}, V^{(2)}, V^{(1)}, V^{(0)}}$ (coarse → fine).\nModification: ×4 bilinear upsampling with → two ×2 upsampling modules + skip connections + deconvolution → finer, smoother flow prediction.\nAdvantage:\nCaptures both large and small displacements; Enables accurate, non-rigid alignment Reduces warping artifacts. 💡 How it works (coarse-to-fine refinement)\nThe coarse flow field (V³) roughly aligns big structures. The next flow (V²) refines alignment for medium details. The fine flows (V¹, V⁰) correct small local misalignments and textures. These flow fields are combined hierarchically — like zooming in step-by-step to improve precision. 3.2 End-to-end Network Structure Network structure of CrossNet\n💡 Network:\na LR image encoder a reference image encoder a decoder → U-Net LR image Encoder Goal: Extract multi-scale feature maps from the low-resolution (LR) image for alignment and fusion.\nStructure:\nUses a Single-Image SR (SISR) upsampling to enlarge the LR image first. Then applies 4 convolutional layers (5×5 filters, 64 channels). Each layer creates a feature map at a different scale (0 → 3). Stride = 1 for the first layer, stride = 2 for deeper ones (downsampling by 2). Output:\nA set of multi-scale LR feature maps. $$ F_{LR}^{(0)}, F_{LR}^{(1)}, F_{LR}^{(2)}, F_{LR}^{(3)} $$\nActivation: ReLU (σ).\nReference image encoder Goal: Extract and align multi-scale reference features from the HR reference image.\nStructure:\nUses the same 4-scale encoder design as the LR encoder. Produces feature maps . $$ {F_{REF}^{(0)}, F_{REF}^{(1)}, F_{REF}^{(2)}, F_{REF}^{(3)}} $$\nLR and reference encoders have different weights, allowing complementary feature learning. Alignment:\nEach reference feature map $F_{REF}^{(i)}$ is warped using the cross-scale flow $V^{(i)}$. This generates spatially aligned reference features $$ \\hat{F}{REF}^{(i)} = warp(F_{REF}^{(i)}, V^{(i)}) $$\nDecoder Goal: Fuse the low-resolution (LR) features and warped reference (Ref) features across multiple scales to reconstruct the super-resolved (SR) image. Structure Overview The decoder follows a U-Net–like architecture. It performs multi-scale fusion and up-sampling using deconvolution layers. Each scale combines: The LR feature at that scale $F_{LR}^{(i)}$, The warped reference feature $\\hat{F}_{REF}^{(i)}$, The decoder feature from the next coarser scale $F_{D}^{(i+1)}$ (if available). 💡 Equations (Eq. 6)\nFor the coarsest scale (i = 3):\n$$ F_{D}^{(3)} = \\sigma \\big( W_{D}^{(3)} \\star (F_{LR}^{(3)}, \\hat{F}{REF}^{(3)}) + b{D}^{(3)} \\big) $$\nFor finer scales (i = 2, 1, 0):\n$$ F_{D}^{(i)} = \\sigma \\big( W_{D}^{(i)} \\star (F_{LR}^{(i+1)}, \\hat{F}{REF}^{(i+1)}, F{D}^{(i+1)}) + b_{D}^{(i)} \\big) $$\nwhere:\n$\\star$ denotes the deconvolution operation (transposed convolution). $W_{D}^{(i)}$: deconvolution filters (size 4×4, 64 filters, stride 2). $\\sigma$: activation function (ReLU). $b_{D}^{(i)}$ : bias term. Thus, features are progressively upsampled and refined from coarse → fine.\n💡 Post-Fusion (Eq. 7)\nAfter obtaining the final decoder feature map $F_{D}^{(0)}$,\nthree convolutional layers (filter size 5×5) are applied to refine and generate the SR image:\n$$ \\begin{aligned} F_1 \u0026amp;= \\sigma(W_1 * F_{D}^{(0)} + b_1), \\ F_2 \u0026amp;= \\sigma(W_2 * F_1 + b_2), \\ I_p \u0026amp;= \\sigma(W_p * F_2 + b_p), \\end{aligned} $$\nwhere:\n$W_1, W_2, W_p$: convolution filters with channel numbers {64, 64, 3}, $I_p$: final super-resolved output image. 3.3 Loss Function Goal: Train CrossNet to generate super-resolved (SR) outputs $I_p$ close to the ground-truth HR images $I_{HR}$.\nFormula:\n$$ L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{s} \\rho(I_{HR}^{(i)}(s) - I_{p}^{(i)}(s)) $$\nPenalty: Uses the Charbonnier loss\n$$ \\rho(x) = \\sqrt{x^2 + 0.001^2} $$\nA smooth, robust version of L1 loss that reduces the effect of outliers.\nVariables:\n$N$: number of training samples $s$: pixel (spatial location) $i$: training sample index 4. Experiment 4.1 Dataset Dataset: The representative Flower dataset and Light Field Video (LFVideo) dataset. Each light field image has: 376 × 541 spatial samples 14 × 14 angular samples Model training: Each light field image has: 320 × 512 spatial samples 8 × 8 angular samples Test generalization: Datasets: Stanford Light Field dataset Scene Light Field dataset During testing, they apply the big input images using a sliding window approach: Window size: 512×512 Stride: 256 4.2 Evaluation Training setup: Trained for 200K iterations on Flower and LFVideo datasets. Scale factors: ×4 and ×8 super-resolution. Learning rate: 1e-4 / 7e-5 → decayed to 1e-5 / 7e-6 after 150K iterations. Optimizer: Adam (β₁ = 0.9, β₂ = 0.999). Comparisons: Competes with RefSR methods (SS-Net, PatchMatch) and SISR methods (SRCNN, VDSR, MDSR). Evaluation metrics: PSNR, SSIM, and IFC on ×4 and ×8 scales. Reference images from position (0,0); LR images from (1,1) and (7,7). Results: CrossNet achieves 2–4 dB PSNR gain over previous methods. CrossNet consistently outperforms the resting approaches under different disparities, datasets and scales. Quantitative evaluation of the sota SISR and RefSR algorithms, in terms of PSNR/SSIM/IFC for scale factors ×4 and ×8 respectively.\nMetric Meaning PSNR (Peak Signal-to-Noise Ratio) Measures reconstruction accuracy (higher = clearer, less error). SSIM (Structural Similarity Index) Measures structural similarity to the ground truth (higher = more visually similar). IFC (Information Fidelity Criterion) Evaluates how much visual information is preserved (higher = better detail). Generalization During training, apply a parallax augmentation procedure this means they randomly shift the reference image by –15 to +15 pixels both horizontally and vertically. The purpose is to simulate different viewpoint disparities (parallax changes) make the model more robust to viewpoint variations. They initialize the model using parameters pre-trained on the LFVideo dataset, Then re-train on the Flower dataset for 200 K iterations to improve generalization. The initial learning rate is 7 × 10⁻⁵, which decays by factors 0.5, 0.2, 0.1 at 50 K, 100 K, 150 K iterations. Table 2 and 3 show PSNR comparison results: Their re-trained model (CrossNet) outperforms PatchMatch [11] and SS-Net [2] on both Stanford and Scene Light Field datasets. The improvement is roughly +1.79 – 2.50 dB (Stanford) and +2.84 dB (Scene LF dataset). Efficiency within 1 seconds machine: 8 Intel Xeon CPU (3.4 GHz) a GeForce GTX 1080 GPU 4.3 Discussion Flows at scale 0–3 were coherent (good). Flows at scale 4–5 were too noisy — because those very small maps (like 32×32) lost too much information. Training setup:\nTrain both CrossNet and CrossNet-iw with the same procedure: Pre-train on LFVideo dataset Fine-tune on Flower dataset 200 K iterations total Additionally, CrossNet-iw pretraining: Pre-train only the flow estimator WS-SRNet using an image warping task for 100 K iterations. Train the whole network jointly for another 100 K iterations. FlowNetS+ adds extra upsampling layers\nIn the original FlowNetS: The final flow map is smaller than the input (maybe ¼ or ½ resolution). This is fine for rough alignment but loses small motion details. In FlowNetS+: They add extra upsampling layers so the final flow map is finer (closer to full size). That’s why it aligns better — it can describe tiny pixel movements more accurately. Downsampling = make smaller. Upsampling = make bigger.\n","permalink":"/notes/crossnet_an_end-to-end_reference-based_super_resol/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003cp\u003eWhat’s the \u003cstrong\u003eReference-based Super-resolution (RefSR)\u003c/strong\u003e Network:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSuper-resolves a \u003cstrong\u003elow-resolution (LR)\u003c/strong\u003e image given an external \u003cstrong\u003ehigh-resolution (HR) reference image\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe reference image and LR image share similar viewpoint but with significant resolution gap (8×).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"solve-the-problem\"\u003eSolve the problem\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eExisting RefSR methods work in a cascaded way such as \u003cstrong\u003epatch matching\u003c/strong\u003e followed by \u003cstrong\u003esynthesis pipeline\u003c/strong\u003e with two independently defined objective functions\n\u003cul\u003e\n\u003cli\u003eDivide the image into many small \u003cstrong\u003epatches\u003c/strong\u003e (like tiny squares), each patch is compared with a \u003cstrong\u003ereference image\u003c/strong\u003e to find its most similar region.\u003c/li\u003e\n\u003cli\u003eBut every patch makes its decision independently. → \u003cstrong\u003eInter-patch misalignment\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eBecause of the small misalignments, the \u003cstrong\u003egrid\u003c/strong\u003e of patch boundaries in the final image shows. → \u003cstrong\u003eGrid artifacts\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eOld methods trained \u003cstrong\u003etwo steps separately → Inefficient training\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe challenge large-scale (8×) super-resolution problem\n\u003cul\u003e\n\u003cli\u003ethe \u003cstrong\u003espatial resolution is increased by 8 times\u003c/strong\u003e in each dimension (width and height).\u003c/li\u003e\n\u003cli\u003eSo the total number of pixels increases from 8×8 to 64×64.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003epatch matching → warping\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStructure:\u003c/p\u003e","title":"CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping"},{"content":" Chain of thought:\nA series of intermediate natural language reasoning steps that lead to the final output — significantly improves the ability of large language models to perform complex reasoning. Chain-of-thought prompting:\nA simple and broadly applicable method for enhancing reasoning in language models. Improving performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Two simple methods to unlock the reasoning ability of LLMs Thinking in steps helps:\nWhen the model explains each step before the answer, it understands the problem better.\nLearning from examples:\nWhen we show a few examples with step-by-step answers, the model learns to do the same.\nFew-shot prompting means giving the model a few examples in the prompt to show it how to do a task before asking it to solve a new one.\nWhat this paper do Combine these two ideas help the language models think step by step to generate a clear and logical chain of ideas that shows how they reach the final answer. Given a prompt that consists of triples: \u0026lt;input, chain of thought, output\u0026gt; Why this method is important It doesn’t need a big training dataset. One model can do many different tasks without extra training. Greedy decoding: let the model choose the most likely next word each time\nResult It only works well for giant models, not smaller ones. It only works well for more-complicated problems, not the simple ones. Chain-of-thought prompting with big models gives results as good as or better than older methods that needed finetune for each task. Ablation study: It’s like testing which parts of your model really matter.\nRelated work Some methods make the input part of the prompt better — for example, adding clearer instructions before the question. But this paper does something different (orthogonal): it improves the output part, by making the model generate reasoning steps (chain of thought) before giving the final answer. ","permalink":"/notes/chain-of-thought-prompting/","summary":"\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eChain of thought:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA series of intermediate \u003cstrong\u003enatural language reasoning steps\u003c/strong\u003e that lead to the final output — significantly improves the ability of large language models to perform complex reasoning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChain-of-thought prompting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple and broadly applicable method for\n\u003cul\u003e\n\u003cli\u003eenhancing reasoning in language models.\u003c/li\u003e\n\u003cli\u003eImproving performance on a range of \u003cstrong\u003earithmetic\u003c/strong\u003e, \u003cstrong\u003ecommonsense\u003c/strong\u003e, and \u003cstrong\u003esymbolic\u003c/strong\u003e\nreasoning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/aside\u003e\n\u003chr\u003e\n\u003ch2 id=\"two-simple-methods-to-unlock-the-reasoning-ability-of-llms\"\u003eTwo simple methods to unlock the reasoning ability of LLMs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThinking in steps helps:\u003c/strong\u003e\u003c/p\u003e","title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"},{"content":"Initial Impression 👉🏼 The light field refres to the representation of a scene. Unlike the tranditional 2D image, it adds an extra dimention: the direction of light. This means each image captures not only the length and width, but also the direction of light.\nTraditional image: 2D spatially (width, height), but each pixel carries a 3-value vector (RGB, 3 channels for color). Light filed(width, heidht, X-direction, Y-direction): 4D images, but each pixel carries a 3-value vector. As a result, light field data is massive and challenging to process.\nTo address it, researchers have turned into machine learning to efficiently handle and enhance light field imaging.\nThe paper mentions 4 main areas that AI can helps:\nDepth estimation Reconstruction Compression Quelity evaluation Abstract 👉🏼 Content:\nBackground Motivation Research focus Paper content the existing learning-based solutions frameworks evaluation methods datasets future research directions Background Traditional cameras capture only 2D images. Light field imaging records the direction of light rays, enabling realistic and immersive 3D-like representations. Motivation Light field data provides richer visual information but comes with large data volume and high computational cost. Machine learning and deep networks offer efficient and intelligent ways to process this data. Research Focus Learning-based methods are applied to:\nDepth estimation Reconstruction and super-resolution Compression and quality enhancement Paper Goals The paper surveys existing learning-based techniques, summarizes the most promising frameworks, reviews current datasets, and evaluation methods and outlook for future research directions.\n1. Introduction 👉🏼 Content:\nConcept and potential of light field imaging Market growth and application fields Technical challenges and processing tasks Shift toward learning-based solutions Purpose and structure of the paper Concept and Potential Light field imaging is a promising 3D imaging technology that records light rays traveling through every point in space and in every direction. Unlike 2D photography, it captures angular information, providing a sense of depth, realism, and immersion. This enables photo-realistic rendering and supports 6-DoF (Degrees of Freedom) experiences for next-generation immersive media, broadcasting, and gaming. Market and Applications The light field market is expanding rapidly, driven by glasses-free 3D displays and multi-view visualization systems. It supports a range of applications such as virtual reality, augmented reality, 3D reconstruction, and computational photography. Challenges and Processing Tasks High-dimensional light field data introduces issues like data redundancy, storage complexity, and inter-view correlation. Essential processing tasks include: Spatial and angular super-resolution to enhance image quality Compression algorithms for efficient data storage and transmission Depth estimation for 3D scene reconstruction These tasks are more complex than traditional 2D image processing due to added angular dimensions. Shift Toward Learning-Based Solutions Traditional geometry-based methods struggle with large datasets and occlusion problems. Deep learning and data-driven frameworks now dominate, improving efficiency and performance in reconstruction, compression, and depth estimation. Learning frameworks enable automation and scalability, addressing the computational challenges of high-dimensional data. Purpose and Structure of the Paper This review provides a comprehensive overview of learning-based solutions for light field imaging. It discusses: Fundamentals of light field imaging and data acquisition Key processing tasks and related learning frameworks Benchmark datasets and evaluation methods Current challenges and future research directions The goal is to summarize progress, identify open issues, and provide a roadmap for future research in learning-based light field processing. 2. Light field imaging background 👉🏼 Content:\nLight field fundamentals Light field acquisition Light field visualization 2.1 Light Field Fundamentals 👉🏼 Content:\nConcept of the Plenoptic Function Dimensional Reduction Light Field Representation Concept of the Plenoptic Function To reproduce realistic 3D scenes, cameras must capture light from many viewpoints.\nA light field describes all light rays in 3D space — their positions, directions, colors, and intensity.\nThe complete description is given by the plenoptic function, a 7-dimensional (7D) function:\n$$ P(θ, φ, λ, τ, V_x, V_y, V_z) $$\nwhich includes direction, wavelength, time, and position of every ray in space.\nDimensional Reduction Capturing the full 7D plenoptic function is practically impossible. Under constant lighting and static scenes, wavelength (λ) and time (τ) can be ignored. This simplifies the representation to a 5D function: $$ P(θ, φ, V_x, V_y, V_z) $$\nThe 5D form forms the foundation for Neural Radiance Fields (NeRF), while further simplification leads to a 4D light field representation. Light Field Representation Two-Plane Parameterization\nThe 4D light field assumes light rays travel in straight lines. Each ray is defined by its intersection with two parallel planes: (u, v) → spatial coordinates on the image/focal plane (Ω) (s, t) → coordinates on the camera plane (Π) The resulting function: $$ P(u, v, s, t) $$\ndefines the light field in terms of spatial and angular information. This two-plane parameterization makes light fields easier to store, process, and compress using 2D image arrays. A 4D light field can be expressed as an array of 2D images indexed by (u, v) for spatial coordinates and (s, t) for different viewpoints. This enables the use of standard 2D codecs for compression and native 2D algorithms for processing. Epipolar Plane Image (EPI)\nEpipolar Plane Image (EPI), a 2D slice of the light field showing spatial–angular relationships. Looks like a single 2D picture made by stacking the same pixel row from all those camera views. Coordinates → (u, s) or (v, t) (only one spatial + one view direction) In the EPI, each object in the scene becomes a slanted line: If an object is close, its line is steeper (because it moves more between views). If it’s far, its line is flatter (moves less). Lines in the EPI correspond to scene depth — analyzing their slopes allows depth estimation and 3D reconstruction. Epipolar Plane Image (EPI)\n2.2 Light Field Acquisition 👉🏼 Acquisition methods:\nSingle-Camera Systems Multi-Camera Arrays Other Methods Computer-generated light fields Handheld or SLAM-based systems LiDAR-assisted capture Overview Light fields can be acquired by single plenoptic cameras, camera arrays, or synthetic and LiDAR-based systems, each balancing data density, resolution, and complexity.\nSingle-Camera Systems Plenoptic (lenslet-based) cameras use microlens arrays to capture dense angular information in one shot. Examples: Raytrix and Lytro cameras. Lytro → The Lytro camera captures a 4-D light field by recording many tiny viewpoint images through its microlens array — those images form the SAI stack. Multi-Camera Arrays Arrays of monocular cameras (planar or spherical) capture scenes from different viewpoints. The camera layout defines the angular sampling and overall field of view. Other Methods Computer-generated light fields provide accurate depth maps for benchmarking. Handheld or SLAM-based systems reconstruct light fields from multiple frames. LiDAR-assisted capture combines sensors and cameras for precise, automated 3D data. 2.3 Light field visualization 👉🏼 Content:\nGoal and use cases Visualization approaches Display principles and challenges Perceptual limitations Goal and Use Cases The main goal is to provide a true 3D visual experience, essential for immersive and interactive applications. Visualization may be passive (no interaction) or active (viewer can move or rotate objects). Used in areas like medical imaging, VR/AR, and 3D display systems. Visualization Approaches Based on the 4D light-field representation combining spatial and angular data. Passive use cases render fixed views; active use cases synthesize new views via view interpolation. (View interpolation means creating new viewpoints images that were never actually captured by a real camera, but are mathematically generated from nearby real ones. Rendering quality strongly influences perceptual realism. Display Principles and Challenges Two key properties: Angular resolution (baseline between views) the baseline = the distance between two camera viewpoints. Small baseline → cameras are close together Wide baseline → cameras are far apart Spatial resolution (image detail) 3D displays must reproduce directional light rays accurately to recreate depth and realism. Capturing dense samples and rendering multiple view angles remain computationally demanding. Perceptual Limitations Standard 2D displays lack realistic cues like vergence and accommodation, limiting immersion. Light-field displays address this by replicating real light rays to the viewer’s eyes but face: Finite ray sampling Vergence–accommodation conflict Restricted field of view (FoV) Horizontal- or vertical-parallax-only designs causing viewing discomfort. 3. Learning‑based light field processing 👉🏼 Content:\nDepth estimation Autoencoders Stereo matching and refinement End‑to‑end feature extraction and disparity regression Light field reconstruction Spatial super‑resolution Single‑view super‑resolution and refinement End‑to‑end residual learning Angular super‑resolution EPI super‑resolution Depth estimation and warping Multi‑plane image generation Spatio‑angular reconstruction Neural scene representation Compression Learning‑based view synthesis on the decoder side Learning‑based view synthesis on the encoder and decoder sides End‑to‑end light field compression architecture Other light field imaging applications This section summarizes the most prominent light field processing tasks studied in the literature and highlights the learning-based imaging techniques deployed for each processing task.\nTerms and explanation:\nTerm Simple meaning Example / Analogy EPI volume Stack of many light-field slices showing how points move across views Like stacking many thin image strips to form a 3D block Stereo view Two or more photos of the same scene from different angles Like your left and right eye views SAI stack Group of small 2D images from a light field camera Like a grid of mini photos from slightly different directions Depth map Image showing distance of each pixel (white = near, black = far) Like a 3D scanner’s output Disparity volume 3D data showing pixel shifts between views Large shift → close object, small shift → far object Details about the difference between Light Field representation:\nTerm Relation to Two-Plane Parameterization Simple meaning EPI A slice through the Two-Plane representation. Shows pixel movement (as slanted lines) between multiple views — used for depth estimation. Stereo view it’s a simpler / smaller subset of the Two-Plane model (only 2 views). Like taking only the left and right images from the light field. SAI stack it’s directly sampled from the Two-Plane model (a grid of (s, t) views, each with its own (u, v) image). Many small 2D images from slightly different viewpoints — a practical way to store the light field. Relationship between Disparity volume and Depth map:\nConcept Type What it represents Used for Disparity volume 3D data (x, y, disparity) All possible matching shifts between views Intermediate step (to find the best match) Depth map 2D image (x, y), 3D data (x, y, depth) Actual distance of each pixel Final result Disparity and depth are inversely related:\n$$ \\text{Depth} = \\frac{f \\times B}{\\text{Disparity}} $$\nwhere:\n(f) = focal length of the camera, (B) = distance between two camera views (baseline). So:\nLarge disparity (big shift) → close object. Small disparity (tiny shift) → far object.\nDepth map\n3.1 Depth estimation 👉🏼 Three main approaches:\nAutoencoders Stereo matching and refinement End-to-end feature extraction and disparity regression Depth estimation model architecture\nOverview Goal: Estimate the distance of each pixel from the camera to recover the scene’s 3D structure. Light field imaging enables capturing a scene from multiple viewpoints so depth information is implicitly encoded in the light field representation and can be acquired by computing the inter-view pixel disparity information. disparity volume → depth map Main challenges: occlusions, non-Lambertian surfaces, and texture-less regions, which make accurate estimation difficult. A Lambertian surface is an ideal matte surface — it reflects light equally in all directions. That means no matter where you look from, the brightness of that point stays the same. Recent progress focuses on learning-based approaches, achieving higher accuracy than traditional geometry-based methods. Autoencoders Take an EPI volume → compress it (encoder) → learn the hidden features → expand it (decoder).\nClassical method:\nHeber \u0026amp; Pock: five-block CNN estimating line orientation in EPIs; Orientation refers to the direction each camera is facing. later extended to a U-shaped encoder–decoder, Further to U-shaped encoder–decoder with skip connections and 3D filters. Step Explanation Input Horizontal and vertical EPI volumes (from the light field). Each EPI shows slanted lines — the slope encodes depth. Model A 5-block CNN that scans small “windows” (patches) of the EPI. Each CNN layer extracts features and estimates the orientation (slope) of the EPI lines. Later versions evolved into a U-shaped encoder–decoder (U-Net) for better reconstruction. Output A depth map, where every pixel’s value = estimated distance from the camera. Alperovich et al.:\nan autoencoder that encodes horizontal and vertical EPI stacks simultaneously using six stages of residual blocks to improve robustness. Then, the compressed representation is expanded using three decoder pathways to address the disparity, diffusion, and specularity estimation problems. Step Explanation Input Combined horizontal + vertical EPI stacks from the light field. Model An autoencoder with: 6 residual blocks (for stronger feature extraction) in the encoder, and 3 decoder branches (to handle disparity, diffusion, specularity). It compresses the light field into a latent code, then reconstructs richer outputs. Output A disparity volume — a 3D array where each pixel position stores multiple disparity hypotheses (possible shifts). From this volume, a final depth map can be derived later. Analysis:\nPros: captures compact depth features, handles EPI geometry directly. Cons: computationally heavy; limited to 2D EPI slices, less effective in occluded regions. Stereo Matching and Refinement Computes disparity between SAIs using neural stereo-matching networks.\nTypical pipeline:\nCoarse disparity estimation via networks like FlowNet 2.0 or encoder–decoder CNNs. Refinement using residual or occlusion-aware learning to correct depth errors. Examples:\nRogge et al. (belief propagation + residual refinement);\nGuo et al. (encoder–decoder concatenation of SAIs).\nAnalysis:\nPros: exploits full 4D light-field correlations, good for complex geometry. Cons: high computation cost; sensitive to reflections and non-Lambertian surfaces. End-to-End Feature Extraction and Disparity Regression Fully end-to-end CNNs learn features and regress depth directly.\nMethods:\nEpinet: Horizontal, vertical, and diagonal SAI stacks → Multi-stream CNN feature extraction → Regression network → Depth map Leistner et al.: Vertical \u0026amp; horizontal SAI stacks → Siamese U-Net → Autoencoder regression module → Classification + regression fusion → Depth map Two-stream CNN: Horizontal \u0026amp; vertical EPIs → Multi-scale feature extraction (four convolutional stages) → Feature concatenation → Multi-label regression → Depth map Zhu et al.: Focal stacks + center view + EPIs → Hybrid feature extraction → Fully connected + softmax layers → Pixel-wise disparity classification → Depth map Tsai et al.: Multi-view SAIs → Residual blocks + spatial pyramid pooling → Cost volume construction + attention module → Disparity regression → Depth map Multi-scale Cost-Volume Method: Shifted SAI feature maps (multi-disparity levels) → 4D cost volume (low memory footprint) → Multi-scale feature extraction → Regression → Depth map Analysis:\nPros: highest accuracy, unified optimization of feature + disparity learning. Cons: large data/training demand; reduced performance on wide-baseline scenes. 👉🏼 Summary:\nDepth estimation for the wide baseline scenario, with an acceptable trade-off between accuracy and computation, is still an open research problem.\n3.2 Light field reconstruction 👉🏼 Content:\nSpatial super‑resolution Single‑view super‑resolution and refinement End‑to‑end residual learning Angular super‑resolution EPI super‑resolution Depth estimation and warping Multi‑plane image generation Spatio‑angular reconstruction Neural scene representation To enable higher spatial and angular resolutions, the development of light field reconstruction/ super-resolution (SR) methods has gained significant attention.\nSpatial SR = make each image (each view) sharper — more pixels, more detail. Angular SR = make more viewpoints — fill in missing views between existing ones. 💡 Spatial super-resolution (SR) focuses on improving the static spatial resolution — that is, the sharpness, clarity, and detail of each individual sub-aperture image (still view).\nAngular consistency ensures smooth and coherent transitions between different viewpoints, maintaining stable motion perception and correct 3D geometry when the view changes or the scene is refocused.\nLight-field SR must improve both:\nSpatial resolution → each view looks higher spatial resolution. Angular consistency → all views agree about geometry and depth. System Spatial resolution (image sharpness) Angular resolution (number of viewpoints) Baseline Notes Plenoptic camera Low High (dense) Narrow Compact but blurry Camera rig High Low (sparse) Wide Sharp but heavy and complex Spatial super‑resolution Examples of architectures for the spatial, angular, and spatio-angular super-resolution (SR) frameworks.\na → Single-view SR using single image super-resolution (SISR) network and inter-view enhancement, b → end-to-end residual learning, c → warping and residual learning for refinement, d → multi-plane image generation, e → residual learning using 4D CNNs and refinement, **** f → GAN-based method Single‑view super‑resolution and refinement\nStep Explanation Input (SAI stack LR) SAI stack LR: all the sub-aperture images captured by the light-field camera. low-resolution — each view is blurry and lacks details (because each microlens only captures a small number of pixels). For example, 9×9 SAIs of 60×60 pixels. SISR Single-Image Super-Resolution: This block applies a 2D super-resolution network (e.g., VDSR) to each SAI individually to ****improve the spatial resolution. HR(init)LF High-Resolution (initial) Light Field: Each image looks sharper, but the views may no longer be perfectly aligned (angular inconsistency). EPI EPIs (Epipolar Plane Images) are extracted. EPIs capture the geometric relationships between views — how pixels shift across viewpoints (slanted lines). Refinement module This is a learning network (often a CNN) that analyzes the EPIs to find and correct misalignments between neighboring SAIs. Making sure all views agree in depth and structure. Residual addition Combine sharpness from the first stage and consistency from the second stage. Output (HR LF) High-Resolution Light Field: The final output light field has: High spatial resolution (sharp individual views), and High angular consistency (smooth geometry across views). For example, 9×9 SAIs of 240×240 pixels. End‑to‑end residual learning\nStep Explanation Input (SAI stack) SAI stacks (Horizontal, Vertical, Diagonal, Center View), Center view → the middle image, used as a geometric reference Feature Extraction Each SAI stack is passed through a feature extraction CNN. Extract features from each SAI direction, like “what the object looks like” and “how it moves between views.” Feature Integration \u0026amp; Processing The feature maps from all directions (horizontal, vertical, diagonal, etc.) are merged or fused here. Upsampling Network Increases the spatial resolution (i.e., number of pixels). Output (HR LF) The final output is a high-resolution light field — a grid of SAIs that are: Spatially sharper and Angularly consistent. Upsampling means making an image larger — that is, increasing its resolution by creating more pixels.\nAngular super‑resolution Angular Super-Resolution (SR) means synthesizing new in-between views to make the light field smoother and more complete. (view synthesis)\nEPI super‑resolution\nAll of these models have the same basic structure:\nStart from a blurry / low-angular-resolution light field (few viewpoints). Use deep networks to predict high-frequency details (the fine geometry and textures missing in the low version). Reconstruct the high-angular-resolution (HR) light field, which includes the new views. Step Explanation Input (SAI stack LR) SAI stack LR: all the sub-aperture images captured by the light-field camera. low-resolution — each view is blurry and lacks details (because each microlens only captures a small number of pixels). For example, 9×9 SAIs of 60×60 pixels. SISR Single-Image Super-Resolution: This block applies a 2D super-resolution network (e.g., VDSR) to each SAI individually to ****improve the spatial resolution. HR(init)LF High-Resolution (initial) Light Field: Each image looks sharper, but the views may no longer be perfectly aligned (angular inconsistency). EPI EPIs (Epipolar Plane Images) are extracted. EPIs capture the geometric relationships between views — how pixels shift across viewpoints (slanted lines). Refinement module This is a learning network (often a CNN) that analyzes the EPIs to find and correct misalignments between neighboring SAIs. Making sure all views agree in depth and structure. Residual addition Combine sharpness from the first stage and consistency from the second stage. Output (HR LF) High-Resolution Light Field: The final output light field has: High spatial resolution (sharp individual views), and High angular consistency (smooth geometry across views). For example, 9×9 SAIs of 240×240 pixels. Depth estimation and warping\nDisparity is the shift of the same object’s position between two different views. Warping = using disparity to reposition pixels. Step Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Depth Estimator Predicts a depth map (distance information) for each SAI or for the entire light field. It learns how far each pixel is from the camera by analyzing geometric cues across views. (wraps_by_an_amount_that_depends_on_its_depth) Warping Module Uses the estimated depth maps to warp (geometrically align) all SAIs toward a common viewpoint — usually the center view. “Warping” means shifting pixels according to their depth so that corresponding points from different views line up. Refinement Module Fine-tunes the warped images using a CNN. Corrects small errors from imperfect depth estimation or warping. Output (HR LF) The final reconstructed light field: All SAIs are now high-resolution (sharp textures). The views are geometrically aligned (consistent depth perception). Some basics:\nConcept Meaning Why it matters Depth map Tells how far each pixel is Needed to compute correct pixel shift Disparity The actual shift caused by viewpoint change Derived from depth Warping Moves pixels according to disparity Aligns all views Without depth All pixels shift equally → wrong alignment Causes blur and ghosting Multi‑plane image generation (MPI generation)\nStep Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Warping Module The Warping Module aligns all SAIs to a reference view (usually the center view) using a range of hypothetical depth planes. Plane-sweep volume After warping, stacks all these reprojected images together — forming a plane-sweep volume. Create a 3D data structure that encodes how well each depth hypothesis aligns across views. No explicit depth estimator — depth is implicitly encoded in the plane-sweep volume. 3D CNN The 3D convolutional neural network processes the plane-sweep volume to analyze spatial and depth correlations. Multi-plane image A multi-plane image (MPI) — a set of 2D images, each representing a scene layer at a specific depth, with color + transparency (α) values. Blending Module Combines (blends) all the multi-plane images (depth layers) into a single coherent high-resolution light field. Output (HR LF) The final light field: All SAIs are now high-resolution and geometrically aligned. Spatio‑angular reconstruction Residual learning using 4D CNNs and refinement\nStep Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Warping Module The Warping Module aligns all SAIs to a reference view (usually the center view) using a range of hypothetical depth planes. 4D CNN A convolutional network that operates directly on the 4D light-field volume. It jointly learns: Spatial features (edges, textures inside each SAI) and Angular features (parallax). The 4D CNN predicts the missing high-frequency details for all views simultaneously. Residual Connection This “residual learning” means the 4D CNN only learns the difference (the missing fine details) rather than reconstructing the entire image from scratch. Refinement Module Polish the HR LF and ensure angular consistency. Output (HR LF) Output: HR LFThe final output = high-resolution light field → same number of views as the input, → each view sharper (higher spatial resolution) and smoothly aligned (angular consistency). GAN-based method\nStep Explanation Input (SAI stack) A set of low-resolution sub-aperture images (SAIs) Generator The generator learns to upsample the SAIs and restore missing details (edges, textures, angular consistency). GAN Discriminator It is trained on real HR light fields (ground truth) and fake HR light fields (from the generator). Its job is to classify them as real or fake. Force the generator to create results that are indistinguishable from real data — not just pixel-wise accurate but also visually realistic (better textures, depth edges, lighting). Output (HR LF) The generator alone can take any new LR light field and output an HR version that: has high spatial detail, maintains angular consistency, and looks visually realistic (not over-smoothed). 💡 Summary: Limitations of LF Reconstruction Techniques: Early reconstruction methods Slow to run. Trained for fixed view sampling patterns → hard to generalize to new setups. Single-view SR methods (Fig. 3a) Each sub-aperture image (SAI) is processed separately. Causes geometric inconsistency between views because inter-view information isn’t used. Depth-based \u0026amp; warping methods (Fig. 3c) Work better for wide-baseline cases (larger view spacing). Depend heavily on accurate depth maps → errors lead to tearing, ghosting, and problems with non-Lambertian (reflective) surfaces. MPI-based methods (Fig. 3d) Memory-hungry and slow to train (often need multiple GPUs and days of training). Model size grows with number of depth planes (larger depth budget → bigger model). Can assign wrong opacity to layers → causes blurry reconstructions. 4D CNN methods (Fig. 3e) Produce high-quality results, but have high computational cost due to expensive 4D convolutions. GAN-based methods (Fig. 3f) Need large training datasets. Training can suffer from instability and mode collapse (generator producing limited or repetitive outputs). Neural scene representation Researchers started using neural networks to represent 3D scenes which replaces 2D pictures.\nFrom image-based to neural 3D representations\nEarlier works used image-based rendering (combine nearby views). Recent advances use neural networks to represent and render 3D scenes directly. 3D representations can be: Explicit: meshes, voxels, point clouds. Implicit: continuous functions learned by networks + differentiable ray marching. NeRF — the core idea\nNeural Radiance Fields (NeRF) represent a scene using an MLP (multi-layer perceptron). The network takes 5D coordinates → (x, y, z, θ, φ): spatial position + viewing direction. It outputs: Density (geometry) Color (view-dependent radiance). Rendering is done by volume rendering along camera rays. 💡 How NeRF learns the scene:\nFeed the network 2D photos of the same scene taken from different camera angles, and you must know exactly where each camera was (its pose = position + orientation). → So NeRF knows which pixel in which image corresponds to which ray in 3D space. NeRF renders its current guess (density and color) of those images using its internal 3D representation. It compares them to the real photos (pixel by pixel). It updates its weights using backpropagation to minimize the difference. To render one image (for a camera view):\nFor each pixel, shoot a ray through the 3D scene.\nSample many points along that ray (like tiny steps through space).\nAt each point, ask the network for its color and density.\nCombine (accumulate) all samples along the ray using a differentiable volume rendering formula:\n$$ C = \\sum_i T_i (1 - e^{-\\sigma_i \\Delta_i}) c_i $$\nwhere:\nC = final pixel color, $\\sigma_i$ = density, $c_i$ = color, $T_i$ = how much light passes through before reaching this point. This process is known as differentiable ray marching.\nSo the process works like this:\nCollect multiple 2D photos of the same scene, taken from different camera angles (with known position and orientation: x, y, z, θ, φ). For each pixel in each photo, sample many 3D points along the corresponding camera ray. For each sampled point, use the network to predict its color and density (or depth). Aggregate the information from all sampled points to estimate the final color of the pixel. Compare the predicted image to the real photo, pixel by pixel, and update the model based on the error. NeRF uses a coarse network (for rough geometry) and a fine network (for detailed structure). Analysis: Pros: realistic view synthesis. Cons: very slow training and rendering. Methods to improve speed and efficiency\nSeveral approaches speed up NeRF by changing how the scene is represented, they replaced the big neural network with simpler structures:\nMethod Representation Key idea Pros / Cons Voxel-based NeRFs (Fridovich et al.) Sparse voxel grid Store opacity + spherical harmonic coefficients Faster, but memory-heavy TensoRF 4D tensors (low-rank decomposition) Factorize radiance field into compact tensor components Efficient, compact SNeRG Sparse 3D voxel grid with color \u0026amp; features Encodes view-dependent effects Fast rendering; still large memory Octree NeRF (Yu et al.) Octree structure (adaptive voxels) Sample dense regions more finely Faster, but higher memory cost NeX Extended MPI (Multi-Plane Image) Model color as function of viewing angle using spherical bases Better view-dependent rendering Ray-space embedding 4D ray embedding → latent space Compact feature embedding Memory efficient but slower rendering KiloNeRF Thousands of tiny MLPs Divide scene into grid cells, each with a small network Great speed, coarse structure 3D Gaussian representation Continuous Gaussians Skip empty-space computation Near real-time rendering Instant-NGP (Müller et al.) Hash-based encoding Store features in hash table for fast lookup Very fast, compact, GPU-friendly Methods to improve quality\nVariant Key idea Benefit Mip-NeRF Multi-scale cones for anti-aliased rendering Handles different resolutions \u0026amp; reduces aliasing NeRF++ Two networks: near-field \u0026amp; far-field (spherical) Better for unbounded, complex scenes Mip-NeRF 360 Extends Mip-NeRF for unbounded scenes; uses nonlinear parameterization \u0026amp; regularization High-quality large-scene rendering Trade-offs and current challenges:\nFast methods (gaussian) → train quickly, but lose visual quality. High-quality methods (NeRF, Mip-NeRF 360) → photorealistic but slow to train. Explicit methods also can’t be optimized directly with gradients → need to convert trained implicit NeRFs into their format (adds complexity). Real-time + high-quality rendering remains an open research challenge. 3.3 Compression 💡 Three main strategies:\nLearning-based view synthesis on the decoder side; Learning-based view synthesis on the encoder and decoder sides; End-to-end light field compression architecture; An efficient codec should be able to explore:\nnot only the spatial and angular redundancies independently (as two-dimensional data), but also the combined spatial–angular redundancy (4D data). The key idea of this compression architecture is bitrate saving by sparsely encoding the views.\nLearning-based view synthesis on the decoder side Step Explanation Input light fields These are the original dense light-field data, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints. Key SAIs selection Instead of sending all the views, we select a few key SAIs. These key SAIs contain enough angular and spatial information to reconstruct the missing ones later. Encoder The encoder compresses these key SAIs into a bitstream (binary data) for transmission or storage. Bitstream This is the compressed data that’s transmitted or saved. It contains only the encoded information of the key SAIs (no non-key views). Decoder The decoder reconstructs the key SAIs from the bitstream. Learning-based view synthesis This is a deep neural network trained to synthesize new views (non-key SAIs) from the nearby key SAIs. Generate all non-key views → fill in the gaps to recreate the full light field. Decoded light fields Produce and output the complete high-quality light field (same size as the original) after combining Decoded key SAIs and Generated non-key SAIs. Learning-based view synthesis on the encoder and decoder sides Idea:\nEncode only a few key views (the main ones), Use deep learning to predict or reconstruct the missing views (non-key views), Send only the residual errors to refine those predictions. Step Explanation Input light fields These are the original dense light-field data, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints. Key SAIs and Non-key SAIs The system splits the light field into: Key SAIs: a few representative images selected for transmission (e.g., every 3rd or 4th view). Non-key SAIs: the remaining views that will be predicted rather than transmitted. Encoder Takes two inputs: Key SAIs and Residuals (for the non-key SAIs), compresses both into a compact bitstream (binary data) for transmission or storage. Learning-based view synthesis (encoder side) This neural network predicts the non-key SAIs from the available key SAIs. so the encoder can calculate the prediction error (residual) → the difference between the predicted and real non-key SAIs is computed as a residual. Residue The encoder subtracts the predicted non-key SAI (from the network) from the actual non-key SAI. only encode the difference, not the entire image — saves lots of bitrate. Bitstream The data sent or stored — contains compressed key SAIs + residuals. Decoder Receives and decompresses the bitstream. Reconstructs the key SAIs first. Then passes them to the learning-based view synthesis network to generate predicted non-key SAIs. Learning-based view synthesis (decoder side) Same (or similar) neural network as on the encoder side. It uses the decoded key SAIs to synthesize (predict) the non-key SAIs. Then it adds the residual (decoded correction data) to refine those synthesized views. Addition (+) and output The synthesized non-key SAIs are added to the decoded residuals → final accurate non-key views. Combine key + non-key SAIs → get the fully decoded light field. 💡 Su et al. →\nInstead of treating every pixel separately, they group light rays that belong to the same 3D point in the scene. After grouping rays, some nearby super-rays may still be very similar. So they merge them into larger super-rays to save even more space. Compress each super-ray using a 4D Discrete Cosine Transform (DCT). a light field varies in both: Spatial dimensions (x, y) — inside each image, Angular dimensions (u, v) — across different viewpoints. Better captures 4D spatial–angular redundancy but is more complex. End-to-end light field compression architecture Step Explanation Input light fields These are the original dense light-field data, meaning many sub-aperture images (SAIs) captured from slightly different viewpoints. Encoder This is the main compression network.It takes the input light field and learns to represent it using fewer numbers (features). Bitstream The bitstream is the final compressed data produced by the encoder. It contains the quantized latent features. Decoder The decoder takes the bitstream and reconstructs (decodes) the light field. It performs the reverse of the encoder: expands the compact features back into full-resolution sub-aperture images. Decoded Light Fields These are the reconstructed sub-aperture images (SAIs).Ideally, they look almost identical to the input views, with small errors due to compression. End-to-end schemes are gaining more attention due to their effectiveness in image compression.\n💡 View synthesis drawbacks can be circumvented by neural representations that achieve a level of detail that is challenging for traditional methods.\n3.4 Other light field imaging applications Light-field (LF) images are more powerful than normal 2D photos because they capture depth, focus, and parallax — this allows better performance in many computer vision tasks.\nDeep learning methods are now being used to apply light-field data to various new areas.\nSaliency Detection (SOD) Goal: detect which objects or regions attract human attention. LF advantage: provides both spatial and angular information, giving richer clues about object boundaries and depth. Typical model: encoder–decoder two-stream networks: One stream uses all-in-focus (center) images, The other stream uses focal stacks or multi-view features. A comprehensive review compares deep LF-SOD models with standard RGB-D models. Face Recognition Goal: identify faces more accurately using multi-view data from light fields. LF advantage: combines intra-view (within one image) and inter-view (across multiple angles) features. Methods: VGG features with LSTM layers to model view changes. Capsule networks with a pose matrix to handle viewpoint shifts. Datasets introduced: LFFW (Light Field Faces in the Wild), LFFC (Light Field Face Constrained) — for benchmarking LF face recognition. Light Field Microscopy Goal: use LF imaging to capture and reconstruct 3D biological structures quickly. LF advantage: captures 3D spatial information in one camera shot → instant 3D imaging. Deep learning usage: improves speed and quality of reconstructions. Methods: Encoder–decoder networks convert 2D LF inputs to 3D volume data. Networks with 2D and 3D residual blocks enhance reconstruction quality. Convolutional sparse coding (CSC) networks use EPIs as input for fast neuron localization. Applications: real-time visualization of cardiovascular or neuronal activity. The network uses EPIs as inputs and generates sparse codes, representing depth data, as outputs. Other applications Image classification — improves feature learning using angular cues. Low-light imaging — LF data helps reconstruct clear images in dark conditions. Overall, these works show that learning-based light-field imaging provides richer 3D understanding and better accuracy than normal 2D or RGB-D methods.\n4. Datasets and quality assessment Datasets Characteristics of the light field datasets used to benchmark the light field imaging systems\nQuality Assessment Light field imaging algorithms are typically evaluated using quantitative methods by comparing generated data to a ground-truth. Due to the diversity of light field acquisition procedures, distortions, and rendering processes, light field quality assessment remains a challenging task. A recent focus has been on developing more accurate objective algorithms that extract features from both spatial and angular domains for light field quality assessment. Metrics can be classified into three categories based on the availability of the reference image: full-reference (FR), reduced-reference (RR), no-reference (NR). Developing NR metrics are gaining more attention due to their success in improving accuracy. Current learning-based light field algorithms are still only evaluated using conventional PSNR and SSIM methods. In this context, the IEEE established a new standard called ’IEEE P3333.1.4’, which defines metrics and provides recommended practices for light field quality assessment. A standardization activity, namely ’JPEG Pleno Quality Assessment’, was recently initiated within the JPEG committee aiming to explore the most promising subjective quality assessment practices as well as the objective methodologies for plenoptic modalities in the context of multiple use cases. Summary of the objective quality assessment methods for light fields\n5. Discussion, challenges and perspectives While parallel to light fields, other plenoptic modalities like point cloud and holography have also been developed. Even though point clouds and holographic content processing and compression have advanced significantly in recent years, these content types may eventually need to be converted to light field views for visualization on the display. Light fields provide more comprehensive information when it comes to capturing scenes. Light fields capture not only the 3D objects but also the entire scene information, which can be essential in many applications like autonomous driving, that require accurate 3D recreation of the vehicle surroundings. Recent advances in using deep learning for spatio-angular reconstruction and the emergence of the NeRF-based approaches. More recent methods, such as 3D Gaussian splatting, show improvements in the quality–speed trade-off. Enhancing the quality–speed trade-off could enable new use cases such as real-time telepresence and robotic tasks with fewer views for reconstruction. Two neural scene representations of light fields: an explicit representation based on multiple SAIs an implicit neural representation encodes light fields as parameters of an MLP. Advances in deep learning frameworks are expected to significantly improve the performance of light field processing algorithms and solve the existing challenges. Better depth estimation or depth-free approaches are critical. Advanced methods are needed to efficiently exploit the huge amount of redundant information about the light rays in the same scene that conveys angular and spatial information Now targeting the creation of a learning-based coding standard to provide competitive compression efficiency compared to state-of-the-art light field coding solutions. The evaluation of light field imaging systems has several shortfalls related to the content and assessment approaches available. Advanced methods are needed to efficiently exploit the huge amount of redundant information about the light rays in the same scene that conveys angular and spatial information. It is essential to provide more comprehensive light field datasets from both the quantitative and content diversity perspectives. The assessment of plenoptic image quality also faces various challenges because of the variety of quality aspects and complexity of the content when compared to the assessment of 2D images. JPEG has begun developing a light field quality assessment standard, defining a framework with subjective quality assessment protocols and objective quality assessment procedures for lossy decoding of light field data within the context of multiple use cases. The IEEE is also developing a standard called “P3333.1.4—Recommended Practice for the Quality Assessment of light field Imaging” that targets to establish methods of quality assessment of light field imaging based on psychophysical studies 6. Conclusions 👉🏼 Content:\nCore Focus Progress and trends Challenges Future outlook Core Focus Main Tasks Reviewed:\nDepth estimation, reconstruction, super-resolution, and compression. Other Tasks:\nMicroscopy, saliency, face recognition, refocusing, and relighting also benefit from learning-based methods. Progress and Trends Deep Learning Integration:\nAI frameworks now appear in almost every stage of light field processing. Growth Drivers:\nBetter capture and display hardware and larger datasets will accelerate progress. Challenges Limited Realism:\nCurrent systems have narrow Field of View (FoV) and Depth of Field (DoF); Still far from true 6-DoF free-view exploration. Data Burden:\nExpanding datasets increase computational cost and reduce processing efficiency. Future Outlook Compression Evolution:\nLearning-based image compression is expected to greatly improve light field storage and transmission, making real-world applications more feasible. ","permalink":"/notes/learningbased_light_field_imaging/","summary":"\u003ch1 id=\"initial-impression\"\u003eInitial Impression\u003c/h1\u003e\n\u003caside\u003e\n👉🏼\n\u003cp\u003eThe light field refres to the representation of a scene. Unlike the tranditional 2D image, it adds an extra dimention: the direction of light. This means each image captures not only the length and width, but also the direction of light.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTraditional image: \u003cstrong\u003e2D spatially\u003c/strong\u003e (width, height), but each pixel carries a \u003cstrong\u003e3-value vector\u003c/strong\u003e (RGB, 3 channels for color).\u003c/li\u003e\n\u003cli\u003eLight filed(width, heidht, X-direction, Y-direction): \u003cstrong\u003e4D images\u003c/strong\u003e, but each pixel carries a \u003cstrong\u003e3-value vector.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs a result, light field data is massive and challenging to process.\u003c/p\u003e","title":"Learning‑based light field imaging"},{"content":"1. Abstract Background:\nsummarization → Traditional RAG works well for specific questions (“When was Company X founded?”), but it struggles with broad, global ones (“What are the main ideas in all these documents?”). scalability → (Such questions need summarization of the whole dataset, not just retrieving a few passages — that’s called query-focused summarization (QFS).) Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. we need to combine scalability and summarization: combines knowledge graph generation and query-focused summarization GraphRAG,\na graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text.\nBuild a graph index in two stages:\nderive an entity knowledge graph from the source documents. a knowledge graph (nodes = entities, edges = relationships) pre-generate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user.\n1. Introduction GraphRAG,\nuses an LLM to construct a knowledge graph a knowledge graph, nodes correspond to key entities in the corpus and edges represent relationships between those entities. it partitions the graph into a hierarchy of communities of closely related entities, before using an LLM to generate community-level summaries. GraphRAG answers queries through map-reduce processing of community summaries. In the map step → the summaries are used to provide partial answers to the query independently and in parallel, In the reduce step → the partial answers are combined and used to generate a final global answer. GraphRAG contrasts with vector RAG (text embeddings) in its ability to answer queries that require global sensemaking over the entire data corpus.\n2. Background Adaptive Benchmarking → the process of dynamically generating evaluation benchmarks tailored to specific domains or use cases.\nGenerating test questions based on the current knowledge base. Measuring how well the model adapts when the corpus changes. Evaluating both retrieval and generation quality together. 3. Methods The high-level data flow of the GraphRAG approach and pipeline:\nCommunity detection is used to partition the graph index into groups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time.\nnum of duplicates → edge weights claims → similarity Entities \u0026amp; Relationships → Knowledge Graph\nComponent Purpose Typical Technique (as described or implied) LLM extraction Identify entities/relations/claims Prompt-based, few-shot examples Entity matching Merge identical names Exact string match (default), fuzzy possible Graph construction Store nodes/edges Simple adjacency list or NetworkX graph Edge weighting Track frequency of relationships Count duplicates Aggregation \u0026amp; summarization Produce node/edge descriptions LLM summarization Community detection Find clusters Leiden algorithm (modularity optimization) For a given community level, the global answer to any user query is generated as follows:\nPrepare community summaries. Community summaries are randomly shuffled and divided into chunks of pre-specified token size. This ensures relevant information is distributed across chunks, rather than concentrated (and potentially lost) in a single context window. Map community answers. Intermediate answers are generated in parallel. The LLM is also asked to generate a score between 0-100 indicating how helpful the generated answer is in answering the target question. Answers with score 0 are filtered out. Reduce to global answer. Intermediate community answers are sorted in descending order of helpfulness score and iteratively added into a new context window until the token limit is reached. This final context is used to generate the global answer returned to the user. ","permalink":"/notes/from_local_to_global_a_graphrag_approach_to_query-/","summary":"\u003ch1 id=\"1-abstract\"\u003e1. Abstract\u003c/h1\u003e\n\u003cp\u003eBackground:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003esummarization →\u003c/strong\u003e Traditional RAG works well for \u003cem\u003especific\u003c/em\u003e questions (“When was Company X founded?”), but it struggles with \u003cem\u003ebroad\u003c/em\u003e, \u003cem\u003eglobal\u003c/em\u003e ones (“What are the main ideas in all these documents?”).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003escalability →\u003c/strong\u003e (Such questions need \u003cstrong\u003esummarization of the whole dataset\u003c/strong\u003e, not just retrieving a few passages — that’s called \u003cstrong\u003equery-focused summarization\u003c/strong\u003e (QFS).) Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems.\u003c/li\u003e\n\u003cli\u003ewe need to combine \u003cstrong\u003escalability\u003c/strong\u003e and \u003cstrong\u003esummarization:\u003c/strong\u003e combines knowledge graph generation and query-focused summarization\u003c/li\u003e\n\u003c/ol\u003e\n\u003caside\u003e\n\u003cp\u003e\u003cstrong\u003eGraphRAG\u003c/strong\u003e,\u003c/p\u003e","title":"From Local to Global A GraphRAG Approach to Query-"},{"content":"Abstract Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. pre-trained models non-parametric memory differentiable access mechanism - In soft differentiable access mechanism, we don’t discard any chunks. - In Hard retrieval (standard RAG), the retriever picks the top-k passages We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. pre-trained models → seq2seq model non-parametric memory → a dense vector index of Wikipedia differentiable access mechanism → a pre-trained neural retriever 1. Prompt (question) arrives. 2. Seq2seq encoder turns it into query vector q. 3. Retriever compares q to all memory keys k_i (Wikipedia passage vectors). 4. Compute similarity scores s_i = q ⋅ k_i. 5. Apply softmax → attention weights α_i. 6. Read vector r = Σ α_i v_i (weighted mixture of passage info). 7. Feed r (plus q) into seq2seq decoder → generate answer token by token. 8. Gradients flow through α_i → retriever learns to attend to more relevant chunks. text chunk → retriever encoder → key/value → FAISS index → query embedding → top-k retrieval → generator We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. same retrieved passages → RAG-Sequence different passages per token → RAG-Token It’s often used for knowledge-intensive tasks, not free-form story generation. Discussion We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining.\nThis is one of RAG’s biggest advantages over standard language models: - You can update its knowledge base without retraining its parameters. The retriever learns the mapping → parametric The index just holds the results → non-parametric Retriever model A neural network that encodes queries and documents into vectors. Parametric — it has learnable weights (parameters) Retrieval index (memory) The database of all document embeddings (keys + values) Non-parametric — stored outside the model’s paras Index = structure that accelerates similarity search using ANN methods (cluster-and-search) (ANN -\u0026gt; Approximate nearest neighbor). Key = pre-computed document embedding; Value = original text (encoded later when used). Hard retrieval = pick top-k texts → concatenate → generator sees text. Soft retrieval = mix all embeddings by attention → generator sees one read vector. Generator (e.g., BART or T5): a Transformer-based seq2seq model. 1. Introduction RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\nWe can make an anology: - RAG\u0026#39;s retriever is like encoder, because it summarizes what info the model should pay attention to before generation. - RAG\u0026#39;s generator is like decoder, because it generate the sequence token-by-token. Steps The retriever (Dense Passage Retriever, henceforth DPR) provides latent documents conditioned on the input, The seq2seq model (BART) then conditions on these latent documents together with the input to generate the output. 2. Methods Our models leverage 2 components:\na retriever $p_η(z|x)$ a generator $p_θ(y_i|x, z, y_{1:i−1})$ x = the query (e.g., a question or a sentence you want to search with) z = a text passage (a possible relevant document) η = the parameters of the model/retriever **p(z∣x) = the prob that passage z is relevant to the query x** --- y1:i-1 = the previous i-1 tokens z = the retrieved passage x = the original input **pθ(yi|x, z, y1:i−1) = the prob that generating token yi, give three inputs.** We propose 2 models (based on the average of the latent documents in different ways to produce a distribution over generated text) :\nRAG-Token → can predict each target token based on a different doc/chunk. RAG-Sequence → the model uses the same doc/chunk to predict each target token. 2.1 Models RAG-Sequence Model: The RAG-Sequence model uses the same retrieved doc/chunk to generate the complete seq.\nRAG-Token Model: we can draw a different latent document for each target token and marginalize accordingly.\n2.2 Retriever: DPR We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. We refer to the document index as the non-parametric memory.\nDPR (Dense Passage Retriever): a bi-encoder architecture: d(z) = a dense representation of a document produced by a BERT document encoder. q(x) = a query representation produced by a query encoder, also based on BERT. MIPS (Maximum Inner Product Search) → The operation of finding top-k documents by inner product between query and every docs. 2.3 Generator: BART We use BART-large, a pre-trained seq2seq transformer with 400M parameters. We simply concatenate the input x and the retrieved content z.\nBART combines the strengths of BERT and GPT: - BERT: bidirectional understanding (encoder) - GPT: left-to-right generation (decoder) 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved.\nUpdating the document encoder **BERTd** during training is costly as it requires the document index to be periodically updated as **REALM** does during pre-training. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder **BERTq** and the **BART generator**. BERTd = document encoder BERTq = query encoder REALM = Retrieval-Enhanced Adaptive Language Model update the doc encoder required re-encoding all documents every few steps — which made it extremely slow and hard to scale. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate $arg max_y p(y|x)$.\nRAG-Token Model: standard beam search RAG-Sequence Model: Thorough Decoding or Fast Decoding An autoregressive model = predicts the next token based on all previous tokens. - Thorough Decoding = Generate and score candidate answers for every retrieved document, then combine their probabilities - most accurate but slow. - Fast Decoding = Only score candidates that were actually generated during beam search, skipping others — much faster but approximate. RAG-Token 在生成过程中，模型会参考每个 chunk 下的条件概率分布：$p_\\theta(y_i \\mid x, z, y_{1:i-1})$ 来预测下一个 token 的可能性。 然后根据检索器给出的每个 chunk 的权重 $p_\\eta(z|x)$，对这些分布进行加权融合，得到一个综合的下一词概率分布：$p\u0026rsquo;(y_i \\mid x, y_{1:i-1}) = \\sum_z p_\\eta(z|x),p_\\theta(y_i \\mid x, z, y_{1:i-1})$ 模型从这个融合分布中选出概率最高的 token，再将其加入到已生成的序列中。 重复该步骤，直到生成完整句子。 💡 This makes generation very fast, but because it can borrow inconsistent or partially incorrect evidence from different chunks, the final sentence may contain blended or wrong facts, even though the decoding itself is efficient.\nRAG-Sequence (Thorough Decoding) 先在每个 chunk 下独立运行 beam search，得到概率最高的候选句子； 然后将这些候选句分别在其他 chunk 上重新计算生成概率 $p_\\theta(y|x,z)$（使用 teacher forcing 强制生成）， 最后根据每个 chunk 的检索权重 $p_\\eta(z|x)$ 对句子概率进行加权求和：$p(y|x) = \\sum_z p_\\eta(z|x),p_\\theta(y|x,z)$ 最终选出整体概率最高的句子作为输出。 💡 This “global reconsideration” allows the model to filter out wrong or inconsistent sentences and select the most related one overall. However, because it must compute the probability of every candidate on every chunk, the process is extremely slow.\nRAG-Sequence (Fast Decoding) 先在每个 chunk 下生成概率最高的候选句子， 但只在生成过该句子的 chunk上计算概率， 未生成该句子的 chunk 直接忽略（认为概率≈0）， 再进行同样的加权求和。 💡 This method is a trade-off between the two. It still generates separate sentences for each chunk, but it skips the expensive re-evaluation on other chunks—only using the chunks that actually produced each sentence.\nAs a result, it’s much faster than thorough decoding while keeping almost the same accuracy, though it’s still slower than RAG-Token.\nComparison Item RAG-Token RAG-Sequence (Thorough) RAG-Sequence (Fast) Fusion Timing Dynamically fuses predictions from all chunks at each token Uses a fixed chunk for the whole sentence, then re-evaluates globally Uses a fixed chunk for the whole sentence, then re-evaluates locally Fusion Granularity Token-level Sentence-level Sentence-level Decoding Method Single beam search Multiple beam searches + full re-evaluation Multiple beam searches + partial re-evaluation Cross-chunk Generation ✅ Allowed ❌ Not allowed ❌ Not allowed Accuracy Medium Highest High Speed Fast Slow Faster Typical Usage Common for online inference Mainly theoretical analysis / small-scale experiments Practical trade-off in real applications Probability Computation Sum across chunks at each token Sum across chunks after full sentence generation Sum across chunks after full sentence generation Core Idea Fuse multiple chunk predictions at every step Generate each sentence independently, then globally combine Generate each sentence independently, then combine locally Key Characteristics Each word leverages all chunks — very fast but may produce inconsistent sentences Theoretically most accurate but computationally slow Approximate yet efficient — widely used in practice 3. Experiments For all experiments:\nNon-parametric knowledge source: the December 2018 dump Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M docs. Build a single MIPS index using FAISS with a Hierarchical Navigable Small World approximation for fast retrieval. During training:\nWe retrieve the top k documents for each query. We consider k ∈ {5, 10} for training and set k for test time using dev data. 3.1 Open-domain Question Answering Compare with:\nThe extractive QA paradigm – extracts short answer spans directly from retrieved documents, relying mainly on non-parametric knowledge. The Closed-Book QA approaches – generate answers without retrieval, depending only on parametric knowledge stored in the model. Consider four popular open-domain QA datasets:\nNatural Questions (NQ) TriviaQA (TQA) WebQuestions (WQ) CuratedTrec (CT) (CT and WQ are small; models are initialized from the NQ-trained RAG model.)\nEvaluate:\nPerformance is measured using Exact Match (EM) – a metric that checks whether the generated answer exactly matches the reference answer. 💡 Focus: finding facts from retrieval, not writing sentences.\n3.2 Abstractive Question Answering Evaluate:\nThe MSMARCO NLG v2.1 task, which tests RAG’s ability to generate free-form, natural language answers in a knowledge-intensive setting. Setup:\nEach example includes a question, ten gold retrieved passages, and a full-sentence human-written answer. RAG ignores the supplied passages and treats MSMARCO as an open-domain QA task (retrieving from Wikipedia instead). Note:\nSome questions cannot be answered correctly without the gold passages (e.g., “What is the weather in Volcano, CA?”). In such cases, RAG relies on its parametric knowledge to generate reasonable responses. 💡 Focus: natural, fluent language generation (NLG).\n3.3 Jeopardy Question Generation Task:\nGiven an answer entity, generate a factual Jeopardy-style question (reverse QA). Dataset:\nSearchQA, with 100K train / 14K dev / 27K test examples. Compare:\nRAG vs BART (baseline model). Evaluate:\nQ-BLEU-1 metric (favors entity matching and factual accuracy). Human evaluation on two criteria: Factuality — whether the question is factually correct. Specificity — whether the question is closely related to the given answer. 💡 Focus: evaluate RAG’s generation abilities in a non-QA setting.\n3.4 Fact Verification Task:\nGiven a claim, classify whether it is supported, refuted, or not enough info using evidence from Wikipedia. Dataset:\nFEVER benchmark. Method:\nMap each class label to a single output token, treating the task as sequence classification. RAG trains without supervision on retrieved evidence, learning retrieval and reasoning jointly. Evaluate:\nReport label accuracy for both: 3-way classification: supports / refutes / not enough info 2-way classification: supports / refutes Purpose:\nTest RAG’s capability for reasoning-based classification, not just text generation. 💡 Focus: reasoning and classification with retrieval (not generation)\n4. Results Open-domain Question Answering Abstractive Question Answering Jeopardy Question Generation Fact Verification Table 1 \u0026amp; 2 💡 Table 1:\nTo show that RAG outperforms previous retrieval-based QA systems (like DPR and REALM) and even large closed-book models (like T5), Proving that retrieval + generation can achieve state-of-the-art results without re-rankers or extractive readers. Table 2:\nTo demonstrate that RAG generalizes beyond simple QA: it performs strongly on abstractive answer generation (MSMARCO), question generation (Jeopardy), and fact verification (FEVER) showing it works for both generation and classification tasks, even without gold evidence Table 3 Table 4 \u0026amp; 5 💡 Table 4:\nTo verify through human judgment that RAG’s generated questions are more factual and more specific than those from BART, confirming that retrieval grounding improves accuracy and relevance in text generation. Table 5:\nTo measure linguistic diversity of generated text — showing that RAG’s outputs are less repetitive and more varied (closer to human text) than BART’s, thanks to diverse retrieved contexts. Factuality → Is the question factually correct? Specificity → Does the question precisely match its given answer (not too generic)? Table 6 “Ablation” means removing or changing a part of the model to test how much it matters.\n💡 Table 6 shows that RAG’s learned dense retriever is essential\nreplacing it with BM25 or freezing it significantly hurts performance. proving that jointly learned retrieval is key for strong open-domain generation and QA results. Figure 2 The heatmap (right) shows which retrieved document (y-axis) the model relies on when generating each token (x-axis) of a sentence.\nThe heatmap shows a dark blue cell at (Doc 2, “Sun”), which means Doc 2 — the one containing “The Sun Also Rises” — is strongly influencing this token. (The model correctly “looks up” the document that mentions that book.)\nAfter that, the dark blue (posterior weight) flattens — it spreads out across documents. That means: once the model has started generating “The Sun…”, it can finish “Also Rises” without continuing to depend on that document.\n💡 After seeing one or two key words from retrieval chunks (non-parametric memory), the generator’s parametric knowledge is enough to recall and complete the title.\nThe non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory.\n","permalink":"/notes/retrieval-augmented_generation_for_knowledge-inten/","summary":"\u003ch1 id=\"abstract\"\u003eAbstract\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003ePre-trained models\u003c/strong\u003e with a \u003cstrong\u003edifferentiable access mechanism\u003c/strong\u003e to \u003cstrong\u003eexplicit non-parametric memory\u003c/strong\u003e have so far been only investigated for extractive downstream tasks.\n\u003cul\u003e\n\u003cli\u003epre-trained models\u003c/li\u003e\n\u003cli\u003enon-parametric memory\u003c/li\u003e\n\u003cli\u003edifferentiable access mechanism\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e- In soft differentiable access mechanism, we don’t discard any chunks.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e- In Hard retrieval (standard RAG), the retriever picks the top-k passages\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\n\u003cli\u003eWe introduce RAG models where the parametric memory is \u003cstrong\u003ea pre-trained seq2seq model\u003c/strong\u003e and the non-parametric memory is \u003cstrong\u003ea dense vector index of Wikipedia\u003c/strong\u003e, accessed with \u003cstrong\u003ea pre-trained neural retriever\u003c/strong\u003e.\n\u003cul\u003e\n\u003cli\u003epre-trained models → seq2seq model\u003c/li\u003e\n\u003cli\u003enon-parametric memory → a dense vector index of Wikipedia\u003c/li\u003e\n\u003cli\u003edifferentiable access mechanism → a pre-trained neural retriever\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e1. Prompt (question) arrives.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e2. Seq2seq encoder turns it into query vector q.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e3. Retriever compares q to all memory keys k_i (Wikipedia passage vectors).\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e4. Compute similarity scores s_i = q ⋅ k_i.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e5. Apply softmax → attention weights α_i.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e6. Read vector r = Σ α_i v_i  (weighted mixture of passage info).\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e7. Feed r (plus q) into seq2seq decoder → generate answer token by token.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e8. Gradients flow through α_i → retriever learns to attend to more relevant chunks.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003etext chunk → retriever encoder → key/value → FAISS index → query embedding → top-k retrieval → generator\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\n\u003cli\u003eWe compare two RAG formulations, one which conditions on the \u003cstrong\u003esame retrieved passages\u003c/strong\u003e across the whole generated sequence, and another which can use \u003cstrong\u003edifferent passages\u003c/strong\u003e per token.\n\u003cul\u003e\n\u003cli\u003esame retrieved passages → RAG-Sequence\u003c/li\u003e\n\u003cli\u003edifferent passages per token → RAG-Token\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-html\" data-lang=\"html\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eIt’s often used for knowledge-intensive tasks, not free-form story generation.\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch1 id=\"discussion\"\u003eDiscussion\u003c/h1\u003e\n\u003cp\u003eWe conducted an thorough investigation of the learned retrieval component, validating\nits effectiveness, and we illustrated how the retrieval index can be \u003cstrong\u003ehot-swapped\u003c/strong\u003e to update the model without requiring any retraining.\u003c/p\u003e","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"content":"Von Neumann model The von Neumann model was an “efficient bridge between software and hardware” because:\nHardware designers could build chips to execute it efficiently. Software developers could write high-level programs that compile into this model. Thus, the von Neumann model is the connecting bridge that enables programs from the diverse and chaotic world of software to run efficientby on machines from the diverse and chaotic world of hardware.\nBulk-synchronous parallel (BSP) model it is a viable candidate for the role of bridging model.\nValiant wants parallel simulations to be almost as fast as ideal ones.\nThe extra cost should be only a small constant factor, not growing with processor count. He tries to avoid efficiency loss that scales with log(p). The model should work well for any number of processors, from a few to millions. Features of BSP model A major feature of the BSP model is that it provides this option with optimal efficiency (i.e., within constant factors) provided the programmer writes programs with sufficient parallel slackness.\nBSP can automatically manage communication and memory efficiently if the program exposes\nenough parallel work. If there’s enough parallelism (many tasks per processor), the model achieves near-optimal performance with little manual tuning.\n","permalink":"/notes/a_bridging_model_for_parallel_computation/","summary":"\u003ch2 id=\"von-neumann-model\"\u003eVon Neumann model\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003evon Neumann model\u003c/strong\u003e was an \u003cem\u003e“efficient bridge between software and hardware”\u003c/em\u003e because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHardware designers\u003c/strong\u003e could build chips to execute it efficiently.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSoftware developers\u003c/strong\u003e could write high-level programs that compile into this model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThus, the \u003cstrong\u003evon Neumann model\u003c/strong\u003e is the connecting bridge that enables programs from\nthe diverse and chaotic world of software to run efficientby on machines from the diverse and chaotic world of hardware.\u003c/p\u003e\n\u003ch2 id=\"bulk-synchronous-parallel-bsp-model\"\u003eBulk-synchronous parallel (BSP) model\u003c/h2\u003e\n\u003cp\u003eit is a viable candidate for the role of \u003cstrong\u003ebridging model\u003c/strong\u003e.\u003c/p\u003e","title":"A Bridging Model for Parallel Computation"},{"content":"Video Source\nPage Source\nPratical Source\n1. Introduction Extension of Abstract.\nSequence Modeling: Recurrent language modelsExtension of Abstract.\nOperate step-by-step, processing one token or time step at a time. one input + hidden state → one output + hidden state (update every time) e.g., RNNs, LSTMs Encoder-decoder architectures\nComposed of two RNNs (or other models): one to encode input into a representation another to decode it into the output sequence one input + hidden state → … → hidden state → one output + hidden state → … e.g., seq2seq Disadvantages about these two: Recurrent models preclude parallelization, so it’s not good.\nEncoder-decoder models have used attention to pass the stuff from encoder to decoder more effectively.\nAttention doesn’t use recurrence and entirely relies on an attention mechanism to draw glabal dependencies between input and output. 2. Background Corresponding Work:\nSpeak clearly about the corresponding papers, the connections between u and the papers, and the differences.\nReduce sequential computation: Typically use convolutional neural networks → the number of operations grows in the distance between positions → transfomer: a constant number of operations\nat the cost of reduced effective resolution due to averaging attention-weighted positions we counteract this bad effect with Multi-Head Attention Self-attention: compute a representation of the sequence based on different positions\nEnd-to-end memory networks: ✔ Reccurrence attention mechanism\n❌ Sequence-aligned recurrence\nTransformer: The first transduction model relying entirely on self-attention to do encoder-decoder model. ( to compute representations of its input and output without using sequence-aligned RNNs or convolution)\ntransduction model refers to making predictions or inferences about specific instances based on the data at hand, without relying on a prior generalization across all possible examples, which is different from Inductive learning.\n3. Model Architecture The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and the decoder.\nEncoder: maps an input sequence of symbol representations x → a sequence of continuous representations z. Decoder: given z, generates an output sequence of symbols one element at a time. At each step, the model is auto-regressive. Why Using LayerNorm not BatchNorm? LayerNorm normalizes across features of a single sample, suitable for variable-length sequences.\nBatchNorm normalizes across the batch, which can be inconsistent for sequence tasks.\n3.1 Encoder and Decoder Stacks Encoder N = 6 identical layers, each layer has two sub-layers.\nmulti-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\ndimension of output = 512\nDecoder N = 6 identical layers, each layer has three sub-layers.\nmasked multi-head attention over the output of the encoder stack multi-head self-attention mechanism simple, position-wise fully connected feed-forward network For each sub-layer, connect a layer normalization and a residual connection.\n3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output where the query, keys, values, and output are all vectors.\nThe output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n( Given a query, output is computed by similarity between query and keys, then different keys have different weights, next combine the values based on the weights. )\n3.2.1 Scaled Dot-Product Attention We compute the dot products of the query with all keys, divide each by length of query √, and apply a softmax function to obtain the weights on the values.\nWhy divide each by length of query √ ?\nIt prevents the input to the softmax from becoming excessively large, ensuring that the softmax outputs remain well-distributed and numerically stable. The softmax function normalize the scores into probabilities, but it doesn\u0026rsquo;t address the problem of large input magnitudes directly. If the inputs to softmax are extremely large, the e^x can become numerically unstable, leading to issues in computation.\nn refers to the length of a sequence, the number of the words.\ndk refers to the length of the one word vector.\nm refers to the number of the target words.\ndv refers to the length of the one target word vector.\nWhat there is a function of the Mask?\nWhen computing the output, I only use the key-value pairs up to the current time and do not use any later key-value pairs.\n3.2.2 Multi-Head Attention Linear projection, just like lots of channels.\nLinearly project the queries, keys and values h times to low dimensions with different, learned linear projections.\nFinally, these are concatenated (stack) and once again projected.\nWe emply h = 8 parallel attention layers, or heads.\nSo for each layer or head, we make their dimension to 64 to make the total computational cost is similar to single-head attention.\n3.2.3 Application of Attention in our Model The Transformer uses multi-head attention in three different ways:\nIn “encoder-decoder attention” layers, queries → previous decoder layer keys and values → the output of the encoder In “encoder self-attention” layers, keys, values and queries all come from the previous layer in the encoder In “decoder self-attention” layers, keys, values and queries all come from the previous layer in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position 3.3 Position-wise Feed-Forward Networks The fully connected feed-forward network is applied to each position separately and identically.\nThe Multi-Head Attention has already get the location information, now we need to add more expression ability by adding non-linear.\nThe output and MLP patterns are the same. The difference are the input source.\n3.4 Embedding and Softmax Embedding convert the input tokens to vectors of dimension ( 512 ), then multiply those weights by\n$\\sqrt {d_{models}}$\nWhy multiply those weights by $\\sqrt {d_{models}}$ ?\nTo boosts the magnitude, making it more aligned with other components of the model. Because the initial value is between 0~1 by using normal distribution.\nWe update the L2 norm of the embeddings to 1 finally, so we need to ensure the value of each dimension not too small. The L2 norm value of a vector to 1 is best to express a word, because it only express direction.\nSoftmax convert the decoder output to predicted next-token probabilities.\n3.5 Positional Encoding The order change, but the values don’t change. So we add the sequential information to the input.\nWe use sine and cosine functions of different frequencies [-1, 1]\n4. Why Self-Attention Length of a word → d Number of words → n Self-Attention → n words ✖️ every words need to multiply with n words and for each two words do d multiplying. Recurrent → d-dimension vector multiply d✖️d matrix, n times Convolutional → k kernel_size, n words, d^2 input_channels ✖️ output_channels (Draw picture clear) Self-Attention (restricted) → r the number of neighbors It seems like Self-Attention architecture has lots of advantages, but it needs more data and bigger model to train to achieve the same effect.\n5. Training 5.1 Training Data and Batching Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. → So we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation.\n5.2 Hardware and Schedule 5.3 Optimizer 5.4 Regularization Residual Dropout Label Smoothing 6. Results $N:$ number of blocks\n$d_{model}:$ the length of a token vector\n$d_{ff}:$ Feed-Forward Full-Connected Layer Intermediate layer output size\n$h:$ the number of heads\n$d_k:$ the dimension of keys in a head\n$d_v :$ the dimension of values in a head\n$P_{drop}:$ dropout rate\n$\\epsilon_{ls}:$ Label Smoothing value, the learned label value\n$train steps:$ the number of batchs\n","permalink":"/notes/attention_is_all_you_need/","summary":"\u003cp\u003e\u003ca href=\"https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.1387.top_right_bar_window_history.content.click\u0026amp;vd_source=83d7a54445de4810b52e58e4864b4605\"\u003eVideo Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1706.03762\"\u003ePage Source\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=kCc8FmEb1nY\"\u003ePratical Source\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"1-introduction\"\u003e1. Introduction\u003c/h1\u003e\n\u003caside\u003e\n\u003cp\u003eExtension of Abstract.\u003c/p\u003e\n\u003c/aside\u003e\n\u003ch2 id=\"sequence-modeling\"\u003eSequence Modeling:\u003c/h2\u003e\n\u003cp\u003eRecurrent language modelsExtension of Abstract.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOperate step-by-step, processing one token or time step at a time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state → one output + hidden state (update every time)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., RNNs, LSTMs\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEncoder-decoder architectures\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eComposed of two RNNs (or other models):\n\u003cul\u003e\n\u003cli\u003eone to encode input into a representation\u003c/li\u003e\n\u003cli\u003eanother to decode it into the output sequence\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eone input + hidden state → … → hidden state → one output + hidden state → …\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003ee.g., seq2seq\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"disadvantages-about-these-two\"\u003eDisadvantages about these two:\u003c/h2\u003e\n\u003cp\u003eRecurrent models preclude parallelization, so it’s not good.\u003c/p\u003e","title":"Attention is All You Need"}]