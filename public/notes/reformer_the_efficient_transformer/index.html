<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reformer: The Efficient Transformer | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: Reformer"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/reformer_the_efficient_transformer/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/reformer_the_efficient_transformer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/reformer_the_efficient_transformer/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Reformer: The Efficient Transformer"><meta property="og:description" content="Paper-reading notes: Reformer"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-12-14T08:39:11+00:00"><meta property="article:modified_time" content="2025-12-14T08:39:11+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/reformer_the_efficient_transformer/image.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/reformer_the_efficient_transformer/image.png"><meta name=twitter:title content="Reformer: The Efficient Transformer"><meta name=twitter:description content="Paper-reading notes: Reformer"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Reformer: The Efficient Transformer","item":"https://my-blog-alpha-vert.vercel.app/notes/reformer_the_efficient_transformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reformer: The Efficient Transformer","name":"Reformer: The Efficient Transformer","description":"Paper-reading notes: Reformer","keywords":[],"articleBody":"1. Problem Transformer models achieve strong performance but are extremely inefficient for long sequences and deep architectures. In practice, they suffer from three fundamental bottlenecks:\nActivation memory scales linearly with the number of layers, because activations must be stored for backpropagation; Feed-forward layers consume large memory, since the intermediate dimension $d_{ff}$ is much larger than the model dimension; Self-attention has quadratic time and memory complexity $O(L^2)$ in sequence length, making long sequences (e.g., 64K tokens) impractical even with memory-efficient implementations. As a result, large Transformers often cannot be trained or even fine-tuned on a single accelerator, limiting scalability and accessibility. The key question posed by the paper is whether this limitation is fundamental—or merely due to architectural inefficiency.\n2. Method Reformer addresses these inefficiencies by redesigning the Transformer with three complementary techniques:\n2.1. Locality-Sensitive Hashing (LSH) Attention Full dot-product attention is replaced with an approximate attention mechanism based on angular locality-sensitive hashing. Queries and keys are shared $(Q=K)$ and hashed so that only nearby tokens (in embedding space) attend to each other. This reduces attention complexity from $O(L^2)$ to $O(L \\log L)$ while preserving accuracy through multiple hashing rounds.\n2.2. Reversible Residual Layers Standard residual connections are replaced with reversible layers, allowing activations from earlier layers to be reconstructed during backpropagation instead of stored. This removes the linear dependence on the number of layers in activation memory.\n2.3. Chunked Feed-Forward Computation Feed-forward layers are computed in chunks over the sequence dimension, exploiting position-wise independence. This avoids storing large intermediate activations of size $d_{ff}$, further reducing peak memory usage.\nTakeaway Together, these changes produce the Reformer architecture, which matches standard Transformer performance while enabling efficient training on very long sequences and deep models with dramatically lower memory requirements.\n","wordCount":"287","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/reformer_the_efficient_transformer/image.png","datePublished":"2025-12-14T08:39:11Z","dateModified":"2025-12-14T08:39:11Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/reformer_the_efficient_transformer/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Reformer: The Efficient Transformer</h1><div class=post-description>Paper-reading notes: Reformer</div><div class=post-meta><span title='2025-12-14 08:39:11 +0000 +0000'>December 14, 2025</span>&nbsp;·&nbsp;<span>287 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-problem aria-label="1. Problem">1. Problem</a></li><li><a href=#2-method aria-label="2. Method">2. Method</a><ul><li><a href=#21-locality-sensitive-hashing-lsh-attention aria-label="2.1. Locality-Sensitive Hashing (LSH) Attention">2.1. Locality-Sensitive Hashing (LSH) Attention</a></li><li><a href=#22-reversible-residual-layers aria-label="2.2. Reversible Residual Layers">2.2. Reversible Residual Layers</a></li><li><a href=#23-chunked-feed-forward-computation aria-label="2.3. Chunked Feed-Forward Computation">2.3. Chunked Feed-Forward Computation</a></li></ul></li><li><a href=#takeaway aria-label=Takeaway>Takeaway</a></li></ul></div></details></div><div class=post-content><h1 id=1-problem>1. Problem<a hidden class=anchor aria-hidden=true href=#1-problem>#</a></h1><p>Transformer models achieve strong performance but are <strong>extremely inefficient for long sequences and deep architectures</strong>. In practice, they suffer from three fundamental bottlenecks:</p><ol><li><strong>Activation memory scales linearly with the number of layers</strong>, because activations must be stored for backpropagation;</li><li><strong>Feed-forward layers consume large memory</strong>, since the intermediate dimension $d_{ff}$ is much larger than the model dimension;</li><li><strong>Self-attention has quadratic time and memory complexity $O(L^2)$</strong> in sequence length, making long sequences (e.g., 64K tokens) impractical even with memory-efficient implementations.</li></ol><p>As a result, large Transformers often <strong>cannot be trained or even fine-tuned on a single accelerator</strong>, limiting scalability and accessibility. The key question posed by the paper is whether this limitation is fundamental—or merely due to architectural inefficiency.</p><h1 id=2-method>2. Method<a hidden class=anchor aria-hidden=true href=#2-method>#</a></h1><p>Reformer addresses these inefficiencies by redesigning the Transformer with three complementary techniques:</p><h2 id=21-locality-sensitive-hashing-lsh-attention><strong>2.1. Locality-Sensitive Hashing (LSH) Attention</strong><a hidden class=anchor aria-hidden=true href=#21-locality-sensitive-hashing-lsh-attention>#</a></h2><p>Full dot-product attention is replaced with an approximate attention mechanism based on angular locality-sensitive hashing. Queries and keys are shared $(Q=K)$ and hashed so that only <strong>nearby tokens (in embedding space)</strong> attend to each other. This reduces attention complexity from $O(L^2)$ to <strong>$O(L \log L)$</strong> while preserving accuracy through multiple hashing rounds.</p><p><img alt=image.png loading=lazy src=/notes/reformer_the_efficient_transformer/image.png></p><h2 id=22-reversible-residual-layers><strong>2.2. Reversible Residual Layers</strong><a hidden class=anchor aria-hidden=true href=#22-reversible-residual-layers>#</a></h2><p>Standard residual connections are replaced with <strong>reversible layers</strong>, allowing activations from earlier layers to be reconstructed during backpropagation instead of stored. This removes the <strong>linear dependence on the number of layers</strong> in activation memory.</p><h2 id=23-chunked-feed-forward-computation><strong>2.3. Chunked Feed-Forward Computation</strong><a hidden class=anchor aria-hidden=true href=#23-chunked-feed-forward-computation>#</a></h2><p>Feed-forward layers are computed in <strong>chunks over the sequence dimension</strong>, exploiting position-wise independence. This avoids storing large intermediate activations of size $d_{ff}$, further reducing peak memory usage.</p><h1 id=takeaway>Takeaway<a hidden class=anchor aria-hidden=true href=#takeaway>#</a></h1><p>Together, these changes produce the <strong>Reformer architecture</strong>, which matches standard Transformer performance while enabling efficient training on <strong>very long sequences and deep models</strong> with dramatically lower memory requirements.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>