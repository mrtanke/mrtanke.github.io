<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ATLAS: Learning to Optimally Memorize the Context at Test Time | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: ATLAS"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/atlas_learning_to_optimally_memorize_the_context_a/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/atlas_learning_to_optimally_memorize_the_context_a/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/atlas_learning_to_optimally_memorize_the_context_a/"><meta property="og:site_name" content="Home"><meta property="og:title" content="ATLAS: Learning to Optimally Memorize the Context at Test Time"><meta property="og:description" content="Paper-reading notes: ATLAS"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-11-29T08:46:44+00:00"><meta property="article:modified_time" content="2025-11-29T08:46:44+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/atlas_learning_to_optimally_memorize_the_context_a/d3087366-d1c2-4e91-8c83-3a9ac1506a66.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/atlas_learning_to_optimally_memorize_the_context_a/d3087366-d1c2-4e91-8c83-3a9ac1506a66.png"><meta name=twitter:title content="ATLAS: Learning to Optimally Memorize the Context at Test Time"><meta name=twitter:description content="Paper-reading notes: ATLAS"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"ATLAS: Learning to Optimally Memorize the Context at Test Time","item":"https://my-blog-alpha-vert.vercel.app/notes/atlas_learning_to_optimally_memorize_the_context_a/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ATLAS: Learning to Optimally Memorize the Context at Test Time","name":"ATLAS: Learning to Optimally Memorize the Context at Test Time","description":"Paper-reading notes: ATLAS","keywords":[],"articleBody":" Titans = learn to memorize token at test time,\nATLAS = learn to memorize context at test time, with more capacity, better objectives, and stronger optimizers.\n1. Introduction ATLAS addresses a fundamental limitation of modern recurrent memory models (e.g., Titans): they update memory token by token, using only the current key–value pair, which limits:\nMemory capacity – only O(d) independent pairs can be stored. Memory quality – memory captures individual tokens rather than the structure of a context span. Memory optimization – update rules (e.g., gradient descent + momentum) are simple and often sub-optimal. Scalability – sequential updates reduce parallelization. To overcome these issues, ATLAS proposes a more expressive and scalable framework in which:\nMemory is a deep MLP updated during inference (like Titans), but the update rule optimizes memory w.r.t. a whole context window, using richer feature mappings and a stronger internal optimizer. This allows the model to memorize context, not just individual tokens, while retaining RNN-style linear complexity.\n2. Method The ATLAS framework has three core components:\n2.1. High-capacity memory via polynomial feature mapping Instead of using raw keys $k$, ATLAS applies a feature map $\\phi(k)$:\nPolynomial kernels increase effective dimensionality from $d → O(d^p)$. Exponential kernels approximate the exponential $q^T k$ of Transformers. This dramatically increases memory capacity without increasing the number of memory parameters.\n2.2. Context-aware memory update (Omega Rule) Unlike Titans (which update with only $(k_t, v_t)$),\nATLAS updates memory using a sliding window of the last c tokens:\n$$ M_t = \\arg\\min_M \\sum_{i=t-c+1}^{t} Y_i^{(t)} | M(\\phi(k_i)) - v_i |^2 $$\nFeatures:\nMulti-token optimization: memory learns from a context span, not a single token. Learned gates $Y_i^t$: controls how much each token contributes. Generalizes classical rules: $c=1$ → Delta rule / Titans $c=\\infty$ → global optimization like attention This is the heart of ATLAS: memory performs small-batch gradient descent at test time on a local context.\n2.3. Stronger internal optimizer (Muon) ATLAS replaces Titans’ first-order memory update (gradient descent + momentum):\n$$ M_t = \\alpha_t M_{t-1} + S_t,\\qquad $$\n$$ S_t = \\gamma_t S_{t-1} - \\eta_t \\nabla \\ell(M_{t-1}; k_t, v_t) $$\nwith a second-order, quasi-Newton style update using the Muon optimizer.\nMuon approximates the Newton update:\n$$ M_t = M_{t-1} - H_t^{-1} \\nabla \\ell_t, $$\nand replaces $H_t^{-1}$ with a cheap matrix-free approximation using the Newton–Schulz iteration:\n$$ H_t^{-1} \\approx \\text{NS}(G_t), $$\nwhere $G_t$ is an approximate curvature / preconditioner matrix.\nThus, the ATLAS memory update using Muon becomes:\n$$ M_t = \\alpha_t M_{t-1} - \\eta_t \\text{NS}(G_t) , \\left( \\sum_{i=t-c+1}^{t} Y_i^{(t)} \\nabla \\ell(M_{t-1}; k_i, v_i) \\right), $$\nwhere:\n$\\alpha_t$ = learned forget gate $\\eta_t$ = learned step size $Y_i^{(t)}$ = per-token contribution gate $\\text{NS}(G_t)$ = Muon second-order curvature approximation ATLAS still computes the same gradient (the gap between predicted value and true value). Muon simply reshapes this gradient using an approximate second-order update, so the memory moves in a smarter direction, not just the steepest direction.\nTraining \u0026 parallelization To make Omega Rule scale to long sequences, ATLAS:\nsplits the sequence into parallel chunks, applies recurrent updates within a chunk, and applies parallelizable gradient accumulation across chunks. Thus ATLAS preserves the GPU/TPU-friendly nature of Titans while significantly improving memory quality.\n3. Novelty Attention replacement with linear complexity\nATLAS Layer substitutes the Transformer attention block with a read–write memory module that enables long-context reasoning at O(n) cost.\nContext-based memory updates\nATLAS replaces token-level updates (Titans) with the Omega Rule, optimizing memory over a window of past tokens rather than only the current one.\nDeep neural memory with high capacity\nThe memory is a deep MLP whose parameters are updated at inference.\nPolynomial/exponential feature maps expand keys/queries and give super-linear memory capacity.\nSecond-order test-time learning\nMemory updates use the Muon optimizer, a quasi-Newton method, providing more stable and expressive learning than Titan’s first-order gradient descent.\n","wordCount":"628","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/atlas_learning_to_optimally_memorize_the_context_a/d3087366-d1c2-4e91-8c83-3a9ac1506a66.png","datePublished":"2025-11-29T08:46:44Z","dateModified":"2025-11-29T08:46:44Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/atlas_learning_to_optimally_memorize_the_context_a/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">ATLAS: Learning to Optimally Memorize the Context at Test Time</h1><div class=post-description>Paper-reading notes: ATLAS</div><div class=post-meta><span title='2025-11-29 08:46:44 +0000 +0000'>November 29, 2025</span>&nbsp;·&nbsp;<span>628 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-introduction aria-label="1. Introduction">1. Introduction</a></li><li><a href=#2-method aria-label="2. Method">2. Method</a><ul><li><a href=#21-high-capacity-memory-via-polynomial-feature-mapping aria-label="2.1. High-capacity memory via polynomial feature mapping">2.1. High-capacity memory via polynomial feature mapping</a></li><li><a href=#22-context-aware-memory-update-omega-rule aria-label="2.2. Context-aware memory update (Omega Rule)">2.2. Context-aware memory update (Omega Rule)</a></li><li><a href=#23-stronger-internal-optimizer-muon aria-label="2.3. Stronger internal optimizer (Muon)">2.3. Stronger internal optimizer (Muon)</a></li><li><a href=#training--parallelization aria-label="Training & parallelization">Training & parallelization</a></li></ul></li><li><a href=#3-novelty aria-label="3. Novelty">3. Novelty</a></li></ul></div></details></div><div class=post-content><aside><p>Titans = learn to memorize token at test time,</p><p><strong>ATLAS</strong> = learn to memorize context at test time, with more capacity, better objectives, and stronger optimizers.</p></aside><h1 id=1-introduction><strong>1. Introduction</strong><a hidden class=anchor aria-hidden=true href=#1-introduction>#</a></h1><p>ATLAS addresses a fundamental limitation of modern recurrent memory models (e.g., Titans): they update memory <strong>token by token</strong>, using only the current key–value pair, which limits:</p><ol><li><strong>Memory capacity</strong> – only O(d) independent pairs can be stored.</li><li><strong>Memory quality</strong> – memory captures individual tokens rather than the structure of a context span.</li><li><strong>Memory optimization</strong> – update rules (e.g., gradient descent + momentum) are simple and often sub-optimal.</li><li><strong>Scalability</strong> – sequential updates reduce parallelization.</li></ol><p>To overcome these issues, ATLAS proposes a more expressive and scalable framework in which:</p><ul><li>Memory is a <strong>deep MLP</strong> updated during inference (like Titans),</li><li>but the update rule optimizes memory <strong>w.r.t. a whole context window</strong>,</li><li>using <strong>richer feature mappings</strong> and a <strong>stronger internal optimizer</strong>.</li></ul><p>This allows the model to memorize <strong>context</strong>, not just individual tokens, while retaining RNN-style linear complexity.</p><h1 id=2-method><strong>2. Method</strong><a hidden class=anchor aria-hidden=true href=#2-method>#</a></h1><p>The ATLAS framework has three core components:</p><h2 id=21-high-capacity-memory-via-polynomial-feature-mapping><strong>2.1. High-capacity memory via polynomial feature mapping</strong><a hidden class=anchor aria-hidden=true href=#21-high-capacity-memory-via-polynomial-feature-mapping>#</a></h2><p>Instead of using raw keys $k$, ATLAS applies a <strong>feature map</strong> $\phi(k)$:</p><ul><li>Polynomial kernels increase effective dimensionality from $d → O(d^p)$.</li><li>Exponential kernels approximate the exponential $q^T k$ of Transformers.</li></ul><p>This dramatically increases memory capacity without increasing the number of memory parameters.</p><h2 id=22-context-aware-memory-update-omega-rule><strong>2.2. Context-aware memory update (Omega Rule)</strong><a hidden class=anchor aria-hidden=true href=#22-context-aware-memory-update-omega-rule>#</a></h2><p>Unlike Titans (which update with only $(k_t, v_t)$),</p><p>ATLAS updates memory using a <strong>sliding window of the last c tokens</strong>:</p><p>$$
M_t = \arg\min_M \sum_{i=t-c+1}^{t} Y_i^{(t)} | M(\phi(k_i)) - v_i |^2
$$</p><p>Features:</p><ul><li><strong>Multi-token optimization</strong>: memory learns from a context span, not a single token.</li><li><strong>Learned gates $Y_i^t$</strong>: controls how much each token contributes.</li><li><strong>Generalizes classical rules</strong>:<ul><li>$c=1$ → Delta rule / Titans</li><li>$c=\infty$ → global optimization like attention</li></ul></li></ul><aside><p>This is the heart of ATLAS: memory performs <strong>small-batch gradient descent at test time</strong> on a local context.</p></aside><h2 id=23-stronger-internal-optimizer-muon><strong>2.3. Stronger internal optimizer (Muon)</strong><a hidden class=anchor aria-hidden=true href=#23-stronger-internal-optimizer-muon>#</a></h2><p>ATLAS replaces Titans’ <strong>first-order memory update</strong> (gradient descent + momentum):</p><p>$$
M_t = \alpha_t M_{t-1} + S_t,\qquad
$$</p><p>$$
S_t = \gamma_t S_{t-1} - \eta_t \nabla \ell(M_{t-1}; k_t, v_t)
$$</p><p>with a <strong>second-order, quasi-Newton style update</strong> using the <strong>Muon optimizer</strong>.</p><p>Muon approximates the Newton update:</p><p>$$
M_t = M_{t-1} - H_t^{-1} \nabla \ell_t,
$$</p><p>and replaces $H_t^{-1}$ with a cheap matrix-free approximation using the <strong>Newton–Schulz iteration</strong>:</p><p>$$
H_t^{-1} \approx \text{NS}(G_t),
$$</p><p>where $G_t$ is an approximate curvature / preconditioner matrix.</p><p>Thus, the <strong>ATLAS memory update using Muon</strong> becomes:</p><p>$$
M_t
= \alpha_t M_{t-1} - \eta_t \text{NS}(G_t) , \left( \sum_{i=t-c+1}^{t} Y_i^{(t)}
\nabla \ell(M_{t-1}; k_i, v_i) \right),
$$</p><p>where:</p><ul><li>$\alpha_t$ = learned forget gate</li><li>$\eta_t$ = learned step size</li><li>$Y_i^{(t)}$ = per-token contribution gate</li><li>$\text{NS}(G_t)$ = <strong>Muon</strong> second-order curvature approximation</li></ul><aside><p>ATLAS still computes the same gradient (the gap between predicted value and true value). Muon simply <strong>reshapes</strong> this gradient using an approximate second-order update, so the memory moves in a <strong>smarter direction</strong>, not just the steepest direction.</p></aside><h2 id=training--parallelization><strong>Training & parallelization</strong><a hidden class=anchor aria-hidden=true href=#training--parallelization>#</a></h2><p>To make Omega Rule scale to long sequences, ATLAS:</p><ul><li>splits the sequence into <strong>parallel chunks</strong>,</li><li>applies recurrent updates within a chunk,</li><li>and applies parallelizable gradient accumulation across chunks.</li></ul><p>Thus ATLAS preserves the GPU/TPU-friendly nature of Titans while significantly improving memory quality.</p><p><img alt=image.png loading=lazy src=/notes/atlas_learning_to_optimally_memorize_the_context_a/d3087366-d1c2-4e91-8c83-3a9ac1506a66.png></p><h1 id=3-novelty><strong>3. Novelty</strong><a hidden class=anchor aria-hidden=true href=#3-novelty>#</a></h1><ul><li><p><strong>Attention replacement with linear complexity</strong></p><p>ATLAS Layer substitutes the Transformer attention block with a <strong>read–write memory module</strong> that enables long-context reasoning at <strong>O(n)</strong> cost.</p></li><li><p><strong>Context-based memory updates</strong></p><p>ATLAS replaces token-level updates (Titans) with the <strong>Omega Rule</strong>, optimizing memory over a window of past tokens rather than only the current one.</p></li><li><p><strong>Deep neural memory with high capacity</strong></p><p>The memory is a <strong>deep MLP</strong> whose parameters are updated at inference.</p><p>Polynomial/exponential feature maps expand keys/queries and give <strong>super-linear memory capacity</strong>.</p></li><li><p><strong>Second-order test-time learning</strong></p><p>Memory updates use the <strong>Muon optimizer</strong>, a quasi-Newton method, providing more stable and expressive learning than Titan’s first-order gradient descent.</p></li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>