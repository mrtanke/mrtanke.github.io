<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Mamba: Linear-Time Sequence Modeling with Selective State Spaces | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: Mamba"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/mamba_linear-time_sequence_modeling_with_selective/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/mamba_linear-time_sequence_modeling_with_selective/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/mamba_linear-time_sequence_modeling_with_selective/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Mamba: Linear-Time Sequence Modeling with Selective State Spaces"><meta property="og:description" content="Paper-reading notes: Mamba"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-11-17T07:38:28+00:00"><meta property="article:modified_time" content="2025-11-17T07:38:28+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mamba_linear-time_sequence_modeling_with_selective/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mamba_linear-time_sequence_modeling_with_selective/image_1.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/mamba_linear-time_sequence_modeling_with_selective/image.png"><meta name=twitter:title content="Mamba: Linear-Time Sequence Modeling with Selective State Spaces"><meta name=twitter:description content="Paper-reading notes: Mamba"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces","item":"https://my-blog-alpha-vert.vercel.app/notes/mamba_linear-time_sequence_modeling_with_selective/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces","name":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces","description":"Paper-reading notes: Mamba","keywords":[],"articleBody":"blog: https://blog.csdn.net/v_JULY_v/article/details/134923301\nS4 video: https://www.youtube.com/watch?v=luCBXCErkCs\nIntroduction Foundation models are large pretrained models used for many tasks like text, image, and audio. Most of them are built on the Transformer, which uses attention to connect information between tokens. However, attention is slow and limited to a fixed window, and its cost grows very fast with sequence length. Many improved versions exist, but none perform as well as Transformers at scale.\nStructured State Space Models (SSMs) are efficient sequence models inspired by RNNs and control theory. They can model long sequences with linear time complexity and work well on continuous data like audio or vision. But they still perform poorly on discrete data such as text because they cannot focus on relevant information.\nThe paper proposes Selective SSMs that let model parameters depend on the input. This helps the model decide what to keep or forget based on the content, similar to attention. The authors also design a new hardware-efficient algorithm to make it run fast and truly linear in sequence length.\nThe resulting Mamba architecture combines these selective SSMs into a simple, recurrent model. Mamba trains and runs much faster than Transformers, handles very long contexts, and needs less memory.\nExperiments show Mamba performs as well as or better than Transformers across language, audio, and genomics tasks. It reaches Transformer-level accuracy with only half the size and runs up to 5× faster. Mamba proves that efficient, attention-free models can match Transformer quality while scaling to very long sequences.\nMethod 1. Problems Transformers are powerful but slow and memory-heavy (quadratic cost). Existing efficient models (like SSMs, RNNs) are not content-aware — can’t decide what to remember or ignore. Recurrent models are often hardware-inefficient on GPUs. 2. Architecture Mamba uses a new Selective State Space Model (SSM). Adds a selection mechanism so parameters depend on the input (content-aware). Each layer combines one Conv (local info) and one SSM (long memory) into a simple, unified block. 3. Noval Selective mechanism: Traditional SSMs use fixed matrices A, B, C Mamba updates them input-dependent: A, B, C, Δ → adaptive reasoning. Hardware-aware selective scan: optimized GPU recurrence using fast SRAM. Practical simplified architecture: Removes heavy math from S4 (HiPPO kernel, complex eigenvalues) Uses simple parameterization + gating Makes SSMs trainable and stable in large models Result:\nMamba keeps linear-time efficiency, adds attention-like adaptability, and achieves Transformer-level or better performance up to 5× faster.\n","wordCount":"397","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/mamba_linear-time_sequence_modeling_with_selective/image.png","datePublished":"2025-11-17T07:38:28Z","dateModified":"2025-11-17T07:38:28Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/mamba_linear-time_sequence_modeling_with_selective/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</h1><div class=post-description>Paper-reading notes: Mamba</div><div class=post-meta><span title='2025-11-17 07:38:28 +0000 +0000'>November 17, 2025</span>&nbsp;·&nbsp;<span>397 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#method aria-label=Method>Method</a><ul><ul><li><a href=#1-problems aria-label="1. Problems">1. Problems</a></li><li><a href=#2-architecture aria-label="2. Architecture">2. Architecture</a></li><li><a href=#3-noval aria-label="3. Noval">3. Noval</a></li></ul></ul></li></ul></div></details></div><div class=post-content><p>blog: <a href=https://blog.csdn.net/v_JULY_v/article/details/134923301>https://blog.csdn.net/v_JULY_v/article/details/134923301</a></p><p>S4 video: <a href="https://www.youtube.com/watch?v=luCBXCErkCs">https://www.youtube.com/watch?v=luCBXCErkCs</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Foundation models are large pretrained models used for many tasks like text, image, and audio. Most of them are built on the Transformer, which uses attention to connect information between tokens. However, attention is slow and limited to a fixed window, and its cost grows very fast with sequence length. Many improved versions exist, but none perform as well as Transformers at scale.</p><p>Structured State Space Models (SSMs) are efficient sequence models inspired by RNNs and control theory. They can model long sequences with <strong>linear time complexity</strong> and work well on continuous data like audio or vision. But they still perform poorly on discrete data such as text because they cannot focus on relevant information.</p><p>The paper proposes <strong>Selective SSMs</strong> that let model parameters depend on the input. This helps the model decide what to keep or forget based on the content, similar to attention. The authors also design a new <strong>hardware-efficient algorithm</strong> to make it run fast and truly linear in sequence length.</p><p>The resulting <strong>Mamba architecture</strong> combines these selective SSMs into a simple, recurrent model. Mamba trains and runs much faster than Transformers, handles very long contexts, and needs less memory.</p><p>Experiments show Mamba performs as well as or better than Transformers across language, audio, and genomics tasks. It reaches Transformer-level accuracy with only half the size and runs up to 5× faster. Mamba proves that efficient, attention-free models can match Transformer quality while scaling to very long sequences.</p><h1 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h1><h3 id=1-problems>1. Problems<a hidden class=anchor aria-hidden=true href=#1-problems>#</a></h3><ul><li>Transformers are powerful but <strong>slow and memory-heavy</strong> (quadratic cost).</li><li>Existing efficient models (like SSMs, RNNs) are <strong>not content-aware</strong> — can’t decide what to remember or ignore.</li><li>Recurrent models are often <strong>hardware-inefficient</strong> on GPUs.</li></ul><p><img alt=image.png loading=lazy src=/notes/mamba_linear-time_sequence_modeling_with_selective/image.png></p><h3 id=2-architecture>2. Architecture<a hidden class=anchor aria-hidden=true href=#2-architecture>#</a></h3><ul><li>Mamba uses a new <strong>Selective State Space Model (SSM)</strong>.</li><li>Adds a <strong>selection mechanism</strong> so parameters depend on the input (content-aware).</li><li>Each layer combines <strong>one Conv (local info)</strong> and <strong>one SSM (long memory)</strong> into a <strong>simple, unified block</strong>.</li></ul><p><img alt=image.png loading=lazy src=/notes/mamba_linear-time_sequence_modeling_with_selective/image_1.png></p><h3 id=3-noval>3. Noval<a hidden class=anchor aria-hidden=true href=#3-noval>#</a></h3><ul><li><strong>Selective mechanism:</strong><ul><li>Traditional SSMs use fixed matrices A, B, C</li><li>Mamba updates them <strong>input-dependent:</strong> A, B, C, Δ → adaptive reasoning.</li></ul></li><li><strong>Hardware-aware selective scan:</strong><ul><li>optimized GPU recurrence using fast SRAM.</li></ul></li><li><strong>Practical simplified architecture:</strong><ul><li>Removes heavy math from S4 (HiPPO kernel, complex eigenvalues)</li><li>Uses simple parameterization + gating</li><li>Makes SSMs trainable and stable in large models</li></ul></li></ul><p><strong>Result:</strong></p><p>Mamba keeps <strong>linear-time efficiency</strong>, adds <strong>attention-like adaptability</strong>, and achieves <strong>Transformer-level or better performance</strong> up to 5× faster.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>