<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: Bayesian Optimization"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/bayesian_optimization_is_superior_to_random_search/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/bayesian_optimization_is_superior_to_random_search/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/bayesian_optimization_is_superior_to_random_search/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning"><meta property="og:description" content="Paper-reading notes: Bayesian Optimization"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-12-10T08:36:10+00:00"><meta property="article:modified_time" content="2025-12-10T08:36:10+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/bayesian_optimization_is_superior_to_random_search/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/bayesian_optimization_is_superior_to_random_search/image_1.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/bayesian_optimization_is_superior_to_random_search/image_2.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/bayesian_optimization_is_superior_to_random_search/image.png"><meta name=twitter:title content="Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning"><meta name=twitter:description content="Paper-reading notes: Bayesian Optimization"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning","item":"https://my-blog-alpha-vert.vercel.app/notes/bayesian_optimization_is_superior_to_random_search/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning","name":"Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning","description":"Paper-reading notes: Bayesian Optimization","keywords":[],"articleBody":"‚ÅâÔ∏è Why Bayesian Optimization Finally Beat Random Search A Beginner-Friendly Review of the NeurIPS 2020 Black-Box Optimization Challenge.\nHyperparameter tuning sounds boring ‚Äî but it quietly determines the final performance of almost every machine learning model.\nYet most researchers still rely on manual tuning, grid search, or random search.\nIn 2020, a large international competition decided to answer a simple but important question:\nIs Bayesian Optimization really better than Random Search for hyperparameter tuning?\nThe results?\nA decisive yes ‚Äî and even more interesting insights about how top teams achieved massive improvements.\nThis article gives you a simple, intuitive overview of the entire paper and competition.\nüåü 1. Background: Why This Competition Matters Hyperparameter tuning is a black-box optimization problem:\nYou adjust parameters ‚Üí train a model ‚Üí observe the score ‚Üí repeat.\nYou don‚Äôt know the shape of the loss surface or its derivatives.\nThe ML community has long believed Bayesian Optimization (BO) should outperform random search ‚Äî but large-scale, ML-focused benchmarks were missing.\nThe NeurIPS 2020 challenge filled this gap:\n65 teams participated Hyperparameter tuning tasks came from real scikit-learn models and real datasets All evaluations were done in a secure Docker environment Final scores were based on hidden test problems to prevent overfitting The goal was simple:\nFind the most effective black-box optimizer for ML hyperparameters.\nüîß 2. How the Competition Worked The organizers built a ‚Äúdataset of optimization tasks‚Äù:\nDifferent ML models Different datasets Different evaluation metrics This created dozens of unique problems such as:\nTune GBDT on MNIST (accuracy) Tune logistic regression (log loss) Tune MLP on Boston housing (RMSE) A summary of the different model, loss, and data set combinations that made up the different phases.\nParticipants submitted an optimizer, not hyperparameters.\nTheir optimizer could:\nSuggest() k hyperparameter candidates Observe() the returned scores from the benchmark Each submission had:\n16 rounds Batch size = 8 evaluations per round Total = 128 evaluations per problem 640 seconds runtime limit per problem All practice problems were public, but feedback and final problems were completely hidden.\nThis design ensured:\nFairness No leaking of datasets No manual tuning on test problems üìä 3. How Scores Were Calculated (Super Simple Version) Scoring used the Bayesmark system:\nRandom Search average ‚Üí normalized score = 1 Best possible performance ‚Üí normalized score = 0 Then scores were transformed to a final leaderboard value:\nü•á Score = 100 √ó (1 ‚Äì normalized_mean_performance)\nSo:\n100 = always finds best hyperparameters 0 = no better than random search This created a clean, unitless, intuitive 0‚Äì100 scale.\nüèÜ 4. What Actually Worked? Key Insights from the Top Teams Insight 1: Bayesian Optimization dominates Out of 65 teams:\n61 beat random search Almost all top submissions used surrogate models + acquisition functions The best solutions achieved 100√ó sample efficiency vs random search Insight 2: Trust-region BO (TuRBO) is incredibly strong TuRBO (a local BO method) was the strongest baseline and appeared in 6 of the top 10 solutions.\nThis suggests:\nIn hyperparameter tuning, the landscape is often locally structured, so local models work well.\nInsight 3: Ensembles win ‚Äî even simple ones Every top-10 team used some form of ensemble.\nThis was the biggest surprise.\nExamples:\nNVIDIA combined TuRBO + Scikit-Optimize (50). Duxiaoman combined TuRBO + pySOT. AutoML.org used a more complex combination with differential evolution in later rounds. These ensembles consistently outperformed their components, especially avoiding failure cases where one method gets stuck.\nInsight 4: Handling categorical/discrete integer variables matters Most BO literature focuses on continuous parameters, but ML models often include:\nnumber of layers max_depth activation choices categorical losses Some teams modified TuRBO or used bandit-style strategies to better treat these.\nThis gave additional performance boosts.\nInsight 5: Meta-learning \u0026 Warm Starting can skyrocket performance During the feedback phase, teams noticed patterns:\n‚ÄúSimilar models like similar hyperparameters.‚Äù Some teams used meta-learning:\nUse good hyperparameters from similar past problems Warm-start the optimizer near plausible good regions When parameter names were revealed in a controlled ‚Äúwarm start experiment,‚Äù\nAutoML.org jumped to 1st place with huge gains.\nüéØ 5. Main Takeaways for Practitioners 1. Always prefer BO over random search The competition provided the clearest proof so far.\nEven simple BO implementations gave orders of magnitude better results.\n2. If you don‚Äôt know what to use ‚Üí start with TuRBO It performed well out-of-the-box across all tasks.\n3. Ensembling is a cheat code Even a basic 50/50 ensemble of two optimizers can dramatically improve stability and performance.\n4. Don‚Äôt ignore categorical parameters A small adjustment to treat them properly can make your optimizer more robust.\n5. Warm-start when you can If you repeatedly solve similar ML tasks, reuse previous experience.\nüîÆ 6. Future Directions The authors highlight several exciting extensions:\nmulti-fidelity optimization (early stopping, partial data) asynchronous parallel BO adding constraints or multi-objective settings giving partial model information to optimize smarter üßæ 7. Conclusion The NeurIPS 2020 Black-Box Optimization Challenge delivered a clear message:\nBayesian Optimization is not only better than random search ‚Äî it‚Äôs much better.\nWith simple ensembles and trust-region methods, teams achieved more than 100√ó speedups in sample efficiency.\nThis competition set a new benchmark and provided practical insights that anyone doing ML hyperparameter tuning can benefit from.\n","wordCount":"864","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/bayesian_optimization_is_superior_to_random_search/image.png","datePublished":"2025-12-10T08:36:10Z","dateModified":"2025-12-10T08:36:10Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/bayesian_optimization_is_superior_to_random_search/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;¬ª&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning</h1><div class=post-description>Paper-reading notes: Bayesian Optimization</div><div class=post-meta><span title='2025-12-10 08:36:10 +0000 +0000'>December 10, 2025</span>&nbsp;¬∑&nbsp;<span>864 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#-why-bayesian-optimization-finally-beat-random-search aria-label="‚ÅâÔ∏è Why Bayesian Optimization Finally Beat Random Search">‚ÅâÔ∏è Why Bayesian Optimization Finally Beat Random Search</a></li><li><a href=#-1-background-why-this-competition-matters aria-label="üåü 1. Background: Why This Competition Matters">üåü 1. Background: Why This Competition Matters</a></li><li><a href=#-2-how-the-competition-worked aria-label="üîß 2. How the Competition Worked">üîß 2. How the Competition Worked</a></li><li><a href=#-3-how-scores-were-calculated-super-simple-version aria-label="üìä 3. How Scores Were Calculated (Super Simple Version)">üìä 3. How Scores Were Calculated (Super Simple Version)</a></li><li><a href=#-4-what-actually-worked-key-insights-from-the-top-teams aria-label="üèÜ 4. What Actually Worked? Key Insights from the Top Teams">üèÜ 4. What Actually Worked? Key Insights from the Top Teams</a><ul><li><a href=#insight-1-bayesian-optimization-dominates aria-label="Insight 1: Bayesian Optimization dominates">Insight 1: Bayesian Optimization dominates</a></li><li><a href=#insight-2-trust-region-bo-turbo-is-incredibly-strong aria-label="Insight 2: Trust-region BO (TuRBO) is incredibly strong">Insight 2: Trust-region BO (TuRBO) is incredibly strong</a></li><li><a href=#insight-3-ensembles-win--even-simple-ones aria-label="Insight 3: Ensembles win ‚Äî even simple ones">Insight 3: Ensembles win ‚Äî even simple ones</a></li><li><a href=#insight-4-handling-categoricaldiscrete-integer-variables-matters aria-label="Insight 4: Handling categorical/discrete integer variables matters">Insight 4: Handling categorical/discrete integer variables matters</a></li><li><a href=#insight-5-meta-learning--warm-starting-can-skyrocket-performance aria-label="Insight 5: Meta-learning & Warm Starting can skyrocket performance">Insight 5: Meta-learning & Warm Starting can skyrocket performance</a></li></ul></li><li><a href=#-5-main-takeaways-for-practitioners aria-label="üéØ 5. Main Takeaways for Practitioners">üéØ 5. Main Takeaways for Practitioners</a><ul><ul><li><a href=#1-always-prefer-bo-over-random-search aria-label="1. Always prefer BO over random search">1. Always prefer BO over random search</a></li><li><a href=#2-if-you-dont-know-what-to-use--start-with-turbo aria-label="2. If you don‚Äôt know what to use ‚Üí start with TuRBO">2. If you don‚Äôt know what to use ‚Üí start with TuRBO</a></li><li><a href=#3-ensembling-is-a-cheat-code aria-label="3. Ensembling is a cheat code">3. Ensembling is a cheat code</a></li><li><a href=#4-dont-ignore-categorical-parameters aria-label="4. Don‚Äôt ignore categorical parameters">4. Don‚Äôt ignore categorical parameters</a></li><li><a href=#5-warm-start-when-you-can aria-label="5. Warm-start when you can">5. Warm-start when you can</a></li></ul></ul></li><li><a href=#-6-future-directions aria-label="üîÆ 6. Future Directions">üîÆ 6. Future Directions</a></li><li><a href=#-7-conclusion aria-label="üßæ 7. Conclusion">üßæ 7. Conclusion</a></li></ul></div></details></div><div class=post-content><h1 id=-why-bayesian-optimization-finally-beat-random-search>‚ÅâÔ∏è Why Bayesian Optimization Finally Beat Random Search<a hidden class=anchor aria-hidden=true href=#-why-bayesian-optimization-finally-beat-random-search>#</a></h1><p><strong>A Beginner-Friendly Review of the NeurIPS 2020 Black-Box Optimization Challenge.</strong></p><p>Hyperparameter tuning sounds boring ‚Äî but it quietly determines the final performance of almost every machine learning model.</p><p>Yet most researchers still rely on manual tuning, grid search, or random search.</p><p>In 2020, a large international competition decided to answer a simple but important question:</p><blockquote><p>Is Bayesian Optimization really better than Random Search for hyperparameter tuning?</p></blockquote><p>The results?</p><p>A decisive <strong>yes</strong> ‚Äî and even more interesting insights about how top teams achieved massive improvements.</p><p>This article gives you a simple, intuitive overview of the entire paper and competition.</p><h1 id=-1-background-why-this-competition-matters>üåü 1. Background: Why This Competition Matters<a hidden class=anchor aria-hidden=true href=#-1-background-why-this-competition-matters>#</a></h1><p>Hyperparameter tuning is a <strong>black-box optimization problem</strong>:</p><p>You adjust parameters ‚Üí train a model ‚Üí observe the score ‚Üí repeat.</p><p>You don‚Äôt know the shape of the loss surface or its derivatives.</p><p>The ML community has long believed Bayesian Optimization (BO) should outperform random search ‚Äî but large-scale, ML-focused benchmarks were missing.</p><p>The NeurIPS 2020 challenge filled this gap:</p><ul><li>65 teams participated</li><li>Hyperparameter tuning tasks came from <strong>real scikit-learn models and real datasets</strong></li><li>All evaluations were done in a <strong>secure Docker environment</strong></li><li>Final scores were based on <strong>hidden test problems</strong> to prevent overfitting</li></ul><p>The goal was simple:</p><blockquote><p>Find the most effective black-box optimizer for ML hyperparameters.</p></blockquote><h1 id=-2-how-the-competition-worked>üîß 2. How the Competition Worked<a hidden class=anchor aria-hidden=true href=#-2-how-the-competition-worked>#</a></h1><p>The organizers built a ‚Äúdataset of optimization tasks‚Äù:</p><ul><li>Different ML models</li><li>Different datasets</li><li>Different evaluation metrics</li></ul><p>This created dozens of unique problems such as:</p><ul><li>Tune GBDT on MNIST (accuracy)</li><li>Tune logistic regression (log loss)</li><li>Tune MLP on Boston housing (RMSE)</li></ul><p><img alt="A summary of the different model, loss, and data set combinations that made up the different phases." loading=lazy src=/notes/bayesian_optimization_is_superior_to_random_search/image.png></p><p>A summary of the different model, loss, and data set combinations that made up the different phases.</p><p>Participants submitted <strong>an optimizer</strong>, not hyperparameters.</p><p>Their optimizer could:</p><ol><li><strong>Suggest()</strong> k hyperparameter candidates</li><li><strong>Observe()</strong> the returned scores from the benchmark</li></ol><p>Each submission had:</p><ul><li>16 rounds</li><li>Batch size = 8 evaluations per round</li><li>Total = 128 evaluations per problem</li><li>640 seconds runtime limit per problem</li></ul><p>All practice problems were public, but <strong>feedback</strong> and <strong>final</strong> problems were completely hidden.</p><p>This design ensured:</p><ul><li>Fairness</li><li>No leaking of datasets</li><li>No manual tuning on test problems</li></ul><h1 id=-3-how-scores-were-calculated-super-simple-version>üìä 3. How Scores Were Calculated (Super Simple Version)<a hidden class=anchor aria-hidden=true href=#-3-how-scores-were-calculated-super-simple-version>#</a></h1><p>Scoring used the Bayesmark system:</p><ul><li>Random Search average ‚Üí <strong>normalized score = 1</strong></li><li>Best possible performance ‚Üí <strong>normalized score = 0</strong></li></ul><p>Then scores were transformed to a final leaderboard value:</p><blockquote><p>ü•á Score = 100 √ó (1 ‚Äì normalized_mean_performance)</p></blockquote><p>So:</p><ul><li><strong>100 = always finds best hyperparameters</strong></li><li><strong>0 = no better than random search</strong></li></ul><p>This created a clean, unitless, intuitive 0‚Äì100 scale.</p><h1 id=-4-what-actually-worked-key-insights-from-the-top-teams>üèÜ 4. What Actually Worked? Key Insights from the Top Teams<a hidden class=anchor aria-hidden=true href=#-4-what-actually-worked-key-insights-from-the-top-teams>#</a></h1><h2 id=insight-1-bayesian-optimization-dominates><strong>Insight 1: Bayesian Optimization dominates</strong><a hidden class=anchor aria-hidden=true href=#insight-1-bayesian-optimization-dominates>#</a></h2><p>Out of 65 teams:</p><ul><li><strong>61 beat random search</strong></li><li>Almost all top submissions used <strong>surrogate models + acquisition functions</strong></li><li>The best solutions achieved <strong>100√ó sample efficiency</strong> vs random search</li></ul><p><img alt=image.png loading=lazy src=/notes/bayesian_optimization_is_superior_to_random_search/image_1.png></p><h2 id=insight-2-trust-region-bo-turbo-is-incredibly-strong><strong>Insight 2: Trust-region BO (TuRBO) is incredibly strong</strong><a hidden class=anchor aria-hidden=true href=#insight-2-trust-region-bo-turbo-is-incredibly-strong>#</a></h2><p>TuRBO (a local BO method) was the strongest baseline and appeared in 6 of the top 10 solutions.</p><p>This suggests:</p><p>In hyperparameter tuning, the landscape is often <strong>locally structured</strong>, so local models work well.</p><h2 id=insight-3-ensembles-win--even-simple-ones><strong>Insight 3: Ensembles win ‚Äî even simple ones</strong><a hidden class=anchor aria-hidden=true href=#insight-3-ensembles-win--even-simple-ones>#</a></h2><p>Every top-10 team used some form of ensemble.</p><p>This was the biggest surprise.</p><p>Examples:</p><ul><li>NVIDIA combined <strong>TuRBO + Scikit-Optimize</strong> (50).</li><li>Duxiaoman combined <strong>TuRBO + pySOT</strong>.</li><li>AutoML.org used a more complex combination with differential evolution in later rounds.</li></ul><p>These ensembles consistently outperformed their components, especially avoiding failure cases where one method gets stuck.</p><h2 id=insight-4-handling-categoricaldiscrete-integer-variables-matters><strong>Insight 4: Handling categorical/discrete integer variables matters</strong><a hidden class=anchor aria-hidden=true href=#insight-4-handling-categoricaldiscrete-integer-variables-matters>#</a></h2><p>Most BO literature focuses on continuous parameters, but ML models often include:</p><ul><li>number of layers</li><li>max_depth</li><li>activation choices</li><li>categorical losses</li></ul><p>Some teams modified TuRBO or used bandit-style strategies to better treat these.</p><p>This gave additional performance boosts.</p><h2 id=insight-5-meta-learning--warm-starting-can-skyrocket-performance><strong>Insight 5: Meta-learning & Warm Starting can skyrocket performance</strong><a hidden class=anchor aria-hidden=true href=#insight-5-meta-learning--warm-starting-can-skyrocket-performance>#</a></h2><p>During the feedback phase, teams noticed patterns:</p><ul><li>‚ÄúSimilar models like similar hyperparameters.‚Äù</li></ul><p>Some teams used <strong>meta-learning</strong>:</p><ul><li>Use good hyperparameters from similar past problems</li><li>Warm-start the optimizer near plausible good regions</li></ul><p>When parameter names were revealed in a controlled ‚Äúwarm start experiment,‚Äù</p><p>AutoML.org jumped to <strong>1st place</strong> with huge gains.</p><p><img alt=image.png loading=lazy src=/notes/bayesian_optimization_is_superior_to_random_search/image_2.png></p><h1 id=-5-main-takeaways-for-practitioners>üéØ 5. Main Takeaways for Practitioners<a hidden class=anchor aria-hidden=true href=#-5-main-takeaways-for-practitioners>#</a></h1><h3 id=1-always-prefer-bo-over-random-search><strong>1. Always prefer BO over random search</strong><a hidden class=anchor aria-hidden=true href=#1-always-prefer-bo-over-random-search>#</a></h3><p>The competition provided the clearest proof so far.</p><p>Even simple BO implementations gave <strong>orders of magnitude</strong> better results.</p><h3 id=2-if-you-dont-know-what-to-use--start-with-turbo><strong>2. If you don‚Äôt know what to use ‚Üí start with TuRBO</strong><a hidden class=anchor aria-hidden=true href=#2-if-you-dont-know-what-to-use--start-with-turbo>#</a></h3><p>It performed well out-of-the-box across all tasks.</p><h3 id=3-ensembling-is-a-cheat-code><strong>3. Ensembling is a cheat code</strong><a hidden class=anchor aria-hidden=true href=#3-ensembling-is-a-cheat-code>#</a></h3><p>Even a basic 50/50 ensemble of two optimizers can dramatically improve stability and performance.</p><h3 id=4-dont-ignore-categorical-parameters><strong>4. Don‚Äôt ignore categorical parameters</strong><a hidden class=anchor aria-hidden=true href=#4-dont-ignore-categorical-parameters>#</a></h3><p>A small adjustment to treat them properly can make your optimizer more robust.</p><h3 id=5-warm-start-when-you-can><strong>5. Warm-start when you can</strong><a hidden class=anchor aria-hidden=true href=#5-warm-start-when-you-can>#</a></h3><p>If you repeatedly solve similar ML tasks, reuse previous experience.</p><h1 id=-6-future-directions>üîÆ 6. Future Directions<a hidden class=anchor aria-hidden=true href=#-6-future-directions>#</a></h1><p>The authors highlight several exciting extensions:</p><ul><li>multi-fidelity optimization (early stopping, partial data)</li><li>asynchronous parallel BO</li><li>adding constraints or multi-objective settings</li><li>giving partial model information to optimize smarter</li></ul><h1 id=-7-conclusion>üßæ 7. Conclusion<a hidden class=anchor aria-hidden=true href=#-7-conclusion>#</a></h1><p>The NeurIPS 2020 Black-Box Optimization Challenge delivered a clear message:</p><blockquote><p>Bayesian Optimization is not only better than random search ‚Äî it‚Äôs much better.</p></blockquote><p>With simple ensembles and trust-region methods, teams achieved more than <strong>100√ó speedups</strong> in sample efficiency.</p><p>This competition set a new benchmark and provided practical insights that anyone doing ML hyperparameter tuning can benefit from.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>