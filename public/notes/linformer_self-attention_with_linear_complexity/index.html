<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Linformer: Self-Attention with Linear Complexity | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: Linformer"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/linformer_self-attention_with_linear_complexity/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/linformer_self-attention_with_linear_complexity/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/linformer_self-attention_with_linear_complexity/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Linformer: Self-Attention with Linear Complexity"><meta property="og:description" content="Paper-reading notes: Linformer"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-12-04T15:10:45+00:00"><meta property="article:modified_time" content="2025-12-04T15:10:45+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/selfile.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/selfile.png"><meta name=twitter:title content="Linformer: Self-Attention with Linear Complexity"><meta name=twitter:description content="Paper-reading notes: Linformer"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Linformer: Self-Attention with Linear Complexity","item":"https://my-blog-alpha-vert.vercel.app/notes/linformer_self-attention_with_linear_complexity/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Linformer: Self-Attention with Linear Complexity","name":"Linformer: Self-Attention with Linear Complexity","description":"Paper-reading notes: Linformer","keywords":[],"articleBody":"1. Method 1.1. Key Observation The self-attention matrix is low-rank. Both empirical spectra and theory show that most information is concentrated in a few singular values.\n$$ P = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right) $$\n1.2. Main Idea If $P$ is low-rank, then instead of computing an $n\\times n$ matrix, we can approximate it using a factorization of size $n \\times k$) where $k \\ll n$.\n1.3. Linear Projections Linformer introduces two projection matrices $E, F \\in R^{n \\times k}$ to reduce the sequence length of keys and values:\n$$ K’ = EK,\\qquad V’ = FV. $$\n1.4. Linear Self-Attention Attention is computed as\n$$ \\text{Attention}(Q, K’, V’) = \\text{softmax}\\left(\\frac{Q {K’}^\\top}{\\sqrt{d}}\\right)V’. $$\nThis reduces complexity from $O(n^2)$ to $O(nk)$.\n1.5. Practical Choices k is small and independent of sequence length. Projections can be shared across heads and layers. 1.6. Empirical Results Linformer achieves similar or better accuracy than standard Transformers while being much faster and using much less memory.\n2. Novelty First to show self-attention is inherently low-rank.\nThis gives a theoretical reason why quadratic attention is unnecessary.\nIntroduces a simple and general low-rank factorization of attention.\nInstead of sparsity or hashing, Linformer directly compresses the sequence dimension.\nAchieves true linear complexity (O(n)) in both time and memory.\nProjection dimension does not grow with sequence length.\nThis enables extremely long-sequence training on standard hardware.\nCompatible with standard Transformer design.\nNo special sparsity patterns, no complex algorithms. Easy to drop in.\n","wordCount":"236","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/selfile.png","datePublished":"2025-12-04T15:10:45Z","dateModified":"2025-12-04T15:10:45Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/linformer_self-attention_with_linear_complexity/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Linformer: Self-Attention with Linear Complexity</h1><div class=post-description>Paper-reading notes: Linformer</div><div class=post-meta><span title='2025-12-04 15:10:45 +0000 +0000'>December 4, 2025</span>&nbsp;·&nbsp;<span>236 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-method aria-label="1. Method">1. Method</a><ul><li><a href=#11-key-observation aria-label="1.1. Key Observation">1.1. Key Observation</a></li><li><a href=#12-main-idea aria-label="1.2. Main Idea">1.2. Main Idea</a></li><li><a href=#13-linear-projections aria-label="1.3. Linear Projections">1.3. Linear Projections</a></li><li><a href=#14-linear-self-attention aria-label="1.4. Linear Self-Attention">1.4. Linear Self-Attention</a></li><li><a href=#15-practical-choices aria-label="1.5. Practical Choices">1.5. Practical Choices</a></li><li><a href=#16-empirical-results aria-label="1.6. Empirical Results">1.6. Empirical Results</a></li></ul></li><li><a href=#2-novelty aria-label="2. Novelty">2. Novelty</a></li></ul></div></details></div><div class=post-content><h1 id=1-method>1. Method<a hidden class=anchor aria-hidden=true href=#1-method>#</a></h1><h2 id=11-key-observation><strong>1.1. Key Observation</strong><a hidden class=anchor aria-hidden=true href=#11-key-observation>#</a></h2><p>The self-attention matrix is <strong>low-rank</strong>. Both empirical spectra and theory show that most information is concentrated in a few singular values.</p><p>$$
P = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)
$$</p><h2 id=12-main-idea><strong>1.2. Main Idea</strong><a hidden class=anchor aria-hidden=true href=#12-main-idea>#</a></h2><p>If $P$ is low-rank, then instead of computing an $n\times n$ matrix, we can approximate it using a factorization of size $n \times k$) where $k \ll n$.</p><h2 id=13-linear-projections><strong>1.3. Linear Projections</strong><a hidden class=anchor aria-hidden=true href=#13-linear-projections>#</a></h2><p>Linformer introduces two projection matrices $E, F \in R^{n \times k}$ to reduce the sequence length of <strong>keys</strong> and <strong>values</strong>:</p><p>$$
K&rsquo; = EK,\qquad V&rsquo; = FV.
$$</p><h2 id=14-linear-self-attention><strong>1.4. Linear Self-Attention</strong><a hidden class=anchor aria-hidden=true href=#14-linear-self-attention>#</a></h2><p>Attention is computed as</p><p>$$
\text{Attention}(Q, K&rsquo;, V&rsquo;) = \text{softmax}\left(\frac{Q {K&rsquo;}^\top}{\sqrt{d}}\right)V&rsquo;.
$$</p><p>This reduces complexity from $O(n^2)$ to <strong>$O(nk)$</strong>.</p><h2 id=15-practical-choices><strong>1.5. Practical Choices</strong><a hidden class=anchor aria-hidden=true href=#15-practical-choices>#</a></h2><ul><li>k is small and independent of sequence length.</li><li>Projections can be <strong>shared</strong> across heads and layers.</li></ul><h2 id=16-empirical-results><strong>1.6. Empirical Results</strong><a hidden class=anchor aria-hidden=true href=#16-empirical-results>#</a></h2><p>Linformer achieves similar or better accuracy than standard Transformers while being much faster and using much less memory.</p><h1 id=2-novelty><strong>2. Novelty</strong><a hidden class=anchor aria-hidden=true href=#2-novelty>#</a></h1><ol><li><p><strong>First to show self-attention is inherently low-rank.</strong></p><p>This gives a theoretical reason why quadratic attention is unnecessary.</p></li><li><p><strong>Introduces a simple and general low-rank factorization of attention.</strong></p><p>Instead of sparsity or hashing, Linformer directly compresses the sequence dimension.</p></li><li><p><strong>Achieves true linear complexity (O(n))</strong> in both time and memory.</p></li><li><p><strong>Projection dimension does not grow with sequence length.</strong></p><p>This enables extremely long-sequence training on standard hardware.</p></li><li><p><strong>Compatible with standard Transformer design.</strong></p><p>No special sparsity patterns, no complex algorithms. Easy to drop in.</p></li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>