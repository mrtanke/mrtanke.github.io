<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Random Search for Hyper-Parameter Optimization | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: Random Search for Hyper-Parameter Optimization"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/random_search_for_hyper-parameter_optimization/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/random_search_for_hyper-parameter_optimization/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/random_search_for_hyper-parameter_optimization/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Random Search for Hyper-Parameter Optimization"><meta property="og:description" content="Paper-reading notes: Random Search for Hyper-Parameter Optimization"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-12-10T08:35:15+00:00"><meta property="article:modified_time" content="2025-12-10T08:35:15+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/random_search_for_hyper-parameter_optimization/image.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/random_search_for_hyper-parameter_optimization/image.png"><meta name=twitter:title content="Random Search for Hyper-Parameter Optimization"><meta name=twitter:description content="Paper-reading notes: Random Search for Hyper-Parameter Optimization"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Random Search for Hyper-Parameter Optimization","item":"https://my-blog-alpha-vert.vercel.app/notes/random_search_for_hyper-parameter_optimization/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Random Search for Hyper-Parameter Optimization","name":"Random Search for Hyper-Parameter Optimization","description":"Paper-reading notes: Random Search for Hyper-Parameter Optimization","keywords":[],"articleBody":"â‰Â Why Is Hyperparameter Tuning So Difficult? Bengioâ€™s Classic Paper Tells You: Stop Using Grid Search! In machine learning, thereâ€™s a saying:\nâ€œTraining the model is easy. Tuning hyperparameters is hell.â€\nLearning rate, number of layers, regularization strength, hidden unitsâ€¦\nEverything seems important, but tuning them feels like opening blind boxes.\nToday we look at a classic paper from Yoshua Bengioâ€™s team:\nâ€œRandom Search for Hyper-Parameter Optimization.â€\nIts key message is surprisingly simple:\nRandom Search is far more efficient than Grid Search.\nItâ€™s cheaper, faster, and works especially well in high-dimensional spaces.\nLetâ€™s walk through the core ideas of this paper in a beginner-friendly way.\nâ“1. Why Is Traditional Grid Search Inefficient? The goal of hyperparameter optimization is simple:\nFind a set of hyperparameters Î» that gives the best generalization performance.\nThe usual routine:\nTry many Î» combinations Train models Evaluate on validation data Pick the best But hereâ€™s the problem: Grid Search does this:\nPick several values for each hyperparameter Try every combination As the number of hyperparameters grows, the number of combinations grows exponentially.\nğŸ“Œ The paper notes:\nGrid Search suffers badly from the curse of dimensionality.\nEven worse:\nâ— Most hyperparameters donâ€™t matter very much..\nAs shown later in the paper:\nFor a given dataset, only 1â€“3 hyperparameters significantly affect performance Many others barely change anything But Grid Search still allocates equal effort across all dimensions In simple words:\nGrid Search wastes massive compute exploring unimportant directions.\nğŸ¤“Â 2. Key Conclusion of the Paper: Random Search Is Better The authors make a bold claim:\nRandom Search is more efficient and often finds better models.\nAnd the reasoning is intuitive:\n2.1. Random Search focuses more effectively on â€œimportant dimensionsâ€ If only 1â€“2 hyperparameters matter:\nGrid Search: spreads effort unnecessarily across all dimensions Random Search: each sample covers all dimensions, increasing the chance of hitting useful values The following figure illustrates:\nIn a 2D function where only x matters, Grid Search tests only a few distinct x-values, Random Search can test many more unique x-values with the same budget. 2.2. Random samples are independent (i.i.d.) This gives several engineering advantages:\nYou can stop early You can add more trials anytime Trials naturally run in parallel, asynchronously Failed runs donâ€™t affect anything Itâ€™s simple, robust, and flexible.\nğŸ”¬ 3. Experiments Show: Random Search Finds Better Models Faster The paper reproduces classic deep learning experiments on datasets like:\nMNIST variants rectangles convex plus experiments with DBNs involving 32 hyperparameters ğŸ”¥ The results are striking: âœ” With the same compute budget: Random Search needs only:\n8 trials to match Grid Searchâ€™s performance at 100 trials 32 trials to outperform Grid Search âœ” In high-dimensional DBN tuning: Random Search performs:\nBetter on 1 dataset Comparable on 4 datasets Slightly worse on 3 datasets In short:\nRandom Search is cheaper, faster, and still high-quality.\nğŸ§  4. A Hidden Highlight: How to Fairly Evaluate the â€œBest Modelâ€ from Hyperparameter Tuning? Because the validation set is finite:\nThe hyperparameter with the best validation score might not truly be the best.\nIt may just have been â€œluckyâ€ on this particular validation split.\nThe paper proposes:\nâœ” Donâ€™t report the test performance of only the single best Î». Instead:\nâœ” Report a weighted average of test performances across all Î» candidates. The weight comes from:\nThe probability that each Î» would be the winner\n(considering validation noise and variance)\nThis gives a more robust, unbiased, and fair estimate of generalization.\nğŸ“ˆ 5. Why You Probably Should Stop Using Grid Search The conclusion states clearly:\nGrid Search wastes computation Random Search is more efficient in high-dimensional spaces Random trials are easy to parallelize Random Search should be the standard baseline for evaluating new tuning algorithms The implicit message:\nUnless you have only 1â€“2 hyperparameters, avoid Grid Search.\nâ­ 6. Practical Takeaways: Start Using Random Search for Hyperparameter Tuning! One-sentence summary of the paper:\nGrid Search is inefficient; Random Search is faster and finds better results.\nWhy Random Search works better:\nReal models have low effective dimensionality. Grid Search wastes effort in irrelevant dimensions. Random Search offers wider, more meaningful exploration. Itâ€™s parallel-friendly and simple to implement. A practical tip:\nIf you have more than 2 hyperparameters, Grid Search is almost always a bad idea.\nğŸ“Œ 7. Practical Advice for ML Engineers \u0026 Students âœ” Prefer Random Search over Grid Search (even for neural networks and classical ML)\nâœ” If budget permits Upgrade to Bayesian Optimization\nâœ” When tuning many hyperparameters (\u003e20) Use:\nRandom Search + Early Stopping\nIt often delivers surprisingly strong results.\nğŸ Final Golden Sentence Random Search isnâ€™t â€œtrying things randomlyâ€; itâ€™s statistically one of the smartest ways to tune models.\n","wordCount":"774","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/random_search_for_hyper-parameter_optimization/image.png","datePublished":"2025-12-10T08:35:15Z","dateModified":"2025-12-10T08:35:15Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/random_search_for_hyper-parameter_optimization/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;Â»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Random Search for Hyper-Parameter Optimization</h1><div class=post-description>Paper-reading notes: Random Search for Hyper-Parameter Optimization</div><div class=post-meta><span title='2025-12-10 08:35:15 +0000 +0000'>December 10, 2025</span>&nbsp;Â·&nbsp;<span>774 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#why-is-hyperparameter-tuning-so-difficult aria-label="â‰Â Why Is Hyperparameter Tuning So Difficult?">â‰Â Why Is Hyperparameter Tuning So Difficult?</a><ul><ul><li><a href=#bengios-classic-paper-tells-you-stop-using-grid-search aria-label="Bengioâ€™s Classic Paper Tells You: Stop Using Grid Search!">Bengioâ€™s Classic Paper Tells You: Stop Using Grid Search!</a></li></ul></ul></li><li><a href=#1-why-is-traditional-grid-search-inefficient aria-label="â“1. Why Is Traditional Grid Search Inefficient?">â“1. Why Is Traditional Grid Search Inefficient?</a><ul><li><a href=#but-heres-the-problem aria-label="But hereâ€™s the problem:">But hereâ€™s the problem:</a></li></ul></li><li><a href=#2-key-conclusion-of-the-paper-random-search-is-better aria-label="ğŸ¤“Â 2. Key Conclusion of the Paper: Random Search Is Better">ğŸ¤“Â 2. Key Conclusion of the Paper: Random Search Is Better</a><ul><ul><li><a href=#21-random-search-focuses-more-effectively-on-important-dimensions aria-label="2.1. Random Search focuses more effectively on â€œimportant dimensionsâ€">2.1. Random Search focuses more effectively on â€œimportant dimensionsâ€</a></li><li><a href=#22-random-samples-are-independent-iid aria-label="2.2. Random samples are independent (i.i.d.)">2.2. Random samples are independent (i.i.d.)</a></li></ul></ul></li><li><a href=#-3-experiments-show-random-search-finds-better-models-faster aria-label="ğŸ”¬ 3. Experiments Show: Random Search Finds Better Models Faster">ğŸ”¬ 3. Experiments Show: Random Search Finds Better Models Faster</a><ul><li><a href=#-the-results-are-striking aria-label="ğŸ”¥ The results are striking:">ğŸ”¥ The results are striking:</a><ul><li><a href=#-with-the-same-compute-budget aria-label="âœ” With the same compute budget:">âœ” With the same compute budget:</a></li><li><a href=#-in-high-dimensional-dbn-tuning aria-label="âœ” In high-dimensional DBN tuning:">âœ” In high-dimensional DBN tuning:</a></li></ul></li></ul></li><li><a href=#-4-a-hidden-highlight aria-label="ğŸ§  4. A Hidden Highlight:">ğŸ§  4. A Hidden Highlight:</a><ul><li><a href=#how-to-fairly-evaluate-the-best-model-from-hyperparameter-tuning aria-label="How to Fairly Evaluate the â€œBest Modelâ€ from Hyperparameter Tuning?">How to Fairly Evaluate the â€œBest Modelâ€ from Hyperparameter Tuning?</a><ul><li><a href=#-dont-report-the-test-performance-of-only-the-single-best-%ce%bb aria-label="âœ” Donâ€™t report the test performance of only the single best Î».">âœ” Donâ€™t report the test performance of only the single best Î».</a></li><li><a href=#-report-a-weighted-average-of-test-performances-across-all-%ce%bb-candidates aria-label="âœ” Report a weighted average of test performances across all Î» candidates.">âœ” Report a weighted average of test performances across all Î» candidates.</a></li></ul></li></ul></li><li><a href=#-5-why-you-probably-should-stop-using-grid-search aria-label="ğŸ“ˆ 5. Why You Probably Should Stop Using Grid Search">ğŸ“ˆ 5. Why You Probably Should Stop Using Grid Search</a></li><li><a href=#-6-practical-takeaways aria-label="â­ 6. Practical Takeaways:">â­ 6. Practical Takeaways:</a><ul><li><a href=#start-using-random-search-for-hyperparameter-tuning aria-label="Start Using Random Search for Hyperparameter Tuning!">Start Using Random Search for Hyperparameter Tuning!</a></li></ul></li><li><a href=#-7-practical-advice-for-ml-engineers--students aria-label="ğŸ“Œ 7. Practical Advice for ML Engineers & Students">ğŸ“Œ 7. Practical Advice for ML Engineers & Students</a><ul><ul><li><a href=#-prefer-random-search-over-grid-search aria-label="âœ” Prefer Random Search over Grid Search">âœ” Prefer Random Search over Grid Search</a></li><li><a href=#-if-budget-permits aria-label="âœ” If budget permits">âœ” If budget permits</a></li><li><a href=#-when-tuning-many-hyperparameters-20 aria-label="âœ” When tuning many hyperparameters (>20)">âœ” When tuning many hyperparameters (>20)</a></li></ul></ul></li><li><a href=#-final-golden-sentence aria-label="ğŸ Final Golden Sentence">ğŸ Final Golden Sentence</a></li></ul></div></details></div><div class=post-content><h1 id=why-is-hyperparameter-tuning-so-difficult>â‰Â Why Is Hyperparameter Tuning So Difficult?<a hidden class=anchor aria-hidden=true href=#why-is-hyperparameter-tuning-so-difficult>#</a></h1><h3 id=bengios-classic-paper-tells-you-stop-using-grid-search>Bengioâ€™s Classic Paper Tells You: <strong>Stop Using Grid Search!</strong><a hidden class=anchor aria-hidden=true href=#bengios-classic-paper-tells-you-stop-using-grid-search>#</a></h3><p>In machine learning, thereâ€™s a saying:</p><blockquote><p>â€œTraining the model is easy. Tuning hyperparameters is hell.â€</p></blockquote><p>Learning rate, number of layers, regularization strength, hidden unitsâ€¦</p><p>Everything seems important, but tuning them feels like opening blind boxes.</p><p>Today we look at a classic paper from Yoshua Bengioâ€™s team:</p><p><strong>â€œRandom Search for Hyper-Parameter Optimization.â€</strong></p><p>Its key message is surprisingly simple:</p><blockquote><p>Random Search is far more efficient than Grid Search.</p></blockquote><p>Itâ€™s cheaper, faster, and works especially well in high-dimensional spaces.</p><p>Letâ€™s walk through the core ideas of this paper in a beginner-friendly way.</p><h1 id=1-why-is-traditional-grid-search-inefficient>â“1. Why Is Traditional Grid Search Inefficient?<a hidden class=anchor aria-hidden=true href=#1-why-is-traditional-grid-search-inefficient>#</a></h1><p>The goal of hyperparameter optimization is simple:</p><blockquote><p>Find a set of hyperparameters Î» that gives the best generalization performance.</p></blockquote><p>The usual routine:</p><ul><li>Try many Î» combinations</li><li>Train models</li><li>Evaluate on validation data</li><li>Pick the best</li></ul><h2 id=but-heres-the-problem>But hereâ€™s the problem:<a hidden class=anchor aria-hidden=true href=#but-heres-the-problem>#</a></h2><p>Grid Search does this:</p><ul><li>Pick several values for each hyperparameter</li><li>Try <strong>every combination</strong></li></ul><p>As the number of hyperparameters grows, the number of combinations grows <strong>exponentially</strong>.</p><p>ğŸ“Œ The paper notes:</p><blockquote><p>Grid Search suffers badly from the curse of dimensionality.</p></blockquote><p>Even worse:</p><aside><p>â— Most hyperparameters donâ€™t matter very much..</p></aside><p>As shown later in the paper:</p><ul><li>For a given dataset, only 1â€“3 hyperparameters significantly affect performance</li><li>Many others barely change anything</li><li>But Grid Search still allocates equal effort across all dimensions</li></ul><p>In simple words:</p><blockquote><p>Grid Search wastes massive compute exploring unimportant directions.</p></blockquote><h1 id=2-key-conclusion-of-the-paper-random-search-is-better>ğŸ¤“Â 2. Key Conclusion of the Paper: <strong>Random Search Is Better</strong><a hidden class=anchor aria-hidden=true href=#2-key-conclusion-of-the-paper-random-search-is-better>#</a></h1><p>The authors make a bold claim:</p><blockquote><p>Random Search is more efficient and often finds better models.</p></blockquote><p>And the reasoning is intuitive:</p><h3 id=21-random-search-focuses-more-effectively-on-important-dimensions>2.1. Random Search focuses more effectively on â€œimportant dimensionsâ€<a hidden class=anchor aria-hidden=true href=#21-random-search-focuses-more-effectively-on-important-dimensions>#</a></h3><p>If only 1â€“2 hyperparameters matter:</p><ul><li>Grid Search: spreads effort unnecessarily across all dimensions</li><li>Random Search: each sample covers all dimensions, increasing the chance of hitting useful values</li></ul><p>The following figure illustrates:</p><ul><li>In a 2D function where only x matters,</li><li>Grid Search tests only a few distinct x-values,</li><li>Random Search can test many more unique x-values with the same budget.</li></ul><p><img alt=image.png loading=lazy src=/notes/random_search_for_hyper-parameter_optimization/image.png></p><h3 id=22-random-samples-are-independent-iid>2.2. Random samples are independent (i.i.d.)<a hidden class=anchor aria-hidden=true href=#22-random-samples-are-independent-iid>#</a></h3><p>This gives several engineering advantages:</p><ul><li>You can stop early</li><li>You can add more trials anytime</li><li>Trials naturally run in parallel, asynchronously</li><li>Failed runs donâ€™t affect anything</li></ul><p>Itâ€™s simple, robust, and flexible.</p><h1 id=-3-experiments-show-random-search-finds-better-models-faster>ğŸ”¬ 3. Experiments Show: Random Search Finds Better Models Faster<a hidden class=anchor aria-hidden=true href=#-3-experiments-show-random-search-finds-better-models-faster>#</a></h1><p>The paper reproduces classic deep learning experiments on datasets like:</p><ul><li>MNIST variants</li><li>rectangles</li><li>convex</li><li>plus experiments with DBNs involving 32 hyperparameters</li></ul><h2 id=-the-results-are-striking>ğŸ”¥ The results are striking:<a hidden class=anchor aria-hidden=true href=#-the-results-are-striking>#</a></h2><h3 id=-with-the-same-compute-budget>âœ” With the same compute budget:<a hidden class=anchor aria-hidden=true href=#-with-the-same-compute-budget>#</a></h3><p>Random Search needs only:</p><ul><li><strong>8 trials to match Grid Searchâ€™s performance at 100 trials</strong></li><li><strong>32 trials to outperform Grid Search</strong></li></ul><h3 id=-in-high-dimensional-dbn-tuning>âœ” In high-dimensional DBN tuning:<a hidden class=anchor aria-hidden=true href=#-in-high-dimensional-dbn-tuning>#</a></h3><p>Random Search performs:</p><ul><li>Better on 1 dataset</li><li>Comparable on 4 datasets</li><li>Slightly worse on 3 datasets</li></ul><p>In short:</p><blockquote><p>Random Search is cheaper, faster, and still high-quality.</p></blockquote><h1 id=-4-a-hidden-highlight>ğŸ§  4. A Hidden Highlight:<a hidden class=anchor aria-hidden=true href=#-4-a-hidden-highlight>#</a></h1><h2 id=how-to-fairly-evaluate-the-best-model-from-hyperparameter-tuning>How to Fairly Evaluate the â€œBest Modelâ€ from Hyperparameter Tuning?<a hidden class=anchor aria-hidden=true href=#how-to-fairly-evaluate-the-best-model-from-hyperparameter-tuning>#</a></h2><p>Because the validation set is finite:</p><blockquote><p>The hyperparameter with the best validation score might not truly be the best.</p><p>It may just have been â€œluckyâ€ on this particular validation split.</p></blockquote><p>The paper proposes:</p><h3 id=-dont-report-the-test-performance-of-only-the-single-best-Î»>âœ” Donâ€™t report the test performance of only the single best Î».<a hidden class=anchor aria-hidden=true href=#-dont-report-the-test-performance-of-only-the-single-best-Î»>#</a></h3><p>Instead:</p><h3 id=-report-a-weighted-average-of-test-performances-across-all-Î»-candidates>âœ” Report a weighted average of test performances across all Î» candidates.<a hidden class=anchor aria-hidden=true href=#-report-a-weighted-average-of-test-performances-across-all-Î»-candidates>#</a></h3><p>The weight comes from:</p><ul><li><p>The probability that each Î» would be the winner</p><p>(considering validation noise and variance)</p></li></ul><p>This gives a more <strong>robust</strong>, <strong>unbiased</strong>, and <strong>fair</strong> estimate of generalization.</p><h1 id=-5-why-you-probably-should-stop-using-grid-search>ğŸ“ˆ 5. Why You Probably Should Stop Using Grid Search<a hidden class=anchor aria-hidden=true href=#-5-why-you-probably-should-stop-using-grid-search>#</a></h1><p>The conclusion states clearly:</p><ul><li>Grid Search wastes computation</li><li>Random Search is more efficient in high-dimensional spaces</li><li>Random trials are easy to parallelize</li><li>Random Search should be the standard baseline for evaluating new tuning algorithms</li></ul><p>The implicit message:</p><blockquote><p>Unless you have only 1â€“2 hyperparameters, avoid Grid Search.</p></blockquote><h1 id=-6-practical-takeaways>â­ 6. Practical Takeaways:<a hidden class=anchor aria-hidden=true href=#-6-practical-takeaways>#</a></h1><h2 id=start-using-random-search-for-hyperparameter-tuning>Start Using Random Search for Hyperparameter Tuning!<a hidden class=anchor aria-hidden=true href=#start-using-random-search-for-hyperparameter-tuning>#</a></h2><p><strong>One-sentence summary of the paper:</strong></p><blockquote><p>Grid Search is inefficient; Random Search is faster and finds better results.</p></blockquote><p><strong>Why Random Search works better:</strong></p><ol><li>Real models have low effective dimensionality.</li><li>Grid Search wastes effort in irrelevant dimensions.</li><li>Random Search offers wider, more meaningful exploration.</li><li>Itâ€™s parallel-friendly and simple to implement.</li></ol><p><strong>A practical tip:</strong></p><blockquote><p>If you have more than 2 hyperparameters, Grid Search is almost always a bad idea.</p></blockquote><h1 id=-7-practical-advice-for-ml-engineers--students>ğŸ“Œ 7. Practical Advice for ML Engineers & Students<a hidden class=anchor aria-hidden=true href=#-7-practical-advice-for-ml-engineers--students>#</a></h1><h3 id=-prefer-random-search-over-grid-search>âœ” Prefer Random Search over Grid Search<a hidden class=anchor aria-hidden=true href=#-prefer-random-search-over-grid-search>#</a></h3><p>(even for neural networks and classical ML)</p><h3 id=-if-budget-permits>âœ” If budget permits<a hidden class=anchor aria-hidden=true href=#-if-budget-permits>#</a></h3><p>Upgrade to <strong>Bayesian Optimization</strong></p><h3 id=-when-tuning-many-hyperparameters-20>âœ” When tuning many hyperparameters (>20)<a hidden class=anchor aria-hidden=true href=#-when-tuning-many-hyperparameters-20>#</a></h3><p>Use:</p><p><strong>Random Search + Early Stopping</strong></p><p>It often delivers surprisingly strong results.</p><h1 id=-final-golden-sentence>ğŸ Final Golden Sentence<a hidden class=anchor aria-hidden=true href=#-final-golden-sentence>#</a></h1><blockquote><p>Random Search isnâ€™t â€œtrying things randomlyâ€; itâ€™s statistically one of the smartest ways to tune models.</p></blockquote></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>