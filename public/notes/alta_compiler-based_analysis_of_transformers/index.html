<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ALTA: Compiler-Based Analysis of Transformers | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: ALTA"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/"><meta property="og:site_name" content="Home"><meta property="og:title" content="ALTA: Compiler-Based Analysis of Transformers"><meta property="og:description" content="Paper-reading notes: ALTA"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-12-09T08:34:00+00:00"><meta property="article:modified_time" content="2025-12-09T08:34:00+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/image_1.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/image_2.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/image_3.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/image_4.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/image.png"><meta name=twitter:title content="ALTA: Compiler-Based Analysis of Transformers"><meta name=twitter:description content="Paper-reading notes: ALTA"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"ALTA: Compiler-Based Analysis of Transformers","item":"https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ALTA: Compiler-Based Analysis of Transformers","name":"ALTA: Compiler-Based Analysis of Transformers","description":"Paper-reading notes: ALTA","keywords":[],"articleBody":"ALTA is a new programming language and a compiler that can map ALTA programs to Transformer weights.\nIt can help clearly analyze what algorithms Transformers can represent, why sometimes fail to learn them, and how to design models that generalize compositionally.\n1. Proposed Framework 1.1. Overview An ALTA program specification includes three key components:\na set of variables, a set of attention heads, a “MLP function” To mirror Transformer computation.\nAn ALTA Program Example - parity\nA Transformer can implement this parity algorithm because:\nResidual stream = variables stored inside the hidden vector\n(“store parity and done as part of the hidden state”)\nAttention = lookup from other tokens\n(“give me the value from the previous token”)\nFFN/MLP = local computation\n(“compute new parity from old parity”)\nThe ALTA framework includes:\nan interpreter symbolically executes a program, a compiler compiles programs to Transformer weights. Symbolic representation:\nan ALTA program: P input: a sequence of integers X output: a sequence of integers Y Interpreter → func I: P x X → Y\nCompiler → func C: θ = C(P)\nT(x, θ) ≈ I(P, x)\nT: a transformer encoder 1.2. Variables There are three kinds of variables ALTA supports:\nCategorical variables Value is from a small finite set (e.g., {0,1}, {A,B,C}) Represented as one-hot vectors inside the Transformer Numerical variables Real-number values Represented as a single neuron (one scalar dimension) Set variables Values are sets of integers (e.g., {1, 4, 7}) Represented as multi-hot vectors Attention-head outputs as variables Some variables come from attention heads These can take a null / undefined value (e.g., when a head attends to no token) One residual block\nThe residual stream is the single vector per token that all sublayers read and write.\n1.3. Execution Attention affects inter-token variables:\nparity_left, done_left Any variable whose value comes from other tokens MLP affects intra-token variables:\nparity, done Other algorithmic state variables ALTA represents each token’s state with symbolic variables, which correspond to segments of the Transformer’s residual vector.\nFor layer k, token i:\nz^k_{} z^k_{} z^k_{} z^k_{} …\nInitialization: Input and position-dependent variables are encoded directly into the initial residual stream using embeddings.\nresidual[i] = [ idx_onehot(0..2) | idx_left_onehot(0..2) | parity | done ]\nPer-layer execution Self-attention:\nselect(query, key) builds a binary attention pattern based on variable equality. aggregate collects the corresponding value variables. This produces inter-token variables like parity_left. MLP:\nDefined by symbolic transition rules. Compiler turns these into a fixed 4-layer ReLU network that updates variables (e.g., parity, done). Layer1 → ReLU → Layer2 → ReLU → Layer3 → ReLU → Layer4 → ReLU\nThe first 2 layers are only responsible for converting numerical/set variables into one-hot bucket representations.; The final 2 layers are based on the set of transition rules. Residual updates: Attention and MLP outputs are added to the residual stream, as in standard Transformers.\nHalting \u0026 output: Execution stops when a halting condition is met; the specified output variable is read from the final residual stream.\n2. Expressibility and Learnability 2.1. Expressibility ALTA shows constructively that Transformers can exactly implement algorithms like parity, addition, and SCAN.\n2.2. Learnability Even if an algorithm is expressible by a Transformer, training on input–output pairs may fail to learn it.\nALTA provides two tools to analyze and improve learnability:\nTrace supervision: Use ALTA’s intermediate states as supervision to help training follow the intended algorithm. Minimality analysis: A program must be minimal on the training set for the compiled Transformer weights to be a stable solution; non-minimal programs lead to unstable or incorrect learning. 3. ALTA Summary Six sentences to summarize ALTA:\nALTA is a formal programming language that describes the computation inside a transformer in a human-interpretable way. It converts opaque neural operations into clear algorithmic steps. ALTA includes a compiler that translates an ALTA program into actual transformer weights (embeddings, attention, FFN). ALTA programs use explicit symbolic variables (e.g., idx, parity, done, parity_left) to track interpretable token-level states throughout the computation. Attention in ALTA implements inter-token communication, enabling a token to read variables from other positions (e.g., computing left-side context like parity_left). FFNs implement intra-token updates, modifying the variables of each token based only on its own state (e.g., updating parity or flags). Because attention moves information only one step per layer, n tokens often require O(n)(n-1) layers for full information propagation. ","wordCount":"720","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/image.png","datePublished":"2025-12-09T08:34:00Z","dateModified":"2025-12-09T08:34:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/alta_compiler-based_analysis_of_transformers/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">ALTA: Compiler-Based Analysis of Transformers</h1><div class=post-description>Paper-reading notes: ALTA</div><div class=post-meta><span title='2025-12-09 08:34:00 +0000 +0000'>December 9, 2025</span>&nbsp;·&nbsp;<span>720 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-proposed-framework aria-label="1. Proposed Framework">1. Proposed Framework</a><ul><li><a href=#11-overview aria-label="1.1. Overview">1.1. Overview</a></li><li><a href=#12-variables aria-label="1.2. Variables">1.2. Variables</a></li><li><a href=#13-execution aria-label="1.3. Execution">1.3. Execution</a><ul><li><a href=#per-layer-execution aria-label="Per-layer execution">Per-layer execution</a></li></ul></li></ul></li><li><a href=#2-expressibility-and-learnability aria-label="2. Expressibility and Learnability">2. Expressibility and Learnability</a><ul><ul><li><a href=#21-expressibility aria-label="2.1. Expressibility">2.1. Expressibility</a></li><li><a href=#22-learnability aria-label="2.2. Learnability">2.2. Learnability</a></li></ul></ul></li><li><a href=#3-alta-summary aria-label="3. ALTA Summary">3. ALTA Summary</a></li></ul></div></details></div><div class=post-content><p>ALTA is a new programming language and a compiler that can map ALTA programs to Transformer weights.</p><p>It can help clearly analyze what algorithms Transformers can represent, why sometimes fail to learn them, and how to design models that generalize compositionally.</p><h1 id=1-proposed-framework>1. Proposed Framework<a hidden class=anchor aria-hidden=true href=#1-proposed-framework>#</a></h1><h2 id=11-overview>1.1. Overview<a hidden class=anchor aria-hidden=true href=#11-overview>#</a></h2><aside><p>An ALTA program specification includes three key components:</p><ul><li>a set of <strong>variables</strong>,</li><li>a set of <strong>attention heads</strong>,</li><li>a “<strong>MLP</strong> function”</li></ul><p>To mirror Transformer computation.</p></aside><p><img alt="An ALTA Program Example - parity" loading=lazy src=/notes/alta_compiler-based_analysis_of_transformers/image.png></p><p>An ALTA Program Example - parity</p><p>A Transformer can implement this parity algorithm because:</p><ol><li><p><strong>Residual stream = variables stored inside the hidden vector</strong></p><p>(“store parity and done as part of the hidden state”)</p></li><li><p><strong>Attention = lookup from other tokens</strong></p><p>(“give me the value from the previous token”)</p></li><li><p><strong>FFN/MLP = local computation</strong></p><p>(“compute new parity from old parity”)</p></li></ol><hr><p>The ALTA framework includes:</p><ul><li>an interpreter<ul><li>symbolically executes a program,</li></ul></li><li>a compiler<ul><li>compiles programs to Transformer weights.</li></ul></li></ul><aside><p><strong>Symbolic representation:</strong></p><ul><li>an ALTA program: P</li><li>input: a sequence of integers X</li><li>output: a sequence of integers Y</li></ul><hr><p>Interpreter → func I: P x X → Y</p><p>Compiler → func C: θ = C(P)</p><hr><p><strong>T(x, θ) ≈ I(P, x)</strong></p><ul><li>T: a transformer encoder</li></ul></aside><h2 id=12-variables>1.2. Variables<a hidden class=anchor aria-hidden=true href=#12-variables>#</a></h2><p>There are <strong>three kinds of variables</strong> ALTA supports:</p><ol><li><strong>Categorical variables</strong><ul><li>Value is from a small finite set (e.g., <code>{0,1}</code>, <code>{A,B,C}</code>)</li><li>Represented as <strong>one-hot vectors</strong> inside the Transformer</li></ul></li><li><strong>Numerical variables</strong><ul><li>Real-number values</li><li>Represented as <strong>a single neuron</strong> (one scalar dimension)</li></ul></li><li><strong>Set variables</strong><ul><li>Values are sets of integers (e.g., <code>{1, 4, 7}</code>)</li><li>Represented as <strong>multi-hot vectors</strong></li></ul></li><li>Attention-head outputs as variables<ul><li>Some variables come from attention heads</li><li>These can take a <strong>null / undefined</strong> value (e.g., when a head attends to no token)</li></ul></li></ol><p><img alt="One residual block" loading=lazy src=/notes/alta_compiler-based_analysis_of_transformers/image_1.png></p><p>One residual block</p><p>The residual stream is the single vector per token that all sublayers read and write.</p><h2 id=13-execution>1.3. <strong>Execution</strong><a hidden class=anchor aria-hidden=true href=#13-execution>#</a></h2><p><img alt=image.png loading=lazy src=/notes/alta_compiler-based_analysis_of_transformers/image_2.png></p><aside><p><strong>Attention</strong> affects inter-token variables:</p><ul><li>parity_left, done_left</li><li>Any variable whose value comes from other tokens</li></ul><p><strong>MLP</strong> affects intra-token variables:</p><ul><li>parity, done</li><li>Other algorithmic state variables</li></ul></aside><hr><p>ALTA represents each token’s state with symbolic variables, which correspond to segments of the Transformer’s residual vector.</p><aside><p>For layer k, token i:</p><p>z^k_{&lt;i, idx>}
z^k_{&lt;i, idx_left>}
z^k_{&lt;i, parity>}
z^k_{&lt;i, done>}
&mldr;</p></aside><p><strong>Initialization:</strong> Input and position-dependent variables are encoded directly into the initial residual stream using embeddings.</p><aside><p>residual[i] = [ idx_onehot(0..2) | idx_left_onehot(0..2) | parity | done ]</p></aside><p><img alt=image.png loading=lazy src=/notes/alta_compiler-based_analysis_of_transformers/image_3.png></p><h3 id=per-layer-execution><strong>Per-layer execution</strong><a hidden class=anchor aria-hidden=true href=#per-layer-execution>#</a></h3><p><strong>Self-attention:</strong></p><ul><li><code>select(query, key)</code> builds a binary attention pattern based on variable equality.</li><li><code>aggregate</code> collects the corresponding <code>value</code> variables.</li><li>This produces inter-token variables like <code>parity_left</code>.</li></ul><p><img alt=image.png loading=lazy src=/notes/alta_compiler-based_analysis_of_transformers/image_4.png></p><p><strong>MLP:</strong></p><ul><li>Defined by symbolic <strong>transition rules</strong>.</li><li>Compiler turns these into a fixed 4-layer ReLU network that updates variables (e.g., <code>parity</code>, <code>done</code>).</li></ul><aside><p>Layer1 → ReLU → Layer2 → ReLU → Layer3 → ReLU → Layer4 → ReLU</p><ul><li>The first 2 layers are only responsible for converting numerical/set variables into <strong>one-hot</strong> bucket representations.;</li><li>The final 2 layers are based on the set of <strong>transition rules</strong>.</li></ul></aside><p><strong>Residual updates:</strong> Attention and MLP outputs are added to the residual stream, as in standard Transformers.</p><p><strong>Halting & output:</strong> Execution stops when a halting condition is met; the specified output variable is read from the final residual stream.</p><h1 id=2-expressibility-and-learnability>2. Expressibility and Learnability<a hidden class=anchor aria-hidden=true href=#2-expressibility-and-learnability>#</a></h1><h3 id=21-expressibility><strong>2.1. Expressibility</strong><a hidden class=anchor aria-hidden=true href=#21-expressibility>#</a></h3><p>ALTA shows constructively that Transformers can exactly implement algorithms like parity, addition, and SCAN.</p><h3 id=22-learnability><strong>2.2. Learnability</strong><a hidden class=anchor aria-hidden=true href=#22-learnability>#</a></h3><p>Even if an algorithm is expressible by a Transformer, <strong>training</strong> on input–output pairs may fail to learn it.</p><p>ALTA provides two tools to analyze and improve learnability:</p><ol><li><strong>Trace supervision:</strong> Use ALTA’s intermediate states as supervision to help training follow the intended algorithm.</li><li><strong>Minimality analysis:</strong> A program must be <strong>minimal</strong> on the training set for the compiled Transformer weights to be a stable solution; non-minimal programs lead to unstable or incorrect learning.</li></ol><h1 id=3-alta-summary>3. <strong>ALTA Summary</strong><a hidden class=anchor aria-hidden=true href=#3-alta-summary>#</a></h1><p>Six sentences to summarize ALTA:</p><ol><li><strong>ALTA is a formal programming language that describes the computation inside a transformer in a human-interpretable way.</strong> It converts opaque neural operations into clear algorithmic steps.</li><li><strong>ALTA includes a compiler that translates an ALTA program into actual transformer weights</strong> (embeddings, attention, FFN).</li><li><strong>ALTA programs use explicit symbolic variables</strong> (e.g., idx, parity, done, parity_left) to track interpretable token-level states throughout the computation.</li><li><strong>Attention in ALTA implements inter-token communication</strong>, enabling a token to read variables from other positions (e.g., computing left-side context like parity_left).</li><li><strong>FFNs implement intra-token updates</strong>, modifying the variables of each token based only on its own state (e.g., updating parity or flags).</li><li><strong>Because attention moves information only one step per layer, n tokens often require O(n)(n-1) layers</strong> for full information propagation.</li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>