<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Retentive Network: A Successor to Transformer for Large Language Models | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: RetNet"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/retentive_network_a_successor_to_transformer_for_l/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/retentive_network_a_successor_to_transformer_for_l/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/retentive_network_a_successor_to_transformer_for_l/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Retentive Network: A Successor to Transformer for Large Language Models"><meta property="og:description" content="Paper-reading notes: RetNet"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-11-11T10:20:33+00:00"><meta property="article:modified_time" content="2025-11-11T10:20:33+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/retentive_network_a_successor_to_transformer_for_l/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/retentive_network_a_successor_to_transformer_for_l/image_1.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/retentive_network_a_successor_to_transformer_for_l/image.png"><meta name=twitter:title content="Retentive Network: A Successor to Transformer for Large Language Models"><meta name=twitter:description content="Paper-reading notes: RetNet"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Retentive Network: A Successor to Transformer for Large Language Models","item":"https://my-blog-alpha-vert.vercel.app/notes/retentive_network_a_successor_to_transformer_for_l/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Retentive Network: A Successor to Transformer for Large Language Models","name":"Retentive Network: A Successor to Transformer for Large Language Models","description":"Paper-reading notes: RetNet","keywords":[],"articleBody":"Introduction Transformer was introduced for its strong performance and efficient parallel training, but its inference cost remains high. This creates an ‚Äúimpossible triangle‚Äù, where the three dimentions can not be balanced simultaneously. Retentive networks (RetNet) make it possible!\nMethod For an L-layer retention network, the author stacks multi-scale retention (MSR) and feed-forward network(FFN) to build the model.\nFormally, the input sequence $x_i$ is transformed to vectors by a word embedding layer. Use the packed embeddings $X^0 = [x_1, ¬∑ ¬∑ ¬∑ , x_{|x|}] ‚àà R^{|x|√ód_{model}}$ as the input and compute the model output $X^L$:\n$$ Y^l = MSR(LN(X^l)) +X^l $$\n$$ X^{l+1} = FFN(LN(Y^l)) +Y^l $$\nwhere LN(¬∑) is LayerNorm. The FFN part is computed as $FFN(X) = gelu(XW_1)W_2$, where W1, W2 are parameter matrices.\nRetention is a new way for a model to remember previous tokens ‚Äî similar to attention, but faster. It combines parallel and recurrent forms:\nDuring training, it runs in parallel (like attention, efficient on GPUs). During inference, it runs recurrently, step by step, like an RNN. The core idea:\nInstead of learning complex attention weights with softmax, Retention uses a simple exponential decay to remember past information which have already combined the weighted values from past tokens. Form How it computes Best for Speed Memory use Key idea Parallel All tokens at once Training ‚ö° Fast üíæ High Full GPU parallelism Recurrent One token at a time Inference üê¢ Slower üß† Very low Step-by-step memory Chunkwise In chunks (hybrid) Long sequences ‚öñÔ∏è Balanced ‚öñÔ∏è Medium Parallel + Recurrent combo 1. Parallel representation (for training) $$ \\text{Retention}(X) = (QK^T \\odot D)V $$\nThis version computes all tokens at once using GPU-friendly matrix operations.\nIt‚Äôs equivalent to the unrolled version of the recurrence:\n$$ s_n = \\sum_{m=1}^{n} Œ≥^{n-m} K_m^T V_m $$\nIt‚Äôs called parallel because every token‚Äôs output $o_n$ can be computed in parallel.\n‚úÖ Best for training ‚Äî fast on GPUs, easy for backpropagation\n‚ùå Needs more memory (because you must store all tokens)\n2. Recurrent representation (for inference) $$ s_n = Œ≥ s_{n-1} + K_n^T V_n, \\quad o_n = Q_n s_n $$\nThis computes one token at a time, keeping a single running memory $s_n$. It doesn‚Äôt need to access all past tokens ‚Äî only the previous memory. So it‚Äôs much more memory-efficient. ‚úÖ Best for inference / streaming ‚Äî efficient step-by-step processing\n‚ùå Slower for training because you can‚Äôt parallelize easily\n3. Chunkwise recurrent representation (for long sequences) $$ \\text{Parallel inside each chunk}, \\quad \\text{Recurrent between chunks} $$\nFor long sequences (e.g., thousands of tokens), full parallel mode is too big for GPU memory. So RetNet splits the input into chunks (e.g., 512 tokens). Inside each chunk ‚Üí compute in parallel (fast). Between chunks ‚Üí pass memory recurrently (keep context). ‚úÖ Best trade-off ‚Äî combines GPU efficiency with long-context capability\n‚ùå Slightly more complex to implement\n","wordCount":"472","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/retentive_network_a_successor_to_transformer_for_l/image.png","datePublished":"2025-11-11T10:20:33Z","dateModified":"2025-11-11T10:20:33Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/retentive_network_a_successor_to_transformer_for_l/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;¬ª&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Retentive Network: A Successor to Transformer for Large Language Models</h1><div class=post-description>Paper-reading notes: RetNet</div><div class=post-meta><span title='2025-11-11 10:20:33 +0000 +0000'>November 11, 2025</span>&nbsp;¬∑&nbsp;<span>472 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#method aria-label=Method>Method</a><ul><li><a href=#1-parallel-representation-for-training aria-label="1. Parallel representation (for training)">1. Parallel representation (for training)</a></li><li><a href=#2-recurrent-representation-for-inference aria-label="2. Recurrent representation (for inference)">2. Recurrent representation (for inference)</a></li><li><a href=#3-chunkwise-recurrent-representation-for-long-sequences aria-label="3. Chunkwise recurrent representation (for long sequences)">3. Chunkwise recurrent representation (for long sequences)</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Transformer was introduced for its strong performance and efficient parallel training, but its inference cost remains high. This creates an ‚Äúimpossible triangle‚Äù, where the three dimentions can not be balanced simultaneously. <strong>Retentive networks (RetNet)</strong> make it possible!</p><p><img alt=image.png loading=lazy src=/notes/retentive_network_a_successor_to_transformer_for_l/image.png></p><h1 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h1><p>For an L-layer retention network, the author stacks <strong>multi-scale retention (MSR)</strong> and <strong>feed-forward network(FFN)</strong> to build the model.</p><p>Formally, the input sequence $x_i$ is transformed to vectors by a word embedding layer. Use the packed embeddings $X^0 = [x_1, ¬∑ ¬∑ ¬∑ , x_{|x|}] ‚àà R^{|x|√ód_{model}}$ as the input and compute the model output $X^L$:</p><p>$$
Y^l = MSR(LN(X^l)) +X^l
$$</p><p>$$
X^{l+1} = FFN(LN(Y^l)) +Y^l
$$</p><p>where LN(¬∑) is LayerNorm. The FFN part is computed as $FFN(X) = gelu(XW_1)W_2$, where W1, W2 are parameter matrices.</p><hr><aside><p>Retention is a new way for a model to <strong>remember previous tokens</strong> ‚Äî similar to attention, but faster. It combines <strong>parallel</strong> and <strong>recurrent</strong> forms:</p><ul><li>During <strong>training</strong>, it runs in <strong>parallel</strong> (like attention, efficient on GPUs).</li><li>During <strong>inference</strong>, it runs <strong>recurrently</strong>, step by step, like an RNN.</li></ul><p><strong>The core idea</strong>:</p><ul><li>Instead of learning complex attention weights with <strong>softmax</strong>,</li><li>Retention uses a simple <strong>exponential decay</strong> to remember past information which have already combined the weighted values from past tokens.</li></ul></aside><p><img alt=image.png loading=lazy src=/notes/retentive_network_a_successor_to_transformer_for_l/image_1.png></p><table><thead><tr><th>Form</th><th>How it computes</th><th>Best for</th><th>Speed</th><th>Memory use</th><th>Key idea</th></tr></thead><tbody><tr><td><strong>Parallel</strong></td><td>All tokens at once</td><td>Training</td><td>‚ö° Fast</td><td>üíæ High</td><td>Full GPU parallelism</td></tr><tr><td><strong>Recurrent</strong></td><td>One token at a time</td><td>Inference</td><td>üê¢ Slower</td><td>üß† Very low</td><td>Step-by-step memory</td></tr><tr><td><strong>Chunkwise</strong></td><td>In chunks (hybrid)</td><td>Long sequences</td><td>‚öñÔ∏è Balanced</td><td>‚öñÔ∏è Medium</td><td>Parallel + Recurrent combo</td></tr></tbody></table><h2 id=1-parallel-representation-for-training>1. Parallel representation (for training)<a hidden class=anchor aria-hidden=true href=#1-parallel-representation-for-training>#</a></h2><p>$$
\text{Retention}(X) = (QK^T \odot D)V
$$</p><ul><li><p>This version computes all tokens <strong>at once</strong> using GPU-friendly matrix operations.</p></li><li><p>It‚Äôs equivalent to the unrolled version of the recurrence:</p><p>$$
s_n = \sum_{m=1}^{n} Œ≥^{n-m} K_m^T V_m
$$</p></li><li><p>It‚Äôs called <strong>parallel</strong> because every token‚Äôs output $o_n$ can be computed in parallel.</p></li></ul><p>‚úÖ Best for <strong>training</strong> ‚Äî fast on GPUs, easy for backpropagation</p><p>‚ùå Needs more memory (because you must store all tokens)</p><h2 id=2-recurrent-representation-for-inference>2. Recurrent representation (for inference)<a hidden class=anchor aria-hidden=true href=#2-recurrent-representation-for-inference>#</a></h2><p>$$
s_n = Œ≥ s_{n-1} + K_n^T V_n, \quad o_n = Q_n s_n
$$</p><ul><li>This computes one token at a time, keeping a single running memory $s_n$.</li><li>It doesn‚Äôt need to access all past tokens ‚Äî only the <strong>previous memory</strong>.</li><li>So it‚Äôs much more memory-efficient.</li></ul><p>‚úÖ Best for <strong>inference / streaming</strong> ‚Äî efficient step-by-step processing</p><p>‚ùå Slower for training because you can‚Äôt parallelize easily</p><h2 id=3-chunkwise-recurrent-representation-for-long-sequences>3. Chunkwise recurrent representation (for long sequences)<a hidden class=anchor aria-hidden=true href=#3-chunkwise-recurrent-representation-for-long-sequences>#</a></h2><p>$$
\text{Parallel inside each chunk}, \quad \text{Recurrent between chunks}
$$</p><ul><li>For long sequences (e.g., thousands of tokens), full parallel mode is too big for GPU memory.</li><li>So RetNet splits the input into <strong>chunks</strong> (e.g., 512 tokens).</li><li>Inside each chunk ‚Üí compute in parallel (fast).</li><li>Between chunks ‚Üí pass memory recurrently (keep context).</li></ul><p>‚úÖ Best trade-off ‚Äî combines GPU efficiency with long-context capability</p><p>‚ùå Slightly more complex to implement</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>