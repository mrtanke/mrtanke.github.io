<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/on_the_representational_capacity_of_neural_languag/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/on_the_representational_capacity_of_neural_languag/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/on_the_representational_capacity_of_neural_languag/"><meta property="og:site_name" content="Home"><meta property="og:title" content="On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning"><meta property="og:description" content="Paper-reading notes: On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-12-01T08:49:03+00:00"><meta property="article:modified_time" content="2025-12-01T08:49:03+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/selfile.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/selfile.png"><meta name=twitter:title content="On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning"><meta name=twitter:description content="Paper-reading notes: On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning","item":"https://my-blog-alpha-vert.vercel.app/notes/on_the_representational_capacity_of_neural_languag/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning","name":"On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning","description":"Paper-reading notes: On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning","keywords":[],"articleBody":"1. Motivation Traditional theoretical results on transformer “Turing-completeness” are not actually about language models. They:\nstudy language recognition, not probabilistic generation, and often add extra symbols to simulate a Turing machine. But a real LM = a probability distribution over strings of a fixed alphabet.\nThis paper provides a theory of expressivity for actual language models, especially when augmented with chain-of-thought (CoT) steps.\n2. Core Idea: Treat LMs as Probabilistic Models of Computation Instead of saying “this LM recognizes this language,” the paper asks: What distributions over strings can an LM represent? (probabilistic Turing-machine viewpoint)\nThis shift allows a proper comparison between:\nLMs without CoT LMs that generate extra intermediate steps (CoT) 3. Key Concept Introduced: Regular Reducibility CoT inserts extra reasoning tokens. To compare LMs with and without CoT, we need a way to ignore these internal steps in the output.\nRegular reducibility: LM (A) is reducible to LM (B) if:\nYou can convert samples from (A) into samples from (B) using only a finite-state transducer (very simple machine). Example: deleting or rewriting CoT reasoning steps.\n4. Main Results (1) CoT increases the power of RNN language models Constant-precision RNN LM (no CoT)\n→ equivalent to deterministic probabilistic finite-state automata (PFSAs)\n→ very weak\nConstant-precision RNN LM with CoT\n→ equivalent to non-deterministic PFSAs\n→ strictly more expressive\nSo CoT makes even weak RNN LMs more powerful.\n(2) Turing-complete RNNs can be viewed as doing implicit CoT Previous “Turing-complete RNN” constructions use complicated hidden-state motions.\nThe authors reinterpret these as implicit chain-of-thought reasoning inside the hidden state.\n(3) The Big Result: CoT makes LMs probabilistic-Turing-complete With enough numeric precision:\nLinear-precision RNN LMs + CoT, and Logarithmic-precision transformer decoder LMs + CoT, can simulate any probabilistic Turing machine.\nThus CoT raises LM generative capacity to the full power of probabilistic computation. This is the probabilistic analogue of Turing-completeness — but now for true LMs, not for recognizers.\n5. Implications CoT adds computational steps, increasing representational power.\nThis gives a theoretical explanation for the empirical success of CoT prompting.\nLM expressivity depends heavily on precision and intermediate computation.\nTransformers without CoT (limited depth, finite precision) behave like small probabilistic automata (very weak).\nTransformers with CoT behave like full probabilistic Turing machines (maximally expressive).\n6. Takeaway Chain-of-thought turns a limited LM into a model as expressive as a probabilistic Turing machine, by giving it extra internal computation steps that fundamentally increase its generative power.\n7. Turing machine A simple abstract computer with an infinite tape, a read-write head, and a finite set of states. It follows a table of rules that tell it how to read/write symbols and move on the tape.\nIt can express any algorithm, which makes it the foundation of computability theory and the “maximal” model of expressivity.\nVideo source: https://www.bilibili.com/video/BV1br4y1N762/?spm_id_from=333.337.search-card.all.click\u0026vd_source=650390d4a2decee4b694a632313a3cca\n","wordCount":"462","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/selfile.png","datePublished":"2025-12-01T08:49:03Z","dateModified":"2025-12-01T08:49:03Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/on_the_representational_capacity_of_neural_languag/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning</h1><div class=post-description>Paper-reading notes: On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning</div><div class=post-meta><span title='2025-12-01 08:49:03 +0000 +0000'>December 1, 2025</span>&nbsp;·&nbsp;<span>462 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-motivation aria-label="1. Motivation">1. Motivation</a></li><li><a href=#2-core-idea-treat-lms-as-probabilistic-models-of-computation aria-label="2. Core Idea: Treat LMs as Probabilistic Models of Computation">2. Core Idea: Treat LMs as Probabilistic Models of Computation</a></li><li><a href=#3-key-concept-introduced-regular-reducibility aria-label="3. Key Concept Introduced: Regular Reducibility">3. Key Concept Introduced: Regular Reducibility</a><ul><li><a href=#regular-reducibility aria-label="Regular reducibility:">Regular reducibility:</a></li></ul></li><li><a href=#4-main-results aria-label="4. Main Results">4. Main Results</a><ul><li><a href=#1-cot-increases-the-power-of-rnn-language-models aria-label="(1) CoT increases the power of RNN language models">(1) CoT increases the power of RNN language models</a></li><li><a href=#2-turing-complete-rnns-can-be-viewed-as-doing-implicit-cot aria-label="(2) Turing-complete RNNs can be viewed as doing implicit CoT">(2) Turing-complete RNNs can be viewed as doing implicit CoT</a></li><li><a href=#3-the-big-result-cot-makes-lms-probabilistic-turing-complete aria-label="(3) The Big Result: CoT makes LMs probabilistic-Turing-complete">(3) The Big Result: CoT makes LMs probabilistic-Turing-complete</a></li></ul></li><li><a href=#5-implications aria-label="5. Implications">5. Implications</a></li><li><a href=#6-takeaway aria-label="6. Takeaway">6. Takeaway</a></li><li><a href=#7-turing-machine aria-label="7. Turing machine">7. Turing machine</a></li></ul></div></details></div><div class=post-content><h2 id=1-motivation><strong>1. Motivation</strong><a hidden class=anchor aria-hidden=true href=#1-motivation>#</a></h2><p>Traditional theoretical results on transformer “Turing-completeness” are not actually about <strong>language models</strong>. They:</p><ul><li>study <strong>language recognition</strong>, not <strong>probabilistic generation</strong>,</li><li>and often add <strong>extra symbols</strong> to simulate a Turing machine.</li></ul><p>But a real LM = a probability distribution over strings of a fixed alphabet.</p><p>This paper provides a theory of expressivity <strong>for actual language models</strong>, especially when augmented with <strong>chain-of-thought (CoT)</strong> steps.</p><h2 id=2-core-idea-treat-lms-as-probabilistic-models-of-computation><strong>2. Core Idea: Treat LMs as Probabilistic Models of Computation</strong><a hidden class=anchor aria-hidden=true href=#2-core-idea-treat-lms-as-probabilistic-models-of-computation>#</a></h2><p>Instead of saying “this LM recognizes this language,” the paper asks: What <strong>distributions</strong> over strings can an LM represent? (probabilistic Turing-machine viewpoint)</p><p>This shift allows a proper comparison between:</p><ul><li>LMs without CoT</li><li>LMs that generate extra intermediate steps (CoT)</li></ul><h2 id=3-key-concept-introduced-regular-reducibility><strong>3. Key Concept Introduced: Regular Reducibility</strong><a hidden class=anchor aria-hidden=true href=#3-key-concept-introduced-regular-reducibility>#</a></h2><p>CoT inserts extra reasoning tokens. To compare LMs with and without CoT, we need a way to ignore these internal steps in the output.</p><aside><h3 id=regular-reducibility><strong>Regular reducibility:</strong><a hidden class=anchor aria-hidden=true href=#regular-reducibility>#</a></h3><p>LM (A) is reducible to LM (B) if:</p><ul><li>You can convert samples from (A) into samples from (B)</li><li>using only a <strong>finite-state transducer</strong> (very simple machine).</li></ul><p>Example: deleting or rewriting CoT reasoning steps.</p></aside><h2 id=4-main-results><strong>4. Main Results</strong><a hidden class=anchor aria-hidden=true href=#4-main-results>#</a></h2><h3 id=1-cot-increases-the-power-of-rnn-language-models><strong>(1) CoT increases the power of RNN language models</strong><a hidden class=anchor aria-hidden=true href=#1-cot-increases-the-power-of-rnn-language-models>#</a></h3><ul><li><p><strong>Constant-precision RNN LM (no CoT)</strong></p><p>→ equivalent to <strong>deterministic probabilistic finite-state automata (PFSAs)</strong></p><p>→ very weak</p></li><li><p>Constant-precision RNN LM <strong>with CoT</strong></p><p>→ equivalent to <strong>non-deterministic PFSAs</strong></p><p>→ strictly more expressive</p></li></ul><p>So <strong>CoT makes even weak RNN LMs more powerful</strong>.</p><h3 id=2-turing-complete-rnns-can-be-viewed-as-doing-implicit-cot><strong>(2) Turing-complete RNNs can be viewed as doing implicit CoT</strong><a hidden class=anchor aria-hidden=true href=#2-turing-complete-rnns-can-be-viewed-as-doing-implicit-cot>#</a></h3><p>Previous “Turing-complete RNN” constructions use complicated hidden-state motions.</p><p>The authors reinterpret these as <strong>implicit chain-of-thought reasoning</strong> inside the hidden state.</p><h3 id=3-the-big-result-cot-makes-lms-probabilistic-turing-complete><strong>(3) The Big Result: CoT makes LMs probabilistic-Turing-complete</strong><a hidden class=anchor aria-hidden=true href=#3-the-big-result-cot-makes-lms-probabilistic-turing-complete>#</a></h3><p>With enough numeric precision:</p><ul><li><strong>Linear-precision RNN LMs + CoT</strong>, and</li><li><strong>Logarithmic-precision transformer decoder LMs + CoT</strong>,</li></ul><p>can simulate <strong>any probabilistic Turing machine</strong>.</p><p>Thus CoT raises LM generative capacity to the full power of probabilistic computation. This is the probabilistic analogue of Turing-completeness — but now for <strong>true LMs</strong>, not for recognizers.</p><h2 id=5-implications><strong>5. Implications</strong><a hidden class=anchor aria-hidden=true href=#5-implications>#</a></h2><ol><li><p><strong>CoT adds computational steps</strong>, increasing representational power.</p><p>This gives a theoretical explanation for the empirical success of CoT prompting.</p></li><li><p><strong>LM expressivity depends heavily on precision and intermediate computation.</strong></p></li><li><p><strong>Transformers without CoT (limited depth, finite precision)</strong> behave like small probabilistic automata (very weak).</p></li><li><p><strong>Transformers with CoT</strong> behave like full probabilistic Turing machines (maximally expressive).</p></li></ol><h2 id=6-takeaway><strong>6. Takeaway</strong><a hidden class=anchor aria-hidden=true href=#6-takeaway>#</a></h2><p><strong>Chain-of-thought</strong> turns a limited LM into a model as expressive as a probabilistic Turing machine, by giving it extra internal computation steps that fundamentally increase its generative power.</p><h2 id=7-turing-machine>7. Turing machine<a hidden class=anchor aria-hidden=true href=#7-turing-machine>#</a></h2><aside><p>A simple abstract computer with an infinite tape, a read-write head, and a finite set of states. It follows a table of rules that tell it how to read/write symbols and move on the tape.</p><p>It can express <strong>any algorithm</strong>, which makes it the foundation of computability theory and the “maximal” model of expressivity.</p><p>Video source: <a href="https://www.bilibili.com/video/BV1br4y1N762/?spm_id_from=333.337.search-card.all.click&amp;vd_source=650390d4a2decee4b694a632313a3cca">https://www.bilibili.com/video/BV1br4y1N762/?spm_id_from=333.337.search-card.all.click&amp;vd_source=650390d4a2decee4b694a632313a3cca</a></p></aside></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>