<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"><meta property="og:description" content="Paper-reading notes: Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-11-09T15:01:10+00:00"><meta property="article:modified_time" content="2025-11-09T15:01:10+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/image_1.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/image_2.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/image_3.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/image_4.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/image.png"><meta name=twitter:title content="Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"><meta name=twitter:description content="Paper-reading notes: Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation","item":"https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation","name":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation","description":"Paper-reading notes: Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation","keywords":[],"articleBody":"Introduction Large Transformer models achieve strong generalization and reasoning ability, but their huge size requires massive memory and computation. This makes them hard to train and deploy outside large data centers. To address this, researchers look for efficient architectures that save parameters or computation. Two main directions are parameter efficiency, which reduces or shares model weights, and adaptive computation, which uses more compute only when necessary.\nA common way to improve parameter efficiency is layer tying, where the same set of weights is reused across multiple layers. For adaptive computation, methods like early exiting let the model stop processing simple tokens earlier. However, most existing models treat these two goals separately. A unified design that combines both efficiency types has been missing. Recursive Transformers reuse one set of layers multiple times, naturally achieving weight sharing. Yet, previous recursive models often fix the number of recursions for all tokens. This wastes computation because every token receives the same processing depth, regardless of difficulty. Attempts at dynamic recursion have also faced training and efficiency challenges.\nTo solve these issues, the Mixture-of-Recursions (MoR) framework was proposed. It introduces a router that learns how many recursive steps each token needs. Easy tokens stop early, while complex ones go through more recursions. This provides true token-level adaptive depth. MoR also includes key–value (KV) caching, storing results from previous recursions to reduce memory use and improve throughput.\nOne Transformer layer = Self-Attention + Feed-Forward + Normalization + Residual connections.\nConceptually, MoR provides a model to “think” recursively inside its latent space during decoding of each token. It adjusts its reasoning depth per token instead of using a fixed number of layers. In this way, MoR unifies parameter efficiency and adaptive computation in one efficient Transformer architecture.\nThrough this design, MoR achieves three goals at once:\nWeight sharing reduces model parameters. Dynamic routing saves computation by skipping redundant steps. KV caching lowers memory traffic and speeds up inference. Methods Mixture-of-Recursions (MoR)—a framework that dynamically adjusts recursion step for each token during pretraining and inference.\nThe core of MoR lies in two components:\na routing mechanism that assigns token-specific recursion steps to adaptively concentrate computation on more challenging tokens; a KV caching strategy that defines how KV pairs are stored and selectively utilized for attention at each recursive step. 1. Routing Stategies: Expert-choice vs. Token-choice Expert-choice routing each recursion —select→ top-k tokens\nIn expert-choice routing, each recursion depth is treated as an expert. At every recursion step $r$, only the top-k tokens (with the highest scores) are selected to pass through that expert’s recursion block.\nThe router computes a scalar routing score for each token:\n$$ g_t^r = G(\\theta_r^\\top H_t^r) $$\nwhere $\\mathcal{G}$ is an activation function (e.g., sigmoid or tanh).\nThen, tokens with scores above the $\\beta$-percentile threshold $P_\\beta(G^r)$ are selected to proceed:\n$$ H_t^{r+1} = \\begin{cases} g_t^r f(H_t^r, \\Phi’) + Ht^r, \u0026 \\text{if } g_t^r \u003e P_\\beta(G^r) \\\\ H_t^r, \u0026 otherwise \\end{cases} $$\n$r$: recursion steps $t$: token This selective routing makes each recursion act like a different “expert,” and tokens move deeper only when needed. A mechanism called hierarchical filtering ensures that tokens selected at one step can still be re-evaluated at later steps, supporting early-exit-like behavior while training from scratch.\nToken-choice routing each token —select→ recursion depth\nIn token-choice routing, each token independently chooses which recursion expert it wants to follow for all subsequent steps. Unlike expert-choice, routing happens once per token, not at every recursion step.\nGiven the hidden state $\\mathcal{H}_t^1$ at the first layer, the router computes routing scores for all experts $N$:\n$$ g_t^j = \\mathcal{G}(\\theta_r^\\top \\mathcal{H}_t^1), \\quad j \\in {1, \\dots, N_r} $$\nEach token selects its expert $i$ with the highest score ($i$ = the recursion depth of the token go through):\n$$ i = \\arg\\max_j g_t^j $$\nand applies that recursion block $i$ times.\nThe hidden state is updated recursively as:\n$$ H_t^{r+1} = \\begin{cases} g_t^r f(H_t^r, \\Phi’) + H_t^1, \u0026 \\text{if } r = i, last~recursion \\\\ g_t^r f(H_t^r, \\Phi’), \u0026 otherwise \\end{cases} $$\nThis approach avoids information leakage and makes each token commit to a fixed recursion path, though it may cause load imbalance among experts.\nFor Expert-choice:\nHierarchical filtering keeps recursion steps causally ordered in data flow — each step selects tokens only from the previous one.\nHowever, during parallelized training, all recursion steps are computed at once, so gradients from deeper steps can flow backward and influence earlier routers.\nThis introduces training-time causality violation — even though the hierarchical filtering itself maintains structural causality.\n2. KV Caching Strategies: Recursion-wise Caching vs. Recursive sharing Dynamic-depth models face challenges with KV cache consistency during autoregressive decoding. When a token exits early, its keys and values from deeper recursion steps are missing, which causes incomplete context for later tokens. Previous methods tried to reuse or recompute these entries but introduced extra complexity. To address this, MoR proposes two efficient strategies: recursion-wise KV caching and recursive KV sharing.\nIn recursion-wise KV caching, only the tokens selected for a specific recursion step store their key–value pairs at that level. The cache size at each depth depends on how many tokens are routed there. Attention computation is restricted to these locally cached tokens. This makes computation more localized, saving memory and reducing input/output operations.\nIn recursive KV sharing, all tokens share the KV pairs produced at the first recursion block. These cached pairs are reused by all later recursion steps, so each step still has access to the full sequence context. Even though fewer tokens may continue deeper, their keys and values still represent the whole sequence, preventing missing-context problems.\nRecursion-wise caching reduces KV memory and IO usage roughly by a factor of $(N_r+1)/(2N_r)$ across the model and decreases attention FLOPs, making both training and inference more efficient. Recursive sharing saves even more memory by globally reusing context, though it provides less FLOP reduction and still faces IO bottlenecks during decoding. Overall, both strategies improve efficiency but trade off between local memory savings (recursion-wise) and global context reuse (recursive sharing).\nMore Details Parameter-sharing strategies in Recursive Transformers.\nSo an equation such as\n$$ f(h_t^{\\ell}; \\Phi’_{\\ell \\bmod (L/N_r)}) $$\nmeans:\ntake the token representation $h_t^{\\ell}$, apply a layer function $f(\\cdot)$ using parameters $\\Phi’$, and reuse weights cyclically according to the current recursion index $\\ell$. Results MoR outperforms baselines with fewer parameters under equal train compute. MoR outperforms baselines with less compute at equal data. MoR performance varies with routing and caching strategies. Conclusion Mixture-of-Recursions (MoR) is a Transformer model that combines parameter sharing, adaptive recursion depth, and efficient KV caching. It uses routers to decide how many recursive steps each token needs and stores KV pairs only for active tokens. This design cuts unnecessary computation and memory use while keeping strong performance. Experiments show that MoR achieves lower perplexity, higher few-shot accuracy, and faster inference than standard Transformers or earlier recursive models.\nFuture Work Future work will focus on improving reasoning ability. Since MoR already adapts depth per token, it can be trained to adjust recursion depth based on reasoning difficulty, helping the model handle chain-of-thought tasks better.\nMoR can also be scaled to larger models. Future versions will train models with over 3B parameters and may use depth-specific LoRA, expert modules, or expert parallelism to improve performance without slowing inference. Reusing pre-trained LLMs could further reduce training cost.\nAnother goal is to make MoR more flexible during inference. The current router outputs are too sharp, making it hard to change capacity or top-k values after training. New routing methods are needed for dynamic control.\nMoR can also benefit from sparsity methods like pruning and quantization to skip unnecessary computation, improving efficiency further.\nFinally, MoR’s adaptive recursion is not limited to text. It can be extended to vision, speech, and multimodal models. Adjusting depth for different tokens or segments could make processing long videos or audio more efficient in both memory and speed.\n","wordCount":"1312","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/image.png","datePublished":"2025-11-09T15:01:10Z","dateModified":"2025-11-09T15:01:10Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/mixture-of-recursions_learning_dynamic_recursive_d/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</h1><div class=post-description>Paper-reading notes: Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</div><div class=post-meta><span title='2025-11-09 15:01:10 +0000 +0000'>November 9, 2025</span>&nbsp;·&nbsp;<span>1312 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#methods aria-label=Methods>Methods</a><ul><li><a href=#1-routing-stategies-expert-choice-vs-token-choice aria-label="1. Routing Stategies: Expert-choice vs. Token-choice">1. Routing Stategies: Expert-choice vs. Token-choice</a><ul><li><a href=#expert-choice-routing aria-label="Expert-choice routing">Expert-choice routing</a></li><li><a href=#token-choice-routing aria-label="Token-choice routing">Token-choice routing</a></li></ul></li><li><a href=#2-kv-caching-strategies-recursion-wise-caching-vs-recursive-sharing aria-label="2. KV Caching Strategies: Recursion-wise Caching vs. Recursive sharing">2. KV Caching Strategies: Recursion-wise Caching vs. Recursive sharing</a></li></ul></li><li><a href=#more-details aria-label="More Details">More Details</a></li><li><a href=#results aria-label=Results>Results</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a><ul><li><a href=#future-work aria-label="Future Work">Future Work</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Large Transformer models achieve strong generalization and reasoning ability, but their huge size requires massive memory and computation. This makes them hard to train and deploy outside large data centers. To address this, researchers look for <strong>efficient architectures</strong> that save parameters or computation. Two main directions are <strong>parameter efficiency</strong>, which reduces or shares model weights, and <strong>adaptive computation</strong>, which uses more compute only when necessary.</p><p>A common way to improve parameter efficiency is <strong>layer tying</strong>, where the same set of weights is reused across multiple layers. For adaptive computation, methods like <strong>early exiting</strong> let the model stop processing simple tokens earlier. However, most existing models treat these two goals separately. A unified design that combines both efficiency types has been missing. <strong>Recursive Transformers</strong> reuse <strong>one set of layers</strong> multiple times, naturally achieving weight sharing. Yet, previous recursive models often fix the number of recursions for all tokens. This wastes computation because every token receives the same processing depth, regardless of difficulty. Attempts at dynamic recursion have also faced training and efficiency challenges.</p><p>To solve these issues, the <strong>Mixture-of-Recursions (MoR)</strong> framework was proposed. It introduces a <strong>router</strong> that learns how many recursive steps each token needs. Easy tokens stop early, while complex ones go through more recursions. This provides true <strong>token-level adaptive depth</strong>. MoR also includes <strong>key–value (KV) caching</strong>, storing results from previous recursions to reduce memory use and improve throughput.</p><p><img alt=image.png loading=lazy src=/notes/mixture-of-recursions_learning_dynamic_recursive_d/image.png></p><aside><p>One Transformer layer = <strong>Self-Attention + Feed-Forward + Normalization + Residual connections</strong>.</p></aside><p>Conceptually, MoR provides a model to “think” recursively inside its latent space during decoding of each token. It adjusts its reasoning depth per token instead of using a fixed number of layers. In this way, MoR unifies <strong>parameter efficiency</strong> and <strong>adaptive computation</strong> in one efficient Transformer architecture.</p><p>Through this design, MoR achieves three goals at once:</p><ol><li><strong>Weight sharing</strong> reduces model parameters.</li><li><strong>Dynamic routing</strong> saves computation by skipping redundant steps.</li><li><strong>KV caching</strong> lowers memory traffic and speeds up inference.</li></ol><h1 id=methods>Methods<a hidden class=anchor aria-hidden=true href=#methods>#</a></h1><p><strong>Mixture-of-Recursions (MoR)</strong>—a framework that dynamically adjusts recursion step for each token during pretraining and inference.</p><p>The core of MoR lies in two components:</p><ol><li><strong>a routing mechanism</strong> that assigns token-specific recursion steps to adaptively concentrate computation on more challenging tokens;</li><li><strong>a KV caching strategy</strong> that defines how KV pairs are stored and selectively utilized for attention at each recursive step.</li></ol><h2 id=1-routing-stategies-expert-choice-vs-token-choice>1. Routing Stategies: Expert-choice vs. Token-choice<a hidden class=anchor aria-hidden=true href=#1-routing-stategies-expert-choice-vs-token-choice>#</a></h2><p><img alt=image.png loading=lazy src=/notes/mixture-of-recursions_learning_dynamic_recursive_d/image_1.png></p><h3 id=expert-choice-routing><strong>Expert-choice routing</strong><a hidden class=anchor aria-hidden=true href=#expert-choice-routing>#</a></h3><aside><p>each recursion —select→ top-k tokens</p></aside><p>In <strong>expert-choice routing</strong>, each recursion depth is treated as an <strong>expert</strong>. At every recursion step $r$, only the top-k tokens (with the highest scores) are selected to pass through that expert’s recursion block.</p><p>The router computes a scalar <strong>routing score</strong> for each token:</p><p>$$
g_t^r = G(\theta_r^\top H_t^r)
$$</p><p>where $\mathcal{G}$ is an activation function (e.g., sigmoid or tanh).</p><p>Then, tokens with scores above the $\beta$-percentile threshold $P_\beta(G^r)$ are selected to proceed:</p><p>$$
H_t^{r+1} =
\begin{cases}
g_t^r f(H_t^r, \Phi&rsquo;) + Ht^r, & \text{if } g_t^r > P_\beta(G^r) \\
H_t^r, & otherwise
\end{cases}
$$</p><ul><li>$r$: recursion steps</li><li>$t$: token</li></ul><p>This selective routing makes each recursion act like a different “expert,” and tokens move deeper only when needed. A mechanism called <strong>hierarchical filtering</strong> ensures that tokens selected at one step can still be re-evaluated at later steps, supporting early-exit-like behavior while training from scratch.</p><h3 id=token-choice-routing><strong>Token-choice routing</strong><a hidden class=anchor aria-hidden=true href=#token-choice-routing>#</a></h3><aside><p>each token —select→ recursion depth</p></aside><p>In <strong>token-choice routing</strong>, each token independently chooses <strong>which recursion expert</strong> it wants to follow for all subsequent steps. Unlike expert-choice, routing happens <strong>once per token</strong>, not at every recursion step.</p><p>Given the hidden state $\mathcal{H}_t^1$ at the first layer, the router computes <strong>routing scores</strong> for all experts $N$:</p><p>$$
g_t^j = \mathcal{G}(\theta_r^\top \mathcal{H}_t^1), \quad j \in {1, \dots, N_r}
$$</p><p>Each token selects its <strong>expert $i$</strong> with the highest score ($i$ = the recursion depth of the token go through):</p><p>$$
i = \arg\max_j g_t^j
$$</p><p>and applies that recursion block $i$ times.</p><p>The hidden state is updated recursively as:</p><p>$$
H_t^{r+1} =
\begin{cases}
g_t^r f(H_t^r, \Phi&rsquo;) + H_t^1, & \text{if } r = i, last~recursion \\
g_t^r f(H_t^r, \Phi&rsquo;), & otherwise
\end{cases}
$$</p><p>This approach avoids information leakage and makes each token commit to a fixed recursion path, though it may cause <strong>load imbalance</strong> among experts.</p><p><img alt=image.png loading=lazy src=/notes/mixture-of-recursions_learning_dynamic_recursive_d/image_2.png></p><aside><p><strong>For Expert-choice:</strong></p><p><strong>Hierarchical filtering</strong> keeps recursion steps causally ordered in data flow — each step selects tokens only from the previous one.</p><p>However, during parallelized training, all recursion steps are computed at once, so gradients from deeper steps can flow backward and influence earlier routers.</p><p>This introduces <strong>training-time causality violation</strong> — even though the hierarchical filtering itself maintains structural causality.</p></aside><h2 id=2-kv-caching-strategies-recursion-wise-caching-vs-recursive-sharing>2. KV Caching Strategies: Recursion-wise Caching vs. Recursive sharing<a hidden class=anchor aria-hidden=true href=#2-kv-caching-strategies-recursion-wise-caching-vs-recursive-sharing>#</a></h2><p><img alt=image.png loading=lazy src=/notes/mixture-of-recursions_learning_dynamic_recursive_d/image_3.png></p><p>Dynamic-depth models face challenges with <strong>KV cache consistency</strong> during autoregressive decoding. When a token exits early, its keys and values from deeper recursion steps are missing, which causes incomplete context for later tokens. Previous methods tried to reuse or recompute these entries but introduced extra complexity. To address this, MoR proposes two efficient strategies: <strong>recursion-wise KV caching</strong> and <strong>recursive KV sharing</strong>.</p><p>In <strong>recursion-wise KV caching</strong>, only the tokens selected for a specific recursion step store their key–value pairs at that level. The cache size at each depth depends on how many tokens are routed there. Attention computation is restricted to these locally cached tokens. This makes computation more localized, saving memory and reducing input/output operations.</p><p>In <strong>recursive KV sharing</strong>, all tokens share the KV pairs produced at the first recursion block. These cached pairs are reused by all later recursion steps, so each step still has access to the full sequence context. Even though fewer tokens may continue deeper, their keys and values still represent the whole sequence, preventing missing-context problems.</p><p><strong>Recursion-wise caching</strong> reduces KV memory and IO usage roughly by a factor of $(N_r+1)/(2N_r)$ across the model and decreases attention FLOPs, making both training and inference more efficient. <strong>Recursive sharing</strong> saves even more memory by globally reusing context, though it provides less FLOP reduction and still faces IO bottlenecks during decoding. Overall, both strategies improve efficiency but trade off between <strong>local memory savings</strong> (recursion-wise) and <strong>global context reuse</strong> (recursive sharing).</p><h1 id=more-details>More Details<a hidden class=anchor aria-hidden=true href=#more-details>#</a></h1><p>Parameter-sharing strategies in Recursive Transformers.</p><p><img alt=image.png loading=lazy src=/notes/mixture-of-recursions_learning_dynamic_recursive_d/image_4.png></p><p>So an equation such as</p><p>$$
f(h_t^{\ell}; \Phi&rsquo;_{\ell \bmod (L/N_r)})
$$</p><p>means:</p><ul><li>take the token representation $h_t^{\ell}$,</li><li>apply a layer function $f(\cdot)$ using parameters $\Phi&rsquo;$,</li><li>and reuse weights cyclically according to the current recursion index $\ell$.</li></ul><h1 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h1><ul><li>MoR outperforms baselines with fewer parameters under equal train compute.</li><li>MoR outperforms baselines with less compute at equal data.</li><li>MoR performance varies with routing and caching strategies.</li></ul><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p><strong>Mixture-of-Recursions (MoR)</strong> is a Transformer model that combines <strong>parameter sharing</strong>, <strong>adaptive recursion depth</strong>, and <strong>efficient KV caching</strong>. It uses routers to decide how many recursive steps each token needs and stores KV pairs only for active tokens. This design cuts unnecessary computation and memory use while keeping strong performance. Experiments show that MoR achieves lower perplexity, higher few-shot accuracy, and faster inference than standard Transformers or earlier recursive models.</p><h2 id=future-work><strong>Future Work</strong><a hidden class=anchor aria-hidden=true href=#future-work>#</a></h2><p>Future work will focus on improving reasoning ability. Since MoR already adapts depth per token, it can be trained to adjust recursion depth based on reasoning difficulty, helping the model handle chain-of-thought tasks better.</p><p>MoR can also be scaled to larger models. Future versions will train models with over 3B parameters and may use depth-specific LoRA, expert modules, or expert parallelism to improve performance without slowing inference. Reusing pre-trained LLMs could further reduce training cost.</p><p>Another goal is to make MoR more flexible during inference. The current router outputs are too sharp, making it hard to change capacity or top-k values after training. New routing methods are needed for dynamic control.</p><p>MoR can also benefit from sparsity methods like pruning and quantization to skip unnecessary computation, improving efficiency further.</p><p>Finally, MoR’s adaptive recursion is not limited to text. It can be extended to vision, speech, and multimodal models. Adjusting depth for different tokens or segments could make processing long videos or audio more efficient in both memory and speed.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>