<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>OpenVLA: An Open-Source Vision-Language-Action Model | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: OpenVLA"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/openvla_an_open-source_vision-language-action_mode/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/openvla_an_open-source_vision-language-action_mode/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/openvla_an_open-source_vision-language-action_mode/"><meta property="og:site_name" content="Home"><meta property="og:title" content="OpenVLA: An Open-Source Vision-Language-Action Model"><meta property="og:description" content="Paper-reading notes: OpenVLA"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-12-12T08:37:15+00:00"><meta property="article:modified_time" content="2025-12-12T08:37:15+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/openvla_an_open-source_vision-language-action_mode/image.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/openvla_an_open-source_vision-language-action_mode/image.png"><meta name=twitter:title content="OpenVLA: An Open-Source Vision-Language-Action Model"><meta name=twitter:description content="Paper-reading notes: OpenVLA"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"OpenVLA: An Open-Source Vision-Language-Action Model","item":"https://my-blog-alpha-vert.vercel.app/notes/openvla_an_open-source_vision-language-action_mode/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"OpenVLA: An Open-Source Vision-Language-Action Model","name":"OpenVLA: An Open-Source Vision-Language-Action Model","description":"Paper-reading notes: OpenVLA","keywords":[],"articleBody":"Github: https://github.com/openvla/openvla\n1. Keywords End-to-end VLA polic Fully open-source (data, code, weights) 2. Problem OpenVLA is proposed to overcome three major limitations in existing robotic learning systems:\nthe lack of end-to-end VLA modeling, insufficiently large and diverse robot datasets, the reliance on closed-source models and data. 3. Method 3.1. Overall goal OpenVLA is a vision–language–action (VLA) foundation model that learns an end-to-end robot control policy, mapping visual observations and natural-language instructions directly to robot actions in a closed loop.\n3.2. Model architecture OpenVLA uses a VLM backbone extended for action prediction:\nVisual encoder Fuses pretrained features from DINOv2 (geometry \u0026 structure) SigLIP (vision–language alignment) Language model backbone Llama-2 (7B) Receives visual tokens + language instruction tokens Outputs action tokens instead of text This forms a single transformer handling vision, language, and action.\n3.3. Action representation (key design choice) Robot actions are continuous. They are discretized per dimension into 256 bins. Bin ranges are defined using the 1st–99th percentiles of training data → ignores outliers, preserves precision.\nEach bin is mapped to a token in the LLM vocabulary, which is implemented by overwriting rarely used tokenizer tokens\nResult:\ncontinuous action → discrete action tokens → LLM prediction → decoded back to discrete action value.\n3.4. Training 3.4.1. Training Objective Trained using standard next-token prediction Loss: cross-entropy Evaluated only on action tokens No custom control head or regression loss This allows reuse of pretrained LLM weights without architectural changes.\n3.4.2. Training data Trained on ~970k robot episodes Includes: diverse tasks multiple robots multiple environments Episodes are full trajectories (closed-loop interaction) 3.4.3. Inference / deployment At each timestep:\nObserve camera images Read language instruction Predict next action tokens Decode to continuous control Execute on robot Repeat (closed-loop control) One-sentence takeaway OpenVLA is an open, end-to-end vision–language–action transformer that discretizes robot actions into tokens, enabling a pretrained LLM to directly perform closed-loop robot control at scale.\n","wordCount":"312","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/openvla_an_open-source_vision-language-action_mode/image.png","datePublished":"2025-12-12T08:37:15Z","dateModified":"2025-12-12T08:37:15Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/openvla_an_open-source_vision-language-action_mode/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">OpenVLA: An Open-Source Vision-Language-Action Model</h1><div class=post-description>Paper-reading notes: OpenVLA</div><div class=post-meta><span title='2025-12-12 08:37:15 +0000 +0000'>December 12, 2025</span>&nbsp;·&nbsp;<span>312 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-keywords aria-label="1. Keywords">1. Keywords</a></li><li><a href=#2-problem aria-label="2. Problem">2. Problem</a></li><li><a href=#3-method aria-label="3. Method">3. Method</a><ul><li><a href=#31-overall-goal aria-label="3.1. Overall goal">3.1. Overall goal</a></li><li><a href=#32-model-architecture aria-label="3.2. Model architecture">3.2. Model architecture</a></li><li><a href=#33-action-representation-key-design-choice aria-label="3.3. Action representation (key design choice)">3.3. Action representation (key design choice)</a></li><li><a href=#34-training aria-label="3.4. Training">3.4. Training</a><ul><li><a href=#341-training-objective aria-label="3.4.1. Training Objective">3.4.1. Training Objective</a></li><li><a href=#342-training-data aria-label="3.4.2. Training data">3.4.2. Training data</a></li><li><a href=#343-inference--deployment aria-label="3.4.3. Inference / deployment">3.4.3. Inference / deployment</a></li></ul></li><li><a href=#one-sentence-takeaway aria-label="One-sentence takeaway">One-sentence takeaway</a></li></ul></li></ul></div></details></div><div class=post-content><p>Github: <a href=https://github.com/openvla/openvla>https://github.com/openvla/openvla</a></p><h1 id=1-keywords>1. Keywords<a hidden class=anchor aria-hidden=true href=#1-keywords>#</a></h1><ul><li>End-to-end VLA polic</li><li>Fully open-source (data, code, weights)</li></ul><h1 id=2-problem>2. Problem<a hidden class=anchor aria-hidden=true href=#2-problem>#</a></h1><p>OpenVLA is proposed to overcome three major limitations in existing robotic learning systems:</p><ul><li>the lack of end-to-end VLA modeling,</li><li>insufficiently large and diverse robot datasets,</li><li>the reliance on closed-source models and data.</li></ul><h1 id=3-method>3. Method<a hidden class=anchor aria-hidden=true href=#3-method>#</a></h1><h2 id=31-overall-goal>3.1. Overall goal<a hidden class=anchor aria-hidden=true href=#31-overall-goal>#</a></h2><p>OpenVLA is a <strong>vision–language–action (VLA) foundation model</strong> that learns an <strong>end-to-end robot control policy</strong>, mapping visual observations and natural-language instructions <strong>directly to robot actions</strong> in a closed loop.</p><h2 id=32-model-architecture>3.2. Model architecture<a hidden class=anchor aria-hidden=true href=#32-model-architecture>#</a></h2><p><strong>OpenVLA uses a VLM backbone extended for action prediction:</strong></p><ul><li><strong>Visual encoder</strong><ul><li>Fuses pretrained features from<ul><li><strong>DINOv2</strong> (geometry & structure)</li><li><strong>SigLIP</strong> (vision–language alignment)</li></ul></li></ul></li><li><strong>Language model backbone</strong><ul><li><strong>Llama-2 (7B)</strong></li><li>Receives visual tokens + language instruction tokens</li><li>Outputs <strong>action tokens</strong> instead of text</li></ul></li></ul><p>This forms a <strong>single transformer</strong> handling vision, language, and action.</p><p><img alt=image.png loading=lazy src=/notes/openvla_an_open-source_vision-language-action_mode/image.png></p><h2 id=33-action-representation-key-design-choice>3.3. Action representation (key design choice)<a hidden class=anchor aria-hidden=true href=#33-action-representation-key-design-choice>#</a></h2><p>Robot actions are <strong>continuous.</strong> They are <strong>discretized per dimension</strong> into <strong>256 bins.</strong> Bin ranges are defined using the <strong>1st–99th percentiles</strong> of training data → ignores outliers, preserves precision.</p><p>Each bin is mapped to a <strong>token</strong> in the LLM vocabulary, which is implemented by <strong>overwriting rarely used tokenizer tokens</strong></p><p>Result:</p><aside><p>continuous action → discrete action tokens → LLM prediction → decoded back to discrete action value.</p></aside><h2 id=34-training>3.4. Training<a hidden class=anchor aria-hidden=true href=#34-training>#</a></h2><h3 id=341-training-objective>3.4.1. Training Objective<a hidden class=anchor aria-hidden=true href=#341-training-objective>#</a></h3><ul><li>Trained using <strong>standard next-token prediction</strong></li><li>Loss: <strong>cross-entropy</strong></li><li>Evaluated <strong>only on action tokens</strong></li><li>No custom control head or regression loss</li></ul><p>This allows reuse of <strong>pretrained LLM weights</strong> without architectural changes.</p><h3 id=342-training-data>3.4.2. Training data<a hidden class=anchor aria-hidden=true href=#342-training-data>#</a></h3><ul><li>Trained on <strong>~970k robot episodes</strong></li><li>Includes:<ul><li>diverse tasks</li><li>multiple robots</li><li>multiple environments</li></ul></li><li>Episodes are full trajectories (closed-loop interaction)</li></ul><h3 id=343-inference--deployment>3.4.3. Inference / deployment<a hidden class=anchor aria-hidden=true href=#343-inference--deployment>#</a></h3><p>At each timestep:</p><ol><li>Observe camera images</li><li>Read language instruction</li><li>Predict next action tokens</li><li>Decode to continuous control</li><li>Execute on robot</li><li>Repeat (closed-loop control)</li></ol><h2 id=one-sentence-takeaway>One-sentence takeaway<a hidden class=anchor aria-hidden=true href=#one-sentence-takeaway>#</a></h2><p>OpenVLA is an open, end-to-end vision–language–action transformer that discretizes robot actions into tokens, enabling a pretrained LLM to directly perform closed-loop robot control at scale.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>