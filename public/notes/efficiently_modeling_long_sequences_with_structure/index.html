<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Efficiently Modeling Long Sequences with Structured State Spaces | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: S4"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/efficiently_modeling_long_sequences_with_structure/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/efficiently_modeling_long_sequences_with_structure/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/efficiently_modeling_long_sequences_with_structure/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Efficiently Modeling Long Sequences with Structured State Spaces"><meta property="og:description" content="Paper-reading notes: S4"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-11-11T13:19:33+00:00"><meta property="article:modified_time" content="2025-11-11T13:19:33+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/efficiently_modeling_long_sequences_with_structure/image.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/efficiently_modeling_long_sequences_with_structure/image.png"><meta name=twitter:title content="Efficiently Modeling Long Sequences with Structured State Spaces"><meta name=twitter:description content="Paper-reading notes: S4"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Efficiently Modeling Long Sequences with Structured State Spaces","item":"https://my-blog-alpha-vert.vercel.app/notes/efficiently_modeling_long_sequences_with_structure/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Efficiently Modeling Long Sequences with Structured State Spaces","name":"Efficiently Modeling Long Sequences with Structured State Spaces","description":"Paper-reading notes: S4","keywords":[],"articleBody":"Introduction Most models (RNNs, CNNs, Transformers) cannot handle very long sequences well. They either forget, see too little context, or are too slow. This makes long-range dependency tasks hard to solve.\nState Space Models (SSMs) can solve this issue by remembering long information. But the old version (LSSL) used too much time and memory, so it was not practical.\nNew solution S4 fixes this by changing how the SSM is built. It splits the main matrix into simple parts and computes in a faster, more stable way. Now it runs much faster and uses much less memory.\nBackground: State Spaces State Space Models (SSMs) describe how input changes hidden states and produces output. They learn parameters A, B, C, D automatically. Usually, D is ignored because it’s just a shortcut.\n$$ x’(t) = A x(t) + B u(t) \\\\ y(t) = C x(t) + D u(t) $$\nThe basic SSM forgets or explodes on long sequences. HiPPO fixes this by giving a special matrix for A that helps the model remember long-term information.\nWe mainly focus on optimizing A because A is the bottleneck — it controls how the model remembers information over time. B and C are much simpler — they only handle input and output scaling.\n$$ A_{nk} = \\begin{cases} -(2n + 1)^{1/2}(2k + 1)^{1/2}, \u0026 \\text{if } n \u003e k \\\\ -(n + 1), \u0026 \\text{if } n = k \\\\ 0, \u0026 \\text{if } n \u003c k \\end{cases} $$\nReal data comes in steps, so the SSM is turned into a discrete version that works like an RNN. To train faster, the model is rewritten as a convolution, which processes all inputs in parallel. The convolution weights are called the SSM kernel (K). (More details move to More Details)\nMethod: Structured State Spaces (S4) S4 is a new way to make State Space Models (SSMs) fast and stable. The technical results focus on developing the S4 parameterization and showing how to efficiently compute all views of the SSM. It connects three forms of SSMs — continuous, recurrent, and convolutional — in one model.\nThe old method was slow because it multiplied a big matrix many times. S4 fixes this by rewriting the matrix as Normal + Low-Rank, which is easier to compute. This design makes S4 very efficient:\nRecurrent form: O(N) time per step Convolution form: O(N + L) total time Each S4 layer has trainable parts (Λ, P, Q, B, C) and works like a CNN or Transformer layer. Stacking layers with activations makes it powerful for long-sequence data.\n$$ A = V \\Lambda V^{} - P Q^{\\top} = V \\left( \\Lambda - (V^{}P)(V^{}Q)^{} \\right) V^{*} $$\nConclusion S4 is based on the State Space Model (SSM), which works like an RNN — it keeps a hidden state and updates it over time.\nThe problem with the old version is that the state update matrix A is hard to compute and unstable for long sequences. S4 solves this by rewriting A into a new, easier form:\n$$ A = V \\Lambda V^{*} - P Q^{\\top} $$\nThis form (called Normal + Low-Rank) makes A stable, efficient, and fast to compute. As a result, S4 keeps the good memory ability of RNNs but runs much faster and handles much longer sequences.\nMore Details Step 1: The original state space equations Continuous-time form:\n$$ x’(t) = A x(t) + B u(t) $$\n$$ y(t) = C x(t) $$\nDiscrete-time version (after discretization):\n$$ x_k = \\bar{A} x_{k-1} + \\bar{B} u_k $$\n$$ y_k = \\bar{C} x_k $$\nHere:\n$x_k$: memory or hidden state at step $k$ $u_k$: input at step $k$ $y_k$: output at step $k$ $\\bar{A}, \\bar{B}, \\bar{C}$: system parameters This is a recurrent process — to get $x_k$, we need $x_{k-1}$. So it must be done step-by-step, like an RNN → no parallelization.\nStep 2: Expand the recurrence We can “unroll” the recurrence to express $x_k$ directly in terms of all previous inputs:\n$$ x_k = \\bar{A}^k x_0 + \\sum_{i=0}^{k-1} \\bar{A}^i \\bar{B} u_{k-i} $$\nIf we ignore the initial state $(x_0 = 0)$, then:\n$$ x_k = \\sum_{i=0}^{k-1} \\bar{A}^i \\bar{B} u_{k-i} $$\nNow substitute this into $y_k = \\bar{C} x_k$:\n$$ y_k = \\bar{C} \\sum_{i=0}^{k-1} \\bar{A}^i \\bar{B} u_{k-i} $$\nRewriting it:\n$$ y_k = \\sum_{i=0}^{k-1} (\\bar{C}\\bar{A}^i\\bar{B}) u_{k-i} $$\nStep 3: Define the kernel Let’s define a kernel $\\bar{K}_i = \\bar{C}\\bar{A}^i\\bar{B}$.\nThen the output becomes:\n$$ y_k = \\sum_{i=0}^{k-1} \\bar{K_i} u_{k-i} $$\nThat’s exactly a 1D convolution:\n$$ y = \\bar{K} * u $$\nSo instead of computing step-by-step updates of $x_k$, we can compute all outputs at once using convolution.\nStep 4: Parallelization using FFT There is a mathematical truth from signal processing:\n→ A convolution in time is equal to a multiplication in frequency.\nA convolution in time domain can be computed efficiently in frequency domain (via FFT):\n$$ F(y) = F(\\bar{K} * u) = F(\\bar{K}) \\odot F(u) $$\n(where $\\odot$ means elementwise multiplication.)\nThen invert it back:\n$$ y = F^{-1}(F(\\bar{K}) \\odot F(u)) $$\nUsing FFT, this whole computation is O(L log L) instead of O(L²), and it can be done for all time steps in parallel — no need to wait for $x_{k-1}$.\nStep 5: Why S4 can do this efficiently In normal SSMs, computing each $\\bar{A}^i\\bar{B}$ is expensive → O(N²L). S4 makes it efficient by reparameterizing A as:\n$$ A = V \\Lambda V^{*} - P Q^{\\top} $$\nwhich allows fast and stable computation of $\\bar{K}$ using frequency-space math (Cauchy kernel + Woodbury identity). That’s how S4 converts a recurrent model into a parallelizable convolutional model while keeping the same meaning.\n","wordCount":"930","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/efficiently_modeling_long_sequences_with_structure/image.png","datePublished":"2025-11-11T13:19:33Z","dateModified":"2025-11-11T13:19:33Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/efficiently_modeling_long_sequences_with_structure/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Efficiently Modeling Long Sequences with Structured State Spaces</h1><div class=post-description>Paper-reading notes: S4</div><div class=post-meta><span title='2025-11-11 13:19:33 +0000 +0000'>November 11, 2025</span>&nbsp;·&nbsp;<span>930 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#background-state-spaces aria-label="Background: State Spaces">Background: State Spaces</a></li><li><a href=#method-structured-state-spaces-s4 aria-label="Method: Structured State Spaces (S4)">Method: Structured State Spaces (S4)</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#more-details aria-label="More Details">More Details</a><ul><li><a href=#step-1-the-original-state-space-equations aria-label="Step 1: The original state space equations">Step 1: The original state space equations</a></li><li><a href=#step-2-expand-the-recurrence aria-label="Step 2: Expand the recurrence">Step 2: Expand the recurrence</a></li><li><a href=#step-3-define-the-kernel aria-label="Step 3: Define the kernel">Step 3: Define the kernel</a></li><li><a href=#step-4-parallelization-using-fft aria-label="Step 4: Parallelization using FFT">Step 4: Parallelization using FFT</a></li><li><a href=#step-5-why-s4-can-do-this-efficiently aria-label="Step 5: Why S4 can do this efficiently">Step 5: Why S4 can do this efficiently</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Most models (RNNs, CNNs, Transformers) cannot handle <strong>very long sequences</strong> well. They either forget, see too little context, or are too slow. This makes long-range dependency tasks hard to solve.</p><p><strong>State Space Models (SSMs)</strong> can solve this issue by remembering long information. But the old version (LSSL) used too much <strong>time and memory</strong>, so it was not practical.</p><p><img alt=image.png loading=lazy src=/notes/efficiently_modeling_long_sequences_with_structure/image.png></p><p>New solution <strong>S4</strong> fixes this by changing how the SSM is built. It splits the main matrix into simple parts and computes in a faster, more stable way. Now it runs <strong>much faster</strong> and uses <strong>much less memory</strong>.</p><h1 id=background-state-spaces>Background: State Spaces<a hidden class=anchor aria-hidden=true href=#background-state-spaces>#</a></h1><p><strong>State Space Models (SSMs)</strong> describe how input changes hidden states and produces output. They learn parameters <strong>A, B, C, D</strong> automatically. Usually, <strong>D</strong> is ignored because it’s just a shortcut.</p><p>$$
x&rsquo;(t) = A x(t) + B u(t) \\
y(t) = C x(t) + D u(t)
$$</p><p>The <strong>basic SSM</strong> forgets or explodes on long sequences. <strong>HiPPO</strong> fixes this by giving a special matrix for A that helps the model remember long-term information.</p><aside><p>We mainly focus on <strong>optimizing A</strong> because A is the <strong>bottleneck</strong> — it controls how the model remembers information over time. B and C are much simpler — they only handle input and output scaling.</p></aside><p>$$
A_{nk} =
\begin{cases}
-(2n + 1)^{1/2}(2k + 1)^{1/2}, & \text{if } n > k \\
-(n + 1), & \text{if } n = k \\
0, & \text{if } n &lt; k
\end{cases}
$$</p><p>Real data comes in steps, so the SSM is turned into a <strong>discrete version</strong> that works like an RNN. To train faster, the model is rewritten as a <strong>convolution</strong>, which processes all inputs <strong>in parallel</strong>. The convolution weights are called the <strong>SSM kernel (K)</strong>. (More details move to <strong>More Details</strong>)</p><h1 id=method-structured-state-spaces-s4>Method: Structured State Spaces (S4)<a hidden class=anchor aria-hidden=true href=#method-structured-state-spaces-s4>#</a></h1><p><strong>S4</strong> is a new way to make State Space Models (SSMs) fast and stable. The technical results focus on developing the <strong>S4 parameterization</strong> and showing how to efficiently compute all views of the SSM. It connects three forms of SSMs — continuous, recurrent, and convolutional — in one model.</p><p>The old method was slow because it multiplied a big matrix many times. S4 fixes this by rewriting the matrix as <strong>Normal + Low-Rank</strong>, which is easier to compute. This design makes S4 very efficient:</p><ul><li><strong>Recurrent form:</strong> O(N) time per step</li><li><strong>Convolution form:</strong> O(N + L) total time</li></ul><p>Each S4 layer has trainable parts <strong>(Λ, P, Q, B, C)</strong> and works like a CNN or Transformer layer. Stacking layers with <strong>activations</strong> makes it powerful for long-sequence data.</p><p>$$
A = V \Lambda V^{} - P Q^{\top} = V \left( \Lambda - (V^{}P)(V^{}Q)^{} \right) V^{*}
$$</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>S4 is based on the <strong>State Space Model (SSM)</strong>, which works like an RNN — it keeps a hidden state and updates it over time.</p><p>The problem with the old version is that the <strong>state update matrix A</strong> is hard to compute and unstable for long sequences. S4 solves this by <strong>rewriting A</strong> into a new, easier form:</p><p>$$
A = V \Lambda V^{*} - P Q^{\top}
$$</p><p>This form (called <strong>Normal + Low-Rank</strong>) makes A stable, efficient, and fast to compute. As a result, S4 keeps the good memory ability of RNNs but runs much faster and handles much longer sequences.</p><hr><h1 id=more-details>More Details<a hidden class=anchor aria-hidden=true href=#more-details>#</a></h1><h2 id=step-1-the-original-state-space-equations>Step 1: The original state space equations<a hidden class=anchor aria-hidden=true href=#step-1-the-original-state-space-equations>#</a></h2><p>Continuous-time form:</p><p>$$
x&rsquo;(t) = A x(t) + B u(t)
$$</p><p>$$
y(t) = C x(t)
$$</p><p>Discrete-time version (after discretization):</p><p>$$
x_k = \bar{A} x_{k-1} + \bar{B} u_k
$$</p><p>$$
y_k = \bar{C} x_k
$$</p><p>Here:</p><ul><li>$x_k$: memory or hidden state at step $k$</li><li>$u_k$: input at step $k$</li><li>$y_k$: output at step $k$</li><li>$\bar{A}, \bar{B}, \bar{C}$: system parameters</li></ul><p>This is a <strong>recurrent process</strong> — to get $x_k$, we need $x_{k-1}$. So it must be done <strong>step-by-step</strong>, like an RNN → <strong>no parallelization</strong>.</p><h2 id=step-2-expand-the-recurrence>Step 2: Expand the recurrence<a hidden class=anchor aria-hidden=true href=#step-2-expand-the-recurrence>#</a></h2><p>We can “unroll” the recurrence to express $x_k$ directly in terms of <strong>all previous inputs</strong>:</p><p>$$
x_k = \bar{A}^k x_0 + \sum_{i=0}^{k-1} \bar{A}^i \bar{B} u_{k-i}
$$</p><p>If we ignore the initial state $(x_0 = 0)$, then:</p><p>$$
x_k = \sum_{i=0}^{k-1} \bar{A}^i \bar{B} u_{k-i}
$$</p><p>Now substitute this into $y_k = \bar{C} x_k$:</p><p>$$
y_k = \bar{C} \sum_{i=0}^{k-1} \bar{A}^i \bar{B} u_{k-i}
$$</p><p>Rewriting it:</p><p>$$
y_k = \sum_{i=0}^{k-1} (\bar{C}\bar{A}^i\bar{B}) u_{k-i}
$$</p><h2 id=step-3-define-the-kernel>Step 3: Define the <strong>kernel</strong><a hidden class=anchor aria-hidden=true href=#step-3-define-the-kernel>#</a></h2><p>Let’s define a <strong>kernel</strong> $\bar{K}_i = \bar{C}\bar{A}^i\bar{B}$.</p><p>Then the output becomes:</p><p>$$
y_k = \sum_{i=0}^{k-1} \bar{K_i} u_{k-i}
$$</p><p>That’s exactly a <strong>1D convolution</strong>:</p><p>$$
y = \bar{K} * u
$$</p><p>So instead of computing step-by-step updates of $x_k$, we can compute all outputs <strong>at once</strong> using convolution.</p><h2 id=step-4-parallelization-using-fft>Step 4: Parallelization using FFT<a hidden class=anchor aria-hidden=true href=#step-4-parallelization-using-fft>#</a></h2><aside><p>There is a mathematical truth from signal processing:</p><p>→ <strong>A convolution in time is equal to a multiplication in frequency.</strong></p></aside><p>A convolution in time domain can be computed efficiently in <strong>frequency domain</strong> (via FFT):</p><p>$$
F(y) = F(\bar{K} * u) = F(\bar{K}) \odot F(u)
$$</p><p>(where $\odot$ means elementwise multiplication.)</p><p>Then invert it back:</p><p>$$
y = F^{-1}(F(\bar{K}) \odot F(u))
$$</p><p>Using FFT, this whole computation is <strong>O(L log L)</strong> instead of <strong>O(L²)</strong>, and it can be done <strong>for all time steps in parallel</strong> — no need to wait for $x_{k-1}$.</p><h2 id=step-5-why-s4-can-do-this-efficiently>Step 5: Why S4 can do this efficiently<a hidden class=anchor aria-hidden=true href=#step-5-why-s4-can-do-this-efficiently>#</a></h2><p>In normal SSMs, computing each $\bar{A}^i\bar{B}$ is expensive → O(N²L). S4 makes it efficient by <strong>reparameterizing A</strong> as:</p><p>$$
A = V \Lambda V^{*} - P Q^{\top}
$$</p><p>which allows <strong>fast and stable</strong> computation of $\bar{K}$ using frequency-space math (Cauchy kernel + Woodbury identity). That’s how S4 converts a <strong>recurrent model</strong> into a <strong>parallelizable convolutional model</strong> while keeping the same meaning.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>