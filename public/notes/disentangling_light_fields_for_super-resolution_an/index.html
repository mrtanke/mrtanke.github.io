<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Disentangling Light Fields for Super-Resolution and Disparity Estimation | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: Distangling mechanism, DistgSSR, DistgASR, DistgDisp"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Disentangling Light Fields for Super-Resolution and Disparity Estimation"><meta property="og:description" content="Paper-reading notes: Distangling mechanism, DistgSSR, DistgASR, DistgDisp"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-11-19T07:42:14+00:00"><meta property="article:modified_time" content="2025-11-19T07:42:14+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/321a8e78-349b-448d-800b-0869ae55c8b8.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/4e6af2d6-df79-48fc-8cad-0393c96ca3bd.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/cb7281c0-a000-40a3-b37a-7dde14dd87fe.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/e9b3ae0b-2969-4cf9-a851-061dc03b7675.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/image_1.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/321a8e78-349b-448d-800b-0869ae55c8b8.png"><meta name=twitter:title content="Disentangling Light Fields for Super-Resolution and Disparity Estimation"><meta name=twitter:description content="Paper-reading notes: Distangling mechanism, DistgSSR, DistgASR, DistgDisp"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Disentangling Light Fields for Super-Resolution and Disparity Estimation","item":"https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Disentangling Light Fields for Super-Resolution and Disparity Estimation","name":"Disentangling Light Fields for Super-Resolution and Disparity Estimation","description":"Paper-reading notes: Distangling mechanism, DistgSSR, DistgASR, DistgDisp","keywords":[],"articleBody":"Introduction CNNs treat inputs as general images with no special structure. But LF is not a random stack of images.\nTo be more specific, CNN expects the disparity of all pixels are the same. But in light field, objects with different depth exist in the same images! So a standard CNN kernel cannot learn mixed spatial + angular + disparity correlations incorrectly. This is why 2D CNNs fail on LF directly. In short, A fixed convolution window cannot follow disparity.\nThere are many existing methods aiming to solve this problem.\nCurrent Questions The current CNN structures for LF image processing:\nNeighbor-view combination → Since only 2 or 4 adjacent views were used to SR a specific view, this method cannot achieve a high reconstruction quality due to the discard of rich angular information. Convolution on EPIs → since an EPI is a 2D slice of a 4D LF, performing conv on EPIs can only incorporate angular information from the same horizontal or vertical views and cannot incorporate the spatial context prior. Multi-stream structure → it discards the views outside the four angular directions, resulting in under-exploitation of the rich angular information in an LF. Spatial-angular alternate convolution → it processes spatial and angular features separately. The model never learns deep joint spatial-angular representations. The key difference between existing schemes and disentangling mechanism is, disentangling mechanism can fully use the information from all angular views and incorporate the LF structure prior.\nWith this mechanism, high-dimensional LF data can be disentangled into different low-dimensional subspaces. Consequently, the difficulty for learning deep CNNs is reduced and several LF image processing tasks can be benefited.\nTake the 4D LF and separate into:\nSpatial component (x,y) Angular component (u,v) Disparity component (slopes in EPI) LF structural priors = the known geometric and angular relationships that exist naturally in light-field images.\nNow the CNN can learn:\nDisparity changes → from angular subspace Texture \u0026 edges → from spatial subspace Parallax = correlation between spatial \u0026 angular subspaces LF 4D consistency = combining both subspaces Method LF Representation SAIs: many 2D views arranged by angle. 3D stack: SAIs stacked to show angular variation. EPI: a 2D slice that reveals disparity as straight lines. SAI array: full grid of all SAIs. MacPI: one 2D image where each spatial location stores angular samples. Compared to SAI and EPI, the authors adopt the MacPI representation because it evenly mixes the spatial and angular information in the light field. This makes it easier to extract and combine spatial and angular features using convolution operations.\nLF Feature Disentanglement The 4D light field contains spatial (H, W) and angular (U, V) information. To make learning easier, the model uses three feature extractors, each handling one 2D subspace.\nAngular feature extractor, Spatial feature extractor, Vertical EPI feature extractor, Horizontal EPI feature extractor. Spatial Feature Extractor SFE extracts spatial texture inside the same view. It uses a 3×3 dilated convolution to avoid mixing different views. Output size stays the same as the input.\nAngular Feature Extractor AFE extracts angular information inside each macro-pixel. It uses an A×A convolution with stride A, so each angular block becomes one spatial pixel. Output size becomes H × W.\nEPI Feature Extractors SFE and AFE miss the spatial–angular relationship. EPIs show this relationship as line patterns.\nEFE-H processes horizontal EPIs (V–W). EFE-V processes vertical EPIs (U–H). They learn disparity and spatial–angular correlation.\nHow it solve current questions This design keeps all angular views, instead of discarding neighbors or directions, so the network can fully use the complete angular information.\nIt also processes spatial, angular, and EPI cues simultaneously, rather than alternating between them, which preserves both spatial context and angular geometry.\nThe network becomes easier to train and can learn deeper, more expressive LF representations, leading to better reconstruction quality for all LF image processing tasks.\nDistgSSR Disentangling Mechanism for Spatial SR.\nDistgSSR applies the disentangling mechanism to do LF spatial SR.\nIt converts LR SAIs to MacPI, extracts spatial, angular, and EPI features, and uses a residual-in-residual structure for better SR quality. Only the Y channel is SR; Cb/Cr are bicubic-upscaled.\nThe 3 channels for image with YCbCr color space RGB:\nY = brightness Cb = blue color difference Cr = red color difference Most of the detail, texture, and edges are stored in the Y channel.\nColor (Cb, Cr) changes slowly and contains low-frequency information.\nDistg-Block Each Distg-Block has four parallel branches:\nSpatial branch: two SFEs to keep spatial details. Angular branch: AFE + 1×1 conv + 2D pixel shuffle for angular-to-spatial upsampling. Two EPI branches: EFE-H/EFE-V + 1×1 conv + 1D pixel shuffle to handle disparity through EPI lines. All branches are fused by 1×1 conv + SFE, with a skip connection for local residual learning.\nPixSF-1D:\nFor EFE-H, the angular variation is only 1D (the V direction) along a horizontal EPI (V–W). Convolution flattens these V angular samples into the horizontal (W) direction, compressing the angular information into the channel space in a 1D form.\nPixSF-1D then:\nun-flattens this 1D angular information. rearranges it back into the correct spatial structure. but only along one direction (horizontal). So PixSF-1D re-expands 1D angular EPIs into spatial feature maps along a single axis.\nSpatial Upsampling After all groups, MacPI features are reshaped back to SAIs.\nA 1×1 conv increases channels, a PixSF-2D upsamples the resolution, and a final 1×1 conv to reduce channels to 1, so the output is the HR Y-SAIs.\nCb and Cr from the input are bicubic-upsampled to the same HR size, channels (Y, Cb, Cr) are combined and convert to RGB version. That gives the final HR LF image.\nExperiments Achieve high spatial reconstruction quality and angular consistency.\nDistgASR Disentangling Mechanism for Angular SR.\nDistgASR applies the disentangling mechanism to angular super-resolution.\nIt takes a sparse angular SAI array and reconstructs a dense angular array. The input SAI array is first converted to a MacPI for spatial, angular, and EPI feature extraction.\nDistg-Block for ASR A Distg-Block separately processes spatial, angular, and EPI features. Then the features are fused.\nFor ASR, angular information is more important because we must create many new views. Also, disparity between sparse views is large, so EPI features are helpful.\nTherefore, the angular and EPI branches output C channels to keep more useful information.\nAngular Upsampling Angular upsampling must handle non-integer upsampling factors (for example 2×2 → 7×7).\nPixel shuffle cannot handle non-integer factors directly.\nSo the network uses a downsample–upsample strategy.\nSteps→\nAFE produces a downsampled angular feature. A 1×1 conv expands channels. A 2D pixel shuffle performs angular upsampling. Finally, a 1×1 conv + SFE recover the output SAI array. This allows clean and flexible angular SR, even when the upsampling factor is non-integer.\nDistgDISP Disentangling Mechanism For Disparity Estimation.\nDistgDisp applies the disentangling mechanism to light-field disparity estimation.\nThe network input is a MacPI with 9×9 angular views. The network performs:\nfeature extraction → cost volume construction → cost aggregation → disparity regression.\nSpatial Res-Block forFeature Extraction A Spatial Res-Block is used to extract spatial features. It uses SFE + BN + LeakyReLU + SFE + BN. A residual skip adds the input back to the block output. This helps model spatial context and smooth texture areas.\nDS-AFEs for CostVolume Construction Cost volume needs features from different disparities. Existing LF methods use “shift-and-concat”, which is slow. DistgDisp replaces this with DS-AFEs, which directly convolve pixels that match a specific disparity.\nDS-AFE works like this:\nFor each view (u, v), the offset of its corresponding pixel depends on disparity d. When converted into MacPI, pixels of the same disparity form a square pattern. A properly designed convolution (padding) can extract all pixels with disparity d in one pass. Different disparity values use different dilation and padding.\nThe author use 9 disparity levels (−4 to +4). For each disparity, a cost volume is produced. All cost volumes are concatenated into a 5D tensor B × 9 × C × H × W.\nCost Aggregation and Disparity Regression Eight 3D convolutions (3×3×3 kernel) aggregate the cost volumes. This produces a final 3D tensor of size D × H × W. A softmax is applied along the disparity axis.\nThe predicted disparity is:\n$$ \\hat{d} = \\sum_{d \\in D} d \\cdot softmax(F_{final}) $$\nThis gives the final disparity map.\n","wordCount":"1379","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/321a8e78-349b-448d-800b-0869ae55c8b8.png","datePublished":"2025-11-19T07:42:14Z","dateModified":"2025-11-19T07:42:14Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/disentangling_light_fields_for_super-resolution_an/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Disentangling Light Fields for Super-Resolution and Disparity Estimation</h1><div class=post-description>Paper-reading notes: Distangling mechanism, DistgSSR, DistgASR, DistgDisp</div><div class=post-meta><span title='2025-11-19 07:42:14 +0000 +0000'>November 19, 2025</span>&nbsp;·&nbsp;<span>1379 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a><ul><li><a href=#current-questions aria-label="Current Questions">Current Questions</a></li></ul></li><li><a href=#method aria-label=Method>Method</a><ul><li><a href=#lf-representation aria-label="LF Representation">LF Representation</a></li><li><a href=#lf-feature-disentanglement aria-label="LF Feature Disentanglement">LF Feature Disentanglement</a><ul><li><a href=#spatial-feature-extractor aria-label="Spatial Feature Extractor">Spatial Feature Extractor</a></li><li><a href=#angular-feature-extractor aria-label="Angular Feature Extractor">Angular Feature Extractor</a></li><li><a href=#epi-feature-extractors aria-label="EPI Feature Extractors">EPI Feature Extractors</a></li><li><a href=#how-it-solve-current-questions aria-label="How it solve current questions">How it solve current questions</a></li></ul></li><li><a href=#distgssr aria-label=DistgSSR>DistgSSR</a><ul><li><a href=#distg-block aria-label=Distg-Block>Distg-Block</a></li><li><a href=#spatial-upsampling aria-label="Spatial Upsampling">Spatial Upsampling</a></li><li><a href=#experiments aria-label=Experiments>Experiments</a></li></ul></li><li><a href=#distgasr aria-label=DistgASR>DistgASR</a><ul><li><a href=#distg-block-for-asr aria-label="Distg-Block for ASR">Distg-Block for ASR</a></li><li><a href=#angular-upsampling aria-label="Angular Upsampling">Angular Upsampling</a></li></ul></li><li><a href=#distgdisp aria-label=DistgDISP>DistgDISP</a><ul><li><a href=#spatial-res-block-forfeature-extraction aria-label="Spatial Res-Block forFeature Extraction">Spatial Res-Block forFeature Extraction</a></li><li><a href=#ds-afes-for-costvolume-construction aria-label="DS-AFEs for CostVolume Construction">DS-AFEs for CostVolume Construction</a></li><li><a href=#cost-aggregation-and-disparity-regression aria-label="Cost Aggregation and Disparity Regression">Cost Aggregation and Disparity Regression</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>CNNs treat inputs as <strong>general images</strong> with no special structure. But LF is <strong>not a random stack of images.</strong></p><p>To be more specific, CNN expects the disparity of all pixels are the same. But in light field, objects with different depth exist in the same images! So a standard CNN kernel cannot learn mixed spatial + angular + disparity correlations incorrectly. This is <strong>why 2D CNNs fail</strong> on LF directly. In short, A fixed convolution window cannot follow disparity.</p><p>There are many existing methods aiming to solve this problem.</p><h2 id=current-questions>Current Questions<a hidden class=anchor aria-hidden=true href=#current-questions>#</a></h2><p>The current CNN structures for LF image processing:</p><ol><li><strong>Neighbor-view combination</strong> → Since only 2 or 4 adjacent views were used to SR a specific view, this method cannot achieve a high reconstruction quality due to the discard of rich <strong>angular information</strong>.</li><li><strong>Convolution on EPIs →</strong> since an EPI is a 2D slice of a 4D LF, performing conv on EPIs can only incorporate angular information from the same horizontal or vertical views and cannot incorporate the <strong>spatial context prior</strong>.</li><li><strong>Multi-stream structure →</strong> it discards the views outside the four angular directions, resulting in under-exploitation of the rich <strong>angular information</strong> in an LF.</li><li><strong>Spatial-angular alternate convolution →</strong> it processes spatial and angular features separately. The model never learns deep joint spatial-angular representations.</li></ol><p>The key difference between existing schemes and disentangling mechanism is, disentangling mechanism can <strong>fully use the information from all angular views</strong> and incorporate the LF structure prior.</p><p>With this mechanism, high-dimensional LF data can be disentangled into different low-dimensional subspaces. Consequently, the difficulty for learning deep CNNs is reduced and several LF image processing tasks can be benefited.</p><aside><p>Take the 4D LF and separate into:</p><ol><li>Spatial component (x,y)</li><li>Angular component (u,v)</li><li>Disparity component (slopes in EPI)</li></ol><hr><p><strong>LF structural priors</strong> = the known geometric and angular relationships that exist naturally in light-field images.</p><hr><p>Now the CNN can learn:</p><ul><li>Disparity changes → from angular subspace</li><li>Texture & edges → from spatial subspace</li><li>Parallax = correlation between spatial & angular subspaces</li><li>LF 4D consistency = combining both subspaces</li></ul></aside><h1 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h1><h2 id=lf-representation>LF Representation<a hidden class=anchor aria-hidden=true href=#lf-representation>#</a></h2><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/image.png></p><ul><li><strong>SAIs</strong>: many 2D views arranged by angle.</li><li><strong>3D stack</strong>: SAIs stacked to show angular variation.</li><li><strong>EPI</strong>: a 2D slice that reveals disparity as straight lines.</li><li><strong>SAI array</strong>: full grid of all SAIs.</li><li><strong>MacPI</strong>: one 2D image where each spatial location stores angular samples.</li></ul><p>Compared to SAI and EPI, the authors adopt the <strong>MacPI</strong> representation because it evenly mixes the spatial and angular information in the light field. This makes it easier to extract and combine spatial and angular features using convolution operations.</p><h2 id=lf-feature-disentanglement>LF Feature Disentanglement<a hidden class=anchor aria-hidden=true href=#lf-feature-disentanglement>#</a></h2><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/4e6af2d6-df79-48fc-8cad-0393c96ca3bd.png></p><p>The 4D light field contains spatial (H, W) and angular (U, V) information. To make learning easier, the model uses <strong>three feature extractors</strong>, each handling one 2D subspace.</p><ul><li>Angular feature extractor,</li><li>Spatial feature extractor,</li><li>Vertical EPI feature extractor,</li><li>Horizontal EPI feature extractor.</li></ul><h3 id=spatial-feature-extractor>Spatial Feature Extractor<a hidden class=anchor aria-hidden=true href=#spatial-feature-extractor>#</a></h3><p>SFE extracts <strong>spatial texture</strong> inside the same view. It uses a 3×3 <strong>dilated</strong> convolution to avoid mixing different views. Output size stays the same as the input.</p><h3 id=angular-feature-extractor>Angular Feature Extractor<a hidden class=anchor aria-hidden=true href=#angular-feature-extractor>#</a></h3><p>AFE extracts <strong>angular information</strong> inside each macro-pixel. It uses an A×A convolution with stride A, so each angular block becomes one spatial pixel. Output size becomes <strong>H × W</strong>.</p><h3 id=epi-feature-extractors>EPI Feature Extractors<a hidden class=anchor aria-hidden=true href=#epi-feature-extractors>#</a></h3><p>SFE and AFE miss the <strong>spatial–angular relationship</strong>. EPIs show this relationship as line patterns.</p><ul><li>EFE-H processes <strong>horizontal EPIs (V–W)</strong>.</li><li>EFE-V processes <strong>vertical EPIs (U–H)</strong>.</li></ul><p>They learn <strong>disparity and spatial–angular correlation</strong>.</p><h3 id=how-it-solve-current-questions>How it solve current questions<a hidden class=anchor aria-hidden=true href=#how-it-solve-current-questions>#</a></h3><p>This design keeps all <strong>angular views</strong>, instead of discarding neighbors or directions, so the network can fully use the complete angular information.</p><p>It also processes spatial, angular, and EPI cues <strong>simultaneously</strong>, rather than alternating between them, which preserves both spatial context and angular geometry.</p><p>The network becomes easier to train and can learn deeper, more expressive LF representations, leading to better reconstruction quality for all LF image processing tasks.</p><h2 id=distgssr>DistgSSR<a hidden class=anchor aria-hidden=true href=#distgssr>#</a></h2><p>Disentangling Mechanism for Spatial SR.</p><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/321a8e78-349b-448d-800b-0869ae55c8b8.png></p><p>DistgSSR applies the <strong>disentangling mechanism</strong> to do LF spatial SR.</p><p>It converts LR SAIs to MacPI, extracts spatial, angular, and EPI features, and uses a <strong>residual-in-residual</strong> structure for better SR quality. Only the <strong>Y channel</strong> is SR; Cb/Cr are bicubic-upscaled.</p><aside><p>The 3 channels for image with <strong>YCbCr color space <del>RGB</del></strong>:</p><ul><li><strong>Y = brightness</strong></li><li><strong>Cb = blue color difference</strong></li><li><strong>Cr = red color difference</strong></li></ul><p>Most of the <strong>detail, texture, and edges</strong> are stored in the <strong>Y channel</strong>.</p><p>Color (Cb, Cr) changes slowly and contains <strong>low-frequency</strong> information.</p></aside><h3 id=distg-block>Distg-Block<a hidden class=anchor aria-hidden=true href=#distg-block>#</a></h3><p>Each Distg-Block has <strong>four parallel branches</strong>:</p><ul><li><strong>Spatial branch:</strong> two SFEs to keep spatial details.</li><li><strong>Angular branch:</strong> AFE + 1×1 conv + <strong>2D pixel shuffle</strong> for angular-to-spatial upsampling.</li><li><strong>Two EPI branches:</strong> EFE-H/EFE-V + 1×1 conv + <strong>1D pixel shuffle</strong> to handle disparity through EPI lines.</li></ul><p>All branches are fused by <strong>1×1 conv + SFE</strong>, with a skip connection for local residual learning.</p><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/image_1.png></p><aside><p><strong>PixSF-1D:</strong></p><p>For <strong>EFE-H</strong>, the <strong>angular variation is only 1D</strong> (the <strong>V</strong> direction) along a <strong>horizontal EPI (V–W)</strong>. Convolution flattens these V angular samples into the <strong>horizontal (W) direction</strong>, compressing the angular information into the <strong>channel</strong> space in a 1D form.</p><p><strong>PixSF-1D</strong> then:</p><ul><li><strong>un-flattens</strong> this 1D angular information.</li><li>rearranges it <strong>back into the correct spatial structure.</strong></li><li>but <strong>only along one direction</strong> (horizontal).</li></ul><p><strong>So PixSF-1D re-expands 1D angular EPIs into spatial feature maps along a single axis.</strong></p></aside><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/image_2.png></p><h3 id=spatial-upsampling>Spatial Upsampling<a hidden class=anchor aria-hidden=true href=#spatial-upsampling>#</a></h3><p>After all groups, MacPI features are reshaped back to SAIs.</p><p>A 1×1 conv increases channels, a <strong>PixSF-2D</strong> upsamples the resolution, and a final 1×1 conv to reduce channels to 1, so the output is the HR <strong>Y-SAIs</strong>.</p><p><strong>Cb</strong> and <strong>Cr</strong> from the input are bicubic-upsampled to the same HR size, channels (Y, Cb, Cr) are combined and convert to RGB version. That gives the final HR LF image.</p><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/e9b3ae0b-2969-4cf9-a851-061dc03b7675.png></p><h3 id=experiments>Experiments<a hidden class=anchor aria-hidden=true href=#experiments>#</a></h3><p>Achieve <strong>high spatial reconstruction quality</strong> and <strong>angular consistency.</strong></p><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/image_3.png></p><h2 id=distgasr>DistgASR<a hidden class=anchor aria-hidden=true href=#distgasr>#</a></h2><p>Disentangling Mechanism for Angular SR.</p><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/image_4.png></p><p>DistgASR applies the disentangling mechanism to <strong>angular super-resolution</strong>.</p><p>It takes a <strong>sparse angular SAI array</strong> and reconstructs a <strong>dense angular array</strong>. The input SAI array is first converted to a <strong>MacPI</strong> for spatial, angular, and EPI feature extraction.</p><h3 id=distg-block-for-asr>Distg-Block for ASR<a hidden class=anchor aria-hidden=true href=#distg-block-for-asr>#</a></h3><p>A Distg-Block separately processes <strong>spatial</strong>, <strong>angular</strong>, and <strong>EPI</strong> features. Then the features are fused.</p><p>For ASR, angular information is more important because we must create many new views. Also, disparity between sparse views is large, so EPI features are helpful.</p><p>Therefore, the angular and EPI branches output <strong>C channels</strong> to keep more useful information.</p><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/image_5.png></p><h3 id=angular-upsampling>Angular Upsampling<a hidden class=anchor aria-hidden=true href=#angular-upsampling>#</a></h3><p>Angular upsampling must handle <strong>non-integer upsampling factors</strong> (for example 2×2 → 7×7).</p><p>Pixel shuffle cannot handle non-integer factors directly.</p><p>So the network uses a <strong>downsample–upsample strategy</strong>.</p><p>Steps→</p><ol><li>AFE produces a <strong>downsampled angular feature</strong>.</li><li>A <strong>1×1 conv</strong> expands channels.</li><li>A <strong>2D pixel shuffle</strong> performs angular upsampling.</li><li>Finally, a <strong>1×1 conv + SFE</strong> recover the output SAI array.</li></ol><p>This allows clean and flexible angular SR, even when the upsampling factor is non-integer.</p><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/cb7281c0-a000-40a3-b37a-7dde14dd87fe.png></p><h2 id=distgdisp>DistgDISP<a hidden class=anchor aria-hidden=true href=#distgdisp>#</a></h2><p>Disentangling Mechanism For Disparity Estimation.</p><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/image_6.png></p><p>DistgDisp applies the disentangling mechanism to <strong>light-field disparity estimation</strong>.</p><p>The network input is a <strong>MacPI</strong> with 9×9 angular views. The network performs:</p><p><strong>feature extraction → cost volume construction → cost aggregation → disparity regression.</strong></p><h3 id=spatial-res-block-forfeature-extraction>Spatial Res-Block forFeature Extraction<a hidden class=anchor aria-hidden=true href=#spatial-res-block-forfeature-extraction>#</a></h3><p>A <strong>Spatial Res-Block</strong> is used to extract <strong>spatial features</strong>. It uses <strong>SFE + BN + LeakyReLU + SFE + BN</strong>. A residual skip adds the input back to the block output. This helps model spatial context and smooth texture areas.</p><h3 id=ds-afes-for-costvolume-construction>DS-AFEs for CostVolume Construction<a hidden class=anchor aria-hidden=true href=#ds-afes-for-costvolume-construction>#</a></h3><p>Cost volume needs features from <strong>different disparities</strong>. Existing LF methods use “shift-and-concat”, which is slow. DistgDisp replaces this with <strong>DS-AFEs</strong>, which directly convolve pixels that match a specific disparity.</p><p>DS-AFE works like this:</p><ul><li>For each view (u, v), the offset of its corresponding pixel depends on <strong>disparity d</strong>.</li><li>When converted into MacPI, pixels of the same disparity form a <strong>square pattern</strong>.</li><li>A properly designed convolution (padding) can extract all pixels with disparity <strong>d</strong> in one pass.</li></ul><p>Different disparity values use different <strong>dilation and padding</strong>.</p><p>The author use 9 disparity levels (−4 to +4). For each disparity, a <strong>cost volume</strong> is produced. All cost volumes are concatenated into a 5D tensor <strong>B × 9 × C × H × W</strong>.</p><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/image_7.png></p><p><img alt=image.png loading=lazy src=/notes/disentangling_light_fields_for_super-resolution_an/image_8.png></p><h3 id=cost-aggregation-and-disparity-regression>Cost Aggregation and Disparity Regression<a hidden class=anchor aria-hidden=true href=#cost-aggregation-and-disparity-regression>#</a></h3><p>Eight <strong>3D convolutions</strong> (3×3×3 kernel) aggregate the cost volumes. This produces a final 3D tensor of size <strong>D × H × W</strong>. A softmax is applied along the disparity axis.</p><p>The predicted disparity is:</p><p>$$
\hat{d} = \sum_{d \in D} d \cdot softmax(F_{final})
$$</p><p>This gives the final disparity map.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>