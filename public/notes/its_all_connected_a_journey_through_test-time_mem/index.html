<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>It’s All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: MIRAS"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/"><meta property="og:site_name" content="Home"><meta property="og:title" content="It’s All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization"><meta property="og:description" content="Paper-reading notes: MIRAS"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-12-06T15:13:02+00:00"><meta property="article:modified_time" content="2025-12-06T15:13:02+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/eb27ba57-576b-4452-89bb-12d4dae9b6a7.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/image_1.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/image_2.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/image_3.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/eb27ba57-576b-4452-89bb-12d4dae9b6a7.png"><meta name=twitter:title content="It’s All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization"><meta name=twitter:description content="Paper-reading notes: MIRAS"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"It’s All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization","item":"https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"It’s All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization","name":"It’s All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization","description":"Paper-reading notes: MIRAS","keywords":[],"articleBody":"1. Introduction Modern sequence models (especially Transformers) achieve strong performance due to their ability to learn from long contexts at scale. However, Transformers suffer from quadratic complexity and linearly growing memory (KV cache), which limits long-context modeling. To overcome this, recent research develops efficient recurrent alternatives that compress information into fixed-size memory, focusing on:\nLearning rules (from Hebbian → Delta → new variants) Forget gates (from LSTM → Mamba → Titan gates) Memory architectures (vector memory in RetNet, LRU; deep memory in Titans and TTT) These advances raise a central question:\nWhat is the unified design framework behind all these sequence models, and how can we extend it?\nThe authors reinterpret Transformers, Titans, and modern linear RNNs as associative memory systems, guided by a new concept called attentional bias—the internal objective that determines how models learn mappings between keys and values. Surprisingly, they observe that almost all existing models use only two types of attentional bias: dot-product similarity or ℓ₂ regression.\nBased on this insight, they reinterpret forgetting mechanisms as retention ℓ₂-regularization, then introduce Miras, a general design framework defined by four choices:\nAttentional bias (memory objective) Retention gate Memory architecture Memory learning algorithm (optimizer) Using Miras, they create three new sequence models—Moneta, Yaad, and Memora—that incorporate new attentional biases and robust forgetting mechanisms. Experiments show that these models outperform current architectures in language modeling, reasoning, and memory-intensive tasks.\n2. Method 2.1. Associative Memory and Attentional Bias Associative memory maps keys (K) to values (V). The mapping is learned by optimizing an objective $\\mathcal{L}$ called attentional bias.\nFormally, memory $\\mathcal{M}$ is learned by:\n$$ \\mathcal{M}^* = \\arg\\min_{\\mathcal{M}} \\mathcal{L}(\\mathcal{M}(K); V). $$\nRemark 1 When memory is parameterized by a matrix $W$, we optimize $W$, not $\\mathcal{M}$. We may also add regularization $R(W)$ to retain past memory. Remark 2 Learning keys–values is a meta-learning problem: inner-loop optimizes memory; outer-loop optimizes the rest of the network. Remark 3 Forgetting is not explicit erasing; rather, the model may fail to retrieve past memory. Therefore they use the term “Retention Gate”, not “Forget Gate”. Remark 4 Most modern sequence models optimize the associative memory objective (attentional bias) via gradient descent. The theory applies beyond GD, any optimization method can be used. 2.1.1. Learning to Memorize and Retain (Optimization View) Memory is updated by gradient descent:\n$$ W_t = W_{t-1} - \\eta_t \\nabla \\ell(W_{t-1}; k_t, v_t), $$\nwhere $\\ell$ is the attentional bias applied to the latest pair.\n2.1.2. Viewpoint 1: Online Regression and Follow-The-Regularized-Leader Gradient descent can be interpreted as minimizing a sequence of losses:\n$$ \\ell(W; k_1, v_1), \\ell(W; k_2, v_2), \\ldots $$\nEquivalent formulation:\n$$ W_t = \\arg\\min_W \\sum_{i=1}^t \\langle W - W_{t-1}, \\nabla \\ell(W_{t-1}; k_i, v_i) \\rangle + \\frac{1}{2\\eta_t}|W|^2.\n$$\nThe first term measures how well memory fits new data;\nthe second term is a regularizer that stabilizes memory size.\nGeneral FTRL form:\n$$ W_t = \\arg\\min_{W \\in \\mathcal{W}} \\left(\\sum_{i=1}^t \\tilde{\\ell}_i(W; k_i, v_i)\\right) + \\frac{1}{\\eta_t} R_t(W).\n$$\nHere:\n$\\tilde{\\ell}_i$ = approximated attentional bias $R_t(W)$ = memory stability regularizer 2.1.3. Viewpoint 2: Learning the Latest Token While Retaining Previous Memory Another interpretation decomposes memory update into: Learning new info + Retaining old memory.\nEquivalent update:\n$$ W_t = \\arg\\min_W \\Big( \\langle W - W_{t-1}, \\nabla \\ell(W_{t-1}; k_t, v_t)\\rangle + \\frac{1}{2\\eta_t} |W - W_{t-1}|^2 \\Big).\n$$\nThe form generalizes to:\n$$ W_t = \\arg\\min_{W \\in \\mathcal{W}} \\Big( \\tilde{\\ell}_t(W; k_t, v_t) + \\text{Ret}_t(W, W{t-1}) \\Big). $$\nAttentional Bias: $\\tilde{\\ell}_t(W; k_t, v_t)$ → learns new key–value mapping. Retention: $\\text{Ret}_t(W, W{t-1})$ → encourages memory to stay close to its previous state. Retention has local and global components:\n$$ \\text{Ret}t(W, W{t-1}) = \\frac{1}{\\eta_t} D_t(W, W_{t-1}) + \\frac{1}{\\alpha_t} G_t(W).\n$$\n$D_t$: local retention → prevents forgetting $G_t$: global retention → controls memory magnitude 2.1.4. Connection Between the Two Viewpoints Both viewpoints describe the same process using online optimization concepts. The two formulations are equivalent under mild assumptions.\nThe FTRL viewpoint emphasizes loss over time + regularization. The Learning–Retaining viewpoint emphasizes new learning + memory retention. 2.2. MIRAS MIRAS says every sequence model is defined by 4 choices:\nMemory Structure\nWhat the memory looks like vector, matrix, MLP.\nAttentional Bias\nThe loss used to learn key→value mapping like dot-product, $\\ell_2$, $\\ell_p$, Huber, KL. → loss function\nRetention Gate\nControls how much old memory is kept. Like: $|W - W_{t-1}|^2$, KL divergence, elastic net, etc.\nMemory Algorithm\nHow memory is updated (GD, momentum, Newton, etc.). → optimizer\nAttentional Bias = write new info.\nRetention Gate = keep old info.\nMemory Learning Algorithm = the formula that mixes them into the final memory update.\nAll existing models fit this form.\nExamples:\nHebbian RNNs (RetNet, LA) $$ M_t = \\alpha M_{t-1} + v_t k_t^\\top $$\nDelta rule models (DeltaNet, RWKV)\nThey optimize MSE: $|M(k_t) - v_t|^2$.\nTitans / TTT\nUse deep memory + gradient descent with retention.\n2.3. Architecture Backbone and Fast Training Architectural Backbone for Miras’s Variants: Moneta, Yaad, and Memora Replace the attention block with a MIRAS block inside a Llama-style model. Use modern components: SwiGLU MLPs, RoPE, RMSNorm, depthwise conv, and ℓ₂-normed q/k. Channel-wise Parameters Parameters like $\\eta_t, \\delta_t, \\alpha_t$ are learned per channel. To reduce cost, apply low-rank projections. Hybrid Models MIRAS layers can be combined with Sliding Window Attention (SWA). Parallel Training Recurrence is broken using chunking: split the sequence into chunks and compute gradients per chunk. This makes training fast and parallelizable. Core recurrence idea Inside a chunk, replace:\n$$ M_t = \\alpha_t M_{t-1} - \\eta_t \\nabla \\ell $$\nwith a parallel form:\n$$ M_t = \\beta_t M_0 - \\sum_{i=1}^t \\frac{\\beta_t}{\\beta_i}\\eta_i\\nabla\\ell(M_0;k_i,v_i) $$\nso no step-by-step recurrence is needed.\n3. Comparison ","wordCount":"923","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/eb27ba57-576b-4452-89bb-12d4dae9b6a7.png","datePublished":"2025-12-06T15:13:02Z","dateModified":"2025-12-06T15:13:02Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/its_all_connected_a_journey_through_test-time_mem/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">It’s All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization</h1><div class=post-description>Paper-reading notes: MIRAS</div><div class=post-meta><span title='2025-12-06 15:13:02 +0000 +0000'>December 6, 2025</span>&nbsp;·&nbsp;<span>923 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-introduction aria-label="1. Introduction">1. Introduction</a></li><li><a href=#2-method aria-label="2. Method">2. Method</a><ul><li><a href=#21-associative-memory-and-attentional-bias aria-label="2.1. Associative Memory and Attentional Bias">2.1. Associative Memory and Attentional Bias</a><ul><li><a href=#remark-1 aria-label="Remark 1">Remark 1</a></li><li><a href=#remark-2 aria-label="Remark 2">Remark 2</a></li><li><a href=#remark-3 aria-label="Remark 3">Remark 3</a></li><li><a href=#remark-4 aria-label="Remark 4">Remark 4</a></li><li><a href=#211-learning-to-memorize-and-retain-optimization-view aria-label="2.1.1. Learning to Memorize and Retain (Optimization View)">2.1.1. Learning to Memorize and Retain (Optimization View)</a></li><li><a href=#212-viewpoint-1-online-regression-and-follow-the-regularized-leader aria-label="2.1.2. Viewpoint 1: Online Regression and Follow-The-Regularized-Leader">2.1.2. Viewpoint 1: Online Regression and Follow-The-Regularized-Leader</a></li><li><a href=#213-viewpoint-2-learning-the-latest-token-while-retaining-previous-memory aria-label="2.1.3. Viewpoint 2: Learning the Latest Token While Retaining Previous Memory">2.1.3. Viewpoint 2: Learning the Latest Token While Retaining Previous Memory</a></li><li><a href=#214-connection-between-the-two-viewpoints aria-label="2.1.4. Connection Between the Two Viewpoints">2.1.4. Connection Between the Two Viewpoints</a></li></ul></li><li><a href=#22-miras aria-label="2.2. MIRAS">2.2. MIRAS</a></li><li><a href=#23-architecture-backbone-and-fast-training aria-label="2.3. Architecture Backbone and Fast Training">2.3. Architecture Backbone and Fast Training</a><ul><li><a href=#architectural-backbone-for-mirass-variants-moneta-yaad-and-memora aria-label="Architectural Backbone for Miras’s Variants: Moneta, Yaad, and Memora">Architectural Backbone for Miras’s Variants: Moneta, Yaad, and Memora</a></li><li><a href=#channel-wise-parameters aria-label="Channel-wise Parameters">Channel-wise Parameters</a></li><li><a href=#hybrid-models aria-label="Hybrid Models">Hybrid Models</a></li><li><a href=#parallel-training aria-label="Parallel Training">Parallel Training</a></li><li><a href=#core-recurrence-idea aria-label="Core recurrence idea">Core recurrence idea</a></li></ul></li></ul></li><li><a href=#3-comparison aria-label="3. Comparison">3. Comparison</a></li></ul></div></details></div><div class=post-content><h1 id=1-introduction>1. Introduction<a hidden class=anchor aria-hidden=true href=#1-introduction>#</a></h1><p>Modern sequence models (especially Transformers) achieve strong performance due to their ability to learn from long contexts at scale. However, Transformers suffer from <strong>quadratic complexity</strong> and <strong>linearly growing memory (KV cache)</strong>, which limits long-context modeling. To overcome this, recent research develops <strong>efficient recurrent alternatives</strong> that compress information into <strong>fixed-size memory</strong>, focusing on:</p><ol><li><strong>Learning rules</strong> (from Hebbian → Delta → new variants)</li><li><strong>Forget gates</strong> (from LSTM → Mamba → Titan gates)</li><li><strong>Memory architectures</strong> (vector memory in RetNet, LRU; deep memory in Titans and TTT)</li></ol><p>These advances raise a central question:</p><aside><p>What is the unified design framework behind all these sequence models, and how can we extend it?</p></aside><p>The authors reinterpret Transformers, Titans, and modern linear RNNs as <strong>associative memory systems</strong>, guided by a new concept called <strong>attentional bias</strong>—the internal objective that determines how models learn mappings between keys and values. Surprisingly, they observe that <strong>almost all existing models use only two types of attentional bias: dot-product similarity or ℓ₂ regression</strong>.</p><p>Based on this insight, they reinterpret forgetting mechanisms as <strong>retention ℓ₂-regularization</strong>, then introduce <strong>Miras</strong>, a general design framework defined by four choices:</p><ol><li>Attentional bias (memory objective)</li><li>Retention gate</li><li>Memory architecture</li><li>Memory learning algorithm (optimizer)</li></ol><p>Using Miras, they create <strong>three new sequence models</strong>—<strong>Moneta, Yaad, and Memora</strong>—that incorporate new attentional biases and robust forgetting mechanisms. Experiments show that these models outperform current architectures in language modeling, reasoning, and memory-intensive tasks.</p><h1 id=2-method>2. Method<a hidden class=anchor aria-hidden=true href=#2-method>#</a></h1><h2 id=21-associative-memory-and-attentional-bias>2.1. Associative Memory and Attentional Bias<a hidden class=anchor aria-hidden=true href=#21-associative-memory-and-attentional-bias>#</a></h2><p>Associative memory maps <strong>keys (K)</strong> to <strong>values (V)</strong>. The mapping is learned by optimizing an <strong>objective $\mathcal{L}$</strong> called <strong>attentional bias</strong>.</p><p>Formally, memory $\mathcal{M}$ is learned by:</p><p>$$
\mathcal{M}^* = \arg\min_{\mathcal{M}} \mathcal{L}(\mathcal{M}(K); V).
$$</p><h3 id=remark-1>Remark 1<a hidden class=anchor aria-hidden=true href=#remark-1>#</a></h3><ul><li>When memory is parameterized by a matrix $W$, we optimize $W$, not $\mathcal{M}$.</li><li>We may also add <strong>regularization $R(W)$</strong> to retain past memory.</li></ul><h3 id=remark-2>Remark 2<a hidden class=anchor aria-hidden=true href=#remark-2>#</a></h3><ul><li>Learning keys–values is a <strong>meta-learning problem</strong>: inner-loop optimizes memory; outer-loop optimizes the rest of the network.</li></ul><h3 id=remark-3>Remark 3<a hidden class=anchor aria-hidden=true href=#remark-3>#</a></h3><ul><li>Forgetting is not explicit erasing; rather, the model may fail to retrieve past memory.</li><li>Therefore they use the term <strong>“Retention Gate”</strong>, not “Forget Gate”.</li></ul><h3 id=remark-4>Remark 4<a hidden class=anchor aria-hidden=true href=#remark-4>#</a></h3><ul><li>Most modern sequence models optimize the associative memory objective (attentional bias) via <strong>gradient descent</strong>.</li><li>The theory applies beyond GD, any optimization method can be used.</li></ul><h3 id=211-learning-to-memorize-and-retain-optimization-view><strong>2.1.1. Learning to Memorize and Retain (Optimization View)</strong><a hidden class=anchor aria-hidden=true href=#211-learning-to-memorize-and-retain-optimization-view>#</a></h3><p>Memory is updated by <strong>gradient descent</strong>:</p><p>$$
W_t = W_{t-1} - \eta_t \nabla \ell(W_{t-1}; k_t, v_t),
$$</p><p>where $\ell$ is the attentional bias applied to the latest pair.</p><h3 id=212-viewpoint-1-online-regression-and-follow-the-regularized-leader><strong>2.1.2. Viewpoint 1: Online Regression and Follow-The-Regularized-Leader</strong><a hidden class=anchor aria-hidden=true href=#212-viewpoint-1-online-regression-and-follow-the-regularized-leader>#</a></h3><p>Gradient descent can be interpreted as minimizing a sequence of losses:</p><p>$$
\ell(W; k_1, v_1), \ell(W; k_2, v_2), \ldots
$$</p><p>Equivalent formulation:</p><p>$$
W_t = \arg\min_W \sum_{i=1}^t \langle W - W_{t-1}, \nabla \ell(W_{t-1}; k_i, v_i) \rangle
+
\frac{1}{2\eta_t}|W|^2.</p><p>$$</p><ul><li><p>The first term measures how well memory fits new data;</p><p>the second term is a <strong>regularizer</strong> that stabilizes memory size.</p></li></ul><p>General FTRL form:</p><p>$$
W_t = \arg\min_{W \in \mathcal{W}}
\left(\sum_{i=1}^t \tilde{\ell}_i(W; k_i, v_i)\right)
+
\frac{1}{\eta_t} R_t(W).</p><p>$$</p><p>Here:</p><ul><li>$\tilde{\ell}_i$ = approximated attentional bias</li><li>$R_t(W)$ = memory stability regularizer</li></ul><h3 id=213-viewpoint-2-learning-the-latest-token-while-retaining-previous-memory><strong>2.1.3. Viewpoint 2: Learning the Latest Token While Retaining Previous Memory</strong><a hidden class=anchor aria-hidden=true href=#213-viewpoint-2-learning-the-latest-token-while-retaining-previous-memory>#</a></h3><p>Another interpretation decomposes memory update into: <strong>Learning new info</strong> + <strong>Retaining old memory</strong>.</p><p>Equivalent update:</p><p>$$
W_t = \arg\min_W
\Big( \langle W - W_{t-1}, \nabla \ell(W_{t-1}; k_t, v_t)\rangle
+
\frac{1}{2\eta_t} |W - W_{t-1}|^2 \Big).</p><p>$$</p><p>The form generalizes to:</p><p>$$
W_t = \arg\min_{W \in \mathcal{W}}
\Big( \tilde{\ell}_t(W; k_t, v_t) + \text{Ret}_t(W, W{t-1}) \Big).
$$</p><ul><li><strong>Attentional Bias:</strong> $\tilde{\ell}_t(W; k_t, v_t)$ → learns new key–value mapping.</li><li><strong>Retention:</strong> $\text{Ret}_t(W, W{t-1})$ → encourages memory to stay close to its previous state.</li></ul><p>Retention has <strong>local</strong> and <strong>global</strong> components:</p><p>$$
\text{Ret}t(W, W{t-1}) =
\frac{1}{\eta_t} D_t(W, W_{t-1})
+
\frac{1}{\alpha_t} G_t(W).</p><p>$$</p><ul><li>$D_t$: local retention → prevents forgetting</li><li>$G_t$: global retention → controls memory magnitude</li></ul><h3 id=214-connection-between-the-two-viewpoints><strong>2.1.4. Connection Between the Two Viewpoints</strong><a hidden class=anchor aria-hidden=true href=#214-connection-between-the-two-viewpoints>#</a></h3><p>Both viewpoints describe the same process using online optimization concepts. The two formulations are equivalent under mild assumptions.</p><ul><li>The FTRL viewpoint emphasizes <strong>loss over time + regularization</strong>.</li><li>The Learning–Retaining viewpoint emphasizes <strong>new learning + memory retention</strong>.</li></ul><h2 id=22-miras>2.2. MIRAS<a hidden class=anchor aria-hidden=true href=#22-miras>#</a></h2><p>MIRAS says every sequence model is defined by <strong>4 choices</strong>:</p><ol><li><p><strong>Memory Structure</strong></p><p>What the memory looks like vector, matrix, MLP.</p></li><li><p><strong>Attentional Bias</strong></p><p>The loss used to learn key→value mapping like dot-product, $\ell_2$, $\ell_p$, Huber, KL. → loss function</p></li><li><p><strong>Retention Gate</strong></p><p>Controls how much old memory is kept. Like: $|W - W_{t-1}|^2$, KL divergence, elastic net, etc.</p></li><li><p><strong>Memory Algorithm</strong></p><p>How memory is updated (GD, momentum, Newton, etc.). → optimizer</p></li></ol><hr><p><img alt=image.png loading=lazy src=/notes/its_all_connected_a_journey_through_test-time_mem/image.png></p><aside><p>Attentional Bias = write new info.</p><p>Retention Gate = keep old info.</p><p>Memory Learning Algorithm = the formula that mixes them into the final memory update.</p></aside><hr><p>All existing models fit this form.</p><p><img alt=image.png loading=lazy src=/notes/its_all_connected_a_journey_through_test-time_mem/image_1.png></p><p><strong>Examples:</strong></p><ul><li><strong>Hebbian RNNs (RetNet, LA)</strong></li></ul><p>$$
M_t = \alpha M_{t-1} + v_t k_t^\top
$$</p><ul><li><p><strong>Delta rule models (DeltaNet, RWKV)</strong></p><p>They optimize MSE: $|M(k_t) - v_t|^2$.</p></li><li><p><strong>Titans / TTT</strong></p><p>Use deep memory + gradient descent with retention.</p></li></ul><p><img alt=image.png loading=lazy src=/notes/its_all_connected_a_journey_through_test-time_mem/eb27ba57-576b-4452-89bb-12d4dae9b6a7.png></p><h2 id=23-architecture-backbone-and-fast-training><strong>2.3. Architecture Backbone and Fast Training</strong><a hidden class=anchor aria-hidden=true href=#23-architecture-backbone-and-fast-training>#</a></h2><h3 id=architectural-backbone-for-mirass-variants-moneta-yaad-and-memora><strong>Architectural Backbone for Miras’s Variants: Moneta, Yaad, and Memora</strong><a hidden class=anchor aria-hidden=true href=#architectural-backbone-for-mirass-variants-moneta-yaad-and-memora>#</a></h3><ul><li>Replace the attention block with a MIRAS block inside a Llama-style model.</li><li>Use modern components: SwiGLU MLPs, RoPE, RMSNorm, depthwise conv, and ℓ₂-normed q/k.</li></ul><p><img alt=image.png loading=lazy src=/notes/its_all_connected_a_journey_through_test-time_mem/image_2.png></p><h3 id=channel-wise-parameters><strong>Channel-wise Parameters</strong><a hidden class=anchor aria-hidden=true href=#channel-wise-parameters>#</a></h3><ul><li>Parameters like $\eta_t, \delta_t, \alpha_t$ are learned per channel.</li><li>To reduce cost, apply <strong>low-rank projections</strong>.</li></ul><h3 id=hybrid-models><strong>Hybrid Models</strong><a hidden class=anchor aria-hidden=true href=#hybrid-models>#</a></h3><ul><li>MIRAS layers can be combined with <strong>Sliding Window Attention</strong> (SWA).</li></ul><h3 id=parallel-training><strong>Parallel Training</strong><a hidden class=anchor aria-hidden=true href=#parallel-training>#</a></h3><ul><li>Recurrence is broken using <strong>chunking</strong>: split the sequence into chunks and compute gradients per chunk.</li><li>This makes training fast and parallelizable.</li></ul><h3 id=core-recurrence-idea><strong>Core recurrence idea</strong><a hidden class=anchor aria-hidden=true href=#core-recurrence-idea>#</a></h3><p>Inside a chunk, replace:</p><p>$$
M_t = \alpha_t M_{t-1} - \eta_t \nabla \ell
$$</p><p>with a <strong>parallel form</strong>:</p><p>$$
M_t = \beta_t M_0 - \sum_{i=1}^t \frac{\beta_t}{\beta_i}\eta_i\nabla\ell(M_0;k_i,v_i)
$$</p><p>so no step-by-step recurrence is needed.</p><h1 id=3-comparison>3. Comparison<a hidden class=anchor aria-hidden=true href=#3-comparison>#</a></h1><p><img alt=image.png loading=lazy src=/notes/its_all_connected_a_journey_through_test-time_mem/image_3.png></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>