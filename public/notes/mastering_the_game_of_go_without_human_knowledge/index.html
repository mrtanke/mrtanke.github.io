<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Mastering the game of Go without human knowledge | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: AlphaGo Zero"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Mastering the game of Go without human knowledge"><meta property="og:description" content="Paper-reading notes: AlphaGo Zero"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-11-24T08:38:30+00:00"><meta property="article:modified_time" content="2025-11-24T08:38:30+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/image_1.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/image_2.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/image_3.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/image_4.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/image.png"><meta name=twitter:title content="Mastering the game of Go without human knowledge"><meta name=twitter:description content="Paper-reading notes: AlphaGo Zero"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Mastering the game of Go without human knowledge","item":"https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Mastering the game of Go without human knowledge","name":"Mastering the game of Go without human knowledge","description":"Paper-reading notes: AlphaGo Zero","keywords":[],"articleBody":"Introduction AlphaGo AlphaGo was the first AI to beat top human Go players. AlphaGo combines CNN with MCTS to play Go at a superhuman level.\nIt first trains two types of networks (policy network \u0026 value network \u0026 rollout policy), using the current game board combined with several handcrafted Go features as input:\nThen integrates the networks into MCTS to enhance the basic tree search.\nAlphaGo Zero removes all human input.\nIt learns only from self-play and starts from random play. Its input is just the raw board (white_stones), with no handcrafted features. It uses one unified neural network that outputs both policy and value. Its search is simpler (no rollouts) and relies entirely on the neural network. Method AlphaGo Zero Network The network takes the raw board state and outputs:\np: a distribution over moves v: probability of winning It is made from deep residual blocks with batch norm and ReLU.\nTraining the Network In each training position, MCTS is run using the current network.\nMCTS produces an improved move distribution π through simulation like AlphaGo.\nMCTS can therefore be viewed as a policy improvement operator. A new move is selected from π, and the self-play game continues.\nAfter each game, the terminal result gives a final value z.\nTraining uses three terms to update the network:\n(z − v)² → match value prediction to the outcome −πᵀ log p → match network policy to the MCTS policy L2 regularization Continuous Self-Improvement AlphaGo Zero repeatedly plays self-play games, updates the network, and replaces the old version when the new model wins enough evaluation games.\nOver many iterations, the system improves rapidly and surpasses earlier AlphaGo versions.\nNovelty It’s trained solely by self-play RL, starting from ran­dom play, without any supervision or use of human data. It uses only the black and white stones from the board as input features. It uses a single neural network, rather than separate policy and value networks. It uses a simpler tree search that relies upon this single neural network to evaluate positions and sample moves. ","wordCount":"342","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/image.png","datePublished":"2025-11-24T08:38:30Z","dateModified":"2025-11-24T08:38:30Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/mastering_the_game_of_go_without_human_knowledge/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Mastering the game of Go without human knowledge</h1><div class=post-description>Paper-reading notes: AlphaGo Zero</div><div class=post-meta><span title='2025-11-24 08:38:30 +0000 +0000'>November 24, 2025</span>&nbsp;·&nbsp;<span>342 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a><ul><li><a href=#alphago aria-label=AlphaGo>AlphaGo</a></li></ul></li><li><a href=#method aria-label=Method>Method</a><ul><li><a href=#alphago-zero aria-label="AlphaGo Zero">AlphaGo Zero</a><ul><li><a href=#network aria-label=Network>Network</a></li><li><a href=#training-the-network aria-label="Training the Network">Training the Network</a></li><li><a href=#continuous-self-improvement aria-label="Continuous Self-Improvement">Continuous Self-Improvement</a></li></ul></li></ul></li><li><a href=#novelty aria-label=Novelty>Novelty</a></li></ul></div></details></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><h2 id=alphago>AlphaGo<a hidden class=anchor aria-hidden=true href=#alphago>#</a></h2><p>AlphaGo was the first AI to beat top human Go players. AlphaGo combines <strong>CNN</strong> with <strong>MCTS</strong> to play Go at a superhuman level.</p><p>It first trains two types of networks (<strong>policy network</strong> & <strong>value network & rollout policy</strong>), using the current game board combined with several handcrafted Go features as <strong>input</strong>:</p><p><img alt=image.png loading=lazy src=/notes/mastering_the_game_of_go_without_human_knowledge/image.png></p><p>Then integrates the networks into MCTS to enhance the basic tree search.</p><p><img alt=image.png loading=lazy src=/notes/mastering_the_game_of_go_without_human_knowledge/image_1.png></p><p><strong>AlphaGo Zero</strong> removes all human input.</p><ul><li>It learns <strong>only</strong> from self-play and starts from random play.</li><li>Its input is <strong>just the raw board</strong> (white_stones), with no handcrafted features.</li><li>It uses <strong>one unified neural network</strong> that outputs both policy and value.</li><li>Its search is simpler (no rollouts) and relies entirely on the neural network.</li></ul><h1 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h1><h2 id=alphago-zero>AlphaGo Zero<a hidden class=anchor aria-hidden=true href=#alphago-zero>#</a></h2><h3 id=network>Network<a hidden class=anchor aria-hidden=true href=#network>#</a></h3><p>The network takes the raw board state and outputs:</p><ul><li><strong>p</strong>: a distribution over moves</li><li><strong>v</strong>: probability of winning</li></ul><p>It is made from deep residual blocks with <strong>batch norm</strong> and <strong>ReLU</strong>.</p><p><img alt=image.png loading=lazy src=/notes/mastering_the_game_of_go_without_human_knowledge/image_2.png></p><h3 id=training-the-network>Training the Network<a hidden class=anchor aria-hidden=true href=#training-the-network>#</a></h3><p>In each training position, MCTS is run using the current network.</p><p>MCTS produces an improved move distribution <strong>π</strong> through simulation like AlphaGo.</p><p><img alt=image.png loading=lazy src=/notes/mastering_the_game_of_go_without_human_knowledge/image_3.png></p><p>MCTS can therefore be viewed as a <strong>policy improvement operator</strong>. A new move is selected from π, and the self-play game continues.</p><p><img alt=image.png loading=lazy src=/notes/mastering_the_game_of_go_without_human_knowledge/image_4.png></p><p>After each game, the terminal result gives a final value <strong>z</strong>.</p><p>Training uses three terms to update the network:</p><ul><li>(z − v)² → match value prediction to the outcome</li><li>−πᵀ log p → match network policy to the MCTS policy</li><li>L2 regularization</li></ul><h3 id=continuous-self-improvement><strong>Continuous Self-Improvement</strong><a hidden class=anchor aria-hidden=true href=#continuous-self-improvement>#</a></h3><p>AlphaGo Zero repeatedly plays self-play games, updates the network, and replaces the old version when the new model wins enough evaluation games.</p><p>Over many iterations, the system improves rapidly and surpasses earlier AlphaGo versions.</p><h1 id=novelty>Novelty<a hidden class=anchor aria-hidden=true href=#novelty>#</a></h1><ol><li>It’s trained solely by self-play RL, starting from ran­dom play, without any supervision or use of human data.</li><li>It uses only the black and white stones from the board as <strong>input</strong> features.</li><li>It uses a <strong>single</strong> <strong>neural network</strong>, rather than separate policy and value networks.</li><li>It uses a <strong>simpler tree search</strong> that relies upon this <strong>single neural network</strong> to evaluate positions and sample moves.</li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>