<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Rethinking Attention with Performers | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: Performers"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/rethinking_attention_with_performers/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/rethinking_attention_with_performers/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/rethinking_attention_with_performers/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Rethinking Attention with Performers"><meta property="og:description" content="Paper-reading notes: Performers"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-12-03T15:09:23+00:00"><meta property="article:modified_time" content="2025-12-03T15:09:23+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/selfile.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/selfile.png"><meta name=twitter:title content="Rethinking Attention with Performers"><meta name=twitter:description content="Paper-reading notes: Performers"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Rethinking Attention with Performers","item":"https://my-blog-alpha-vert.vercel.app/notes/rethinking_attention_with_performers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Rethinking Attention with Performers","name":"Rethinking Attention with Performers","description":"Paper-reading notes: Performers","keywords":[],"articleBody":"1. Introduction Transformers are widely used in many areas, but their softmax attention requires quadratic time and memory, making them expensive for long sequences. Because of this limitation, many prior works propose “efficient attention” by adding structural assumptions.\nCommon ideas include:\nlimiting attention to local neighbors, enforcing sparsity, using pooling or clustering (e.g., k-means), hashing similar tokens together, sliding windows, or using low-rank approximations of the attention matrix. These methods reduce cost but depend heavily on hand-designed priors such as sparsity patterns or low-rank structure. They often lack theoretical guarantees, and some approaches still fail to capture long-range interactions or introduce approximation bias.\nPerformers provide a new solution: they approximate full softmax attention accurately while using only linear time and memory. The key technique, FAVOR+ (Fast Attention Via positive Orthogonal Random features), approximates softmax and other kernels using random features with strong theoretical guarantees (unbiased or nearly-unbiased estimation and low variance).\nBecause FAVOR+ works for many kernelizable attention mechanisms, Performers make it feasible to compare different attention kernels on large-scale tasks, something that was previously too expensive with traditional quadratic attention.\nFinally, experiments show that Performers achieve competitive performance on tasks from pixel prediction to text and protein modeling, while remaining fully compatible with standard Transformer architectures.\n2. Method Performers reformulate softmax attention as a kernel function and then approximating this kernel using random feature maps so that attention can be computed without constructing the quadratic matrix.\nThe key observation is that the softmax kernel $\\exp(q^\\top k)$ can be written as an expectation over random features. Performer introduces Positive Random Features (PRF) that approximate $\\exp(q^\\top k)$ using mappings of the form\n$$ \\phi(x) = h(x) (\\exp(\\omega_1^\\top x),\\dots, \\exp(\\omega_m^\\top x)), $$\nwhere $h(x)$ is a stabilizing factor and $\\omega_i$ are sampled from an isotropic Gaussian distribution. Unlike classical Fourier features (cos), PRFs always produce positive, non-oscillatory values, yielding an unbiased approximation to the softmax kernel that remains stable even when dot products are small or negative. This allows the softmax kernel to be approximated by\n$$ \\exp(q^\\top k) \\approx \\phi(q)^\\top \\phi(k), $$\nso the attention can be computed as\n$$ \\widetilde{\\text{Att}}(Q,K,V) = D^{-1}\\big(Q’((K’)^\\top V)\\big), $$\nwhich requires only O(Lrd) operations and never forms an $L \\times L$ matrix.\nTo further reduce variance, Performer augments PRFs with Orthogonal Random Features (ORF). Here, the random vectors $\\omega_1,\\dots,\\omega_m$ are orthogonalized (e.g., by Gram–Schmidt), significantly tightening concentration bounds and producing exponentially smaller variance compared to independently sampled features.\nThe combined PRF + ORF mechanism forms FAVOR+ (Fast Attention Via Orthogonal Random features), achieving high accuracy with relatively few random features $r \\ll L$. This enables Performers to match or surpass the accuracy of softmax attention while using linear time and memory.\n3. Novelty Performers introduce FAVOR+, a new linear-time attention mechanism that accurately approximates softmax using positive orthogonal random features.\nThis replaces the quadratic $(L^2)$ attention matrix with a linear $O(Lrd)$ computation while preserving accuracy.\nThe method is unbiased, low-variance, and fully compatible with standard Transformers, enabling fast and memory-efficient training on long sequences.\n","wordCount":"499","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/selfile.png","datePublished":"2025-12-03T15:09:23Z","dateModified":"2025-12-03T15:09:23Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/rethinking_attention_with_performers/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Rethinking Attention with Performers</h1><div class=post-description>Paper-reading notes: Performers</div><div class=post-meta><span title='2025-12-03 15:09:23 +0000 +0000'>December 3, 2025</span>&nbsp;·&nbsp;<span>499 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-introduction aria-label="1. Introduction">1. Introduction</a></li><li><a href=#2-method aria-label="2. Method">2. Method</a></li><li><a href=#3-novelty aria-label="3. Novelty">3. Novelty</a></li></ul></div></details></div><div class=post-content><h1 id=1-introduction>1. Introduction<a hidden class=anchor aria-hidden=true href=#1-introduction>#</a></h1><p>Transformers are widely used in many areas, but their <strong>softmax attention</strong> requires <strong>quadratic time and memory</strong>, making them expensive for long sequences. Because of this limitation, many prior works propose “efficient attention” by adding <strong>structural assumptions</strong>.</p><p>Common ideas include:</p><ul><li>limiting attention to local neighbors,</li><li>enforcing sparsity,</li><li>using pooling or clustering (e.g., k-means),</li><li>hashing similar tokens together,</li><li>sliding windows,</li><li>or using low-rank approximations of the attention matrix.</li></ul><p>These methods reduce cost but depend heavily on <strong>hand-designed priors</strong> such as sparsity patterns or low-rank structure. They often lack theoretical guarantees, and some approaches still fail to capture long-range interactions or introduce approximation bias.</p><p><strong>Performers</strong> provide a new solution: they approximate <strong>full softmax attention</strong> accurately while using only <strong>linear</strong> time and memory. The key technique, <strong>FAVOR+</strong> (Fast Attention Via positive Orthogonal Random features), approximates softmax and other kernels using random features with strong theoretical guarantees (unbiased or nearly-unbiased estimation and low variance).</p><p>Because FAVOR+ works for many kernelizable attention mechanisms, Performers make it feasible to compare different attention kernels on large-scale tasks, something that was previously too expensive with traditional quadratic attention.</p><p>Finally, experiments show that Performers achieve competitive performance on tasks from pixel prediction to text and protein modeling, while remaining fully compatible with standard Transformer architectures.</p><h1 id=2-method>2. Method<a hidden class=anchor aria-hidden=true href=#2-method>#</a></h1><p>Performers reformulate softmax attention as a <strong>kernel function</strong> and then approximating this kernel using <strong>random feature maps</strong> so that attention can be computed without constructing the quadratic matrix.</p><p>The key observation is that the softmax kernel $\exp(q^\top k)$ can be written as an expectation over random features. Performer introduces <strong>Positive Random Features (PRF)</strong> that approximate $\exp(q^\top k)$ using mappings of the form</p><p>$$
\phi(x) = h(x) (\exp(\omega_1^\top x),\dots, \exp(\omega_m^\top x)),
$$</p><p>where $h(x)$ is a stabilizing factor and $\omega_i$ are sampled from an isotropic Gaussian distribution. Unlike classical Fourier features (cos), PRFs always produce <strong>positive</strong>, <strong>non-oscillatory</strong> values, yielding an <strong>unbiased</strong> approximation to the softmax kernel that remains stable even when dot products are small or negative. This allows the softmax kernel to be approximated by</p><p>$$
\exp(q^\top k) \approx \phi(q)^\top \phi(k),
$$</p><p>so the attention can be computed as</p><p>$$
\widetilde{\text{Att}}(Q,K,V) = D^{-1}\big(Q&rsquo;((K&rsquo;)^\top V)\big),
$$</p><p>which requires only <strong>O(Lrd)</strong> operations and never forms an $L \times L$ matrix.</p><p>To further reduce variance, Performer augments PRFs with <strong>Orthogonal Random Features (ORF)</strong>. Here, the random vectors $\omega_1,\dots,\omega_m$ are orthogonalized (e.g., by Gram–Schmidt), significantly tightening concentration bounds and producing exponentially smaller variance compared to independently sampled features.</p><p>The combined <strong>PRF + ORF</strong> mechanism forms FAVOR+ (Fast Attention Via Orthogonal Random features), achieving high accuracy with relatively few random features $r \ll L$. This enables Performers to match or surpass the accuracy of softmax attention while using <strong>linear time and memory</strong>.</p><h1 id=3-novelty>3. Novelty<a hidden class=anchor aria-hidden=true href=#3-novelty>#</a></h1><p>Performers introduce <strong>FAVOR+</strong>, a new linear-time attention mechanism that accurately approximates softmax using <strong>positive orthogonal random features</strong>.</p><p>This replaces the quadratic $(L^2)$ attention matrix with a linear $O(Lrd)$ computation while preserving accuracy.</p><p>The method is <strong>unbiased, low-variance, and fully compatible</strong> with standard Transformers, enabling fast and memory-efficient training on long sequences.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>