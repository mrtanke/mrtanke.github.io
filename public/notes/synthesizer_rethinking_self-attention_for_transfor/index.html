<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Synthesizer: Rethinking Self-Attention for Transformer Models | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: Synthesizer"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/synthesizer_rethinking_self-attention_for_transfor/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/synthesizer_rethinking_self-attention_for_transfor/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/synthesizer_rethinking_self-attention_for_transfor/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Synthesizer: Rethinking Self-Attention for Transformer Models"><meta property="og:description" content="Paper-reading notes: Synthesizer"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-12-16T08:40:53+00:00"><meta property="article:modified_time" content="2025-12-16T08:40:53+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/synthesizer_rethinking_self-attention_for_transfor/image.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/synthesizer_rethinking_self-attention_for_transfor/image.png"><meta name=twitter:title content="Synthesizer: Rethinking Self-Attention for Transformer Models"><meta name=twitter:description content="Paper-reading notes: Synthesizer"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Synthesizer: Rethinking Self-Attention for Transformer Models","item":"https://my-blog-alpha-vert.vercel.app/notes/synthesizer_rethinking_self-attention_for_transfor/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Synthesizer: Rethinking Self-Attention for Transformer Models","name":"Synthesizer: Rethinking Self-Attention for Transformer Models","description":"Paper-reading notes: Synthesizer","keywords":[],"articleBody":"Problem Self-attention in Transformers is built on query–key dot products, which are widely believed to be essential for modeling token interactions and long-range dependencies. However, it is unclear whether this content-based, pairwise similarity computation is truly necessary for good performance.\nThe paper questions three common assumptions:\nThat attention weights must be computed from token–token interactions (Q·K) That attention must be instance-specific rather than globally learned That dot-product attention is the key reason for Transformer success In short, the problem is to understand how important dot-product self-attention really is, and whether simpler or alternative mechanisms can replace it without hurting performance .\nMethod The paper proposes Synthetic Attention, which removes query–key dot products entirely and instead directly learns (or generates) the attention/alignment matrix.\nCore idea:\nInstead of computing attention weights via token similarity, synthesize them using parameterized functions.\nThe proposed SYNTHESIZER replaces standard self-attention with:\nDense Synthesizer:\nEach token independently predicts its attention weights using an MLP (no token–token interaction).\nRandom Synthesizer:\nAttention weights are global, randomly initialized matrices (trainable or fixed), shared across all inputs.\nFactorized Synthesizers:\nLow-rank versions to reduce parameters and improve efficiency.\nMixture Models:\nCombine synthetic attention with dot-product attention, showing they are complementary.\nThe model keeps the rest of the Transformer unchanged (values, feed-forward layers, multi-head structure) and is evaluated across machine translation, language modeling, text generation, and GLUE/SuperGLUE benchmarks.\nResults show that synthetic attention alone is often competitive, and combining it with dot-product attention can outperform standard Transformers .\n","wordCount":"244","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/synthesizer_rethinking_self-attention_for_transfor/image.png","datePublished":"2025-12-16T08:40:53Z","dateModified":"2025-12-16T08:40:53Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/synthesizer_rethinking_self-attention_for_transfor/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Synthesizer: Rethinking Self-Attention for Transformer Models</h1><div class=post-description>Paper-reading notes: Synthesizer</div><div class=post-meta><span title='2025-12-16 08:40:53 +0000 +0000'>December 16, 2025</span>&nbsp;·&nbsp;<span>244 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#problem aria-label=Problem>Problem</a></li><li><a href=#method aria-label=Method>Method</a></li></ul></div></details></div><div class=post-content><h1 id=problem>Problem<a hidden class=anchor aria-hidden=true href=#problem>#</a></h1><p><strong>Self-attention in Transformers is built on query–key dot products</strong>, which are widely believed to be essential for modeling token interactions and long-range dependencies. However, it is unclear <strong>whether this content-based, pairwise similarity computation is truly necessary</strong> for good performance.</p><p>The paper questions three common assumptions:</p><ul><li>That attention weights must be computed from <strong>token–token interactions (Q·K)</strong></li><li>That attention must be <strong>instance-specific</strong> rather than globally learned</li><li>That dot-product attention is the key reason for Transformer success</li></ul><p>In short, the problem is to understand <strong>how important dot-product self-attention really is</strong>, and whether simpler or alternative mechanisms can replace it without hurting performance .</p><h1 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h1><p>The paper proposes <strong>Synthetic Attention</strong>, which <strong>removes query–key dot products entirely</strong> and instead <strong>directly learns (or generates) the attention/alignment matrix</strong>.</p><p>Core idea:</p><aside><p>Instead of computing attention weights via token similarity, synthesize them using parameterized functions.</p></aside><p>The proposed <strong>SYNTHESIZER</strong> replaces standard self-attention with:</p><ul><li><p><strong>Dense Synthesizer</strong>:</p><p>Each token independently predicts its attention weights using an MLP (no token–token interaction).</p></li><li><p><strong>Random Synthesizer</strong>:</p><p>Attention weights are global, randomly initialized matrices (trainable or fixed), shared across all inputs.</p></li></ul><p><img alt=image.png loading=lazy src=/notes/synthesizer_rethinking_self-attention_for_transfor/image.png></p><ul><li><p><strong>Factorized Synthesizers</strong>:</p><p>Low-rank versions to reduce parameters and improve efficiency.</p></li><li><p><strong>Mixture Models</strong>:</p><p>Combine synthetic attention with dot-product attention, showing they are complementary.</p></li></ul><p>The model keeps the rest of the Transformer unchanged (values, feed-forward layers, multi-head structure) and is evaluated across machine translation, language modeling, text generation, and GLUE/SuperGLUE benchmarks.</p><p>Results show that <strong>synthetic attention alone is often competitive</strong>, and <strong>combining it with dot-product attention can outperform standard Transformers</strong> .</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>