<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: AlphaZero"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"><meta property="og:description" content="Paper-reading notes: AlphaZero"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-11-24T08:39:45+00:00"><meta property="article:modified_time" content="2025-11-24T08:39:45+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image_1.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image_2.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image_3.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image.png"><meta name=twitter:title content="Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"><meta name=twitter:description content="Paper-reading notes: AlphaZero"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm","item":"https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm","name":"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm","description":"Paper-reading notes: AlphaZero","keywords":[],"articleBody":"Introduction AlphaGo AlphaGo combines CNN with MCTS to play Go at a superhuman level.\nIt first trains two types of networks (policy network \u0026 value network \u0026 rollout policy), using the current game board combined with several handcrafted Go features as input:\nThen integrates the networks into MCTS to enhance the basic tree search.\nAlphaGo Zero AlphaGo Zero combines a deep residual network ResNet with MCTS. Unlike AlphaGo, where MCTS is separate from training, AlphaGo Zero integrates MCTS directly inside the training loop.\nIt trains a single unified network (two heads, produce policy and value outputs) directly from raw board positions, without using any human games or handcrafted Go features.\nTrain the model to minimize the gap between p and π | v and z.\nHow to choose the next move? → Policy as prior probability in MCTS. The actual move is always selected based on the MCTS-improved policy π, produced after many MCTS simulations.\nMethod AlphaZero generalizes AlphaGo Zero to Go, Chess, and Shogi using one unified algorithm.\nIt removes all handcrafted knowledge and relies only on the basic game rules. A single neural network $f_\\theta(s)$ outputs both the policy p and value v from the raw board. Training MCTS uses this network to produce an improved policy $\\pi$, which selects moves and supervises training. Self-play generates $(s, \\pi, z)$ data, and training minimizes value error $(z - v)^2$ and policy cross-entropy $-\\pi^\\top \\log p$.\nThe same network is updated continuously, with no best-player selection stage. Hyperparameters are reused across games, and the board is encoded only by simple rule-based planes with no extra features.\nNovelty Compared with AlphaGo Zero:\nAlphaZero works for multiple games, not only Go. Predicts expected outcome (handles draws), not just win/loss. Removes all symmetry augmentation and board transformations. Uses one continuously updated network, no best-player selection. Reuses one set of hyperparameters across all games. Best-player selection → In AlphaGo Zero:\nTrain a new network for one iteration. Play it against the previous best network for several round games. If it wins more than 55%, it becomes the new best network. Otherwise, it is discarded. Only the best network is used for future self-play training. ","wordCount":"360","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image.png","datePublished":"2025-11-24T08:39:45Z","dateModified":"2025-11-24T08:39:45Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</h1><div class=post-description>Paper-reading notes: AlphaZero</div><div class=post-meta><span title='2025-11-24 08:39:45 +0000 +0000'>November 24, 2025</span>&nbsp;·&nbsp;<span>360 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a><ul><li><a href=#alphago aria-label=AlphaGo>AlphaGo</a></li><li><a href=#alphago-zero aria-label="AlphaGo Zero">AlphaGo Zero</a></li></ul></li><li><a href=#method aria-label=Method>Method</a><ul><li><a href=#training aria-label=Training>Training</a></li></ul></li><li><a href=#novelty aria-label=Novelty>Novelty</a></li></ul></div></details></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><h2 id=alphago>AlphaGo<a hidden class=anchor aria-hidden=true href=#alphago>#</a></h2><p>AlphaGo combines <strong>CNN</strong> with <strong>MCTS</strong> to play Go at a superhuman level.</p><p>It first trains two types of networks (<strong>policy network</strong> & <strong>value network & rollout policy</strong>), using the current game board combined with several handcrafted Go features as input:</p><p><img alt=image.png loading=lazy src=/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image.png></p><p>Then integrates the networks into MCTS to enhance the basic tree search.</p><p><img alt=image.png loading=lazy src=/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image_1.png></p><h2 id=alphago-zero>AlphaGo Zero<a hidden class=anchor aria-hidden=true href=#alphago-zero>#</a></h2><p>AlphaGo Zero combines a <strong>deep residual network ResNet</strong> with <strong>MCTS</strong>. Unlike AlphaGo, where MCTS is separate from training, AlphaGo Zero integrates MCTS directly inside the training loop.</p><p>It trains a <strong>single unified network</strong> (two heads, produce <strong>policy</strong> and <strong>value</strong> outputs) directly from <strong>raw board positions</strong>, without using any human games or handcrafted Go features.</p><p>Train the model to minimize the gap between <strong>p and π | v and z.</strong></p><p><img alt=image.png loading=lazy src=/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image_2.png></p><p>How to choose the next move? → Policy as prior probability in MCTS. The actual move is always selected based on the MCTS-improved policy <strong>π</strong>, produced after many <strong>MCTS simulations</strong>.</p><p><img alt=image.png loading=lazy src=/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image_3.png></p><h1 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h1><p>AlphaZero <strong>generalizes</strong> AlphaGo Zero to Go, Chess, and Shogi using <strong>one unified algorithm</strong>.</p><ul><li>It removes all handcrafted knowledge and relies only on the basic game rules.</li><li>A single neural network $f_\theta(s)$ outputs both the policy p and value v from the raw board.</li></ul><h2 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h2><p>MCTS uses this network to produce an improved policy $\pi$, which selects moves and supervises training. Self-play generates $(s, \pi, z)$ data, and training minimizes value error $(z - v)^2$ and policy cross-entropy $-\pi^\top \log p$.</p><p>The same network is updated continuously, with no best-player selection stage. Hyperparameters are reused across games, and the board is encoded only by simple rule-based planes with no extra features.</p><p><img alt=image.png loading=lazy src=/notes/mastering_chess_and_shogi_by_self-play_with_a_gene/image_3.png></p><h1 id=novelty>Novelty<a hidden class=anchor aria-hidden=true href=#novelty>#</a></h1><p>Compared with <strong>AlphaGo Zero</strong>:</p><ul><li>AlphaZero works for <strong>multiple games</strong>, not only Go.</li><li>Predicts <strong>expected outcome</strong> (handles draws), not just win/loss.</li><li>Removes all <strong>symmetry augmentation</strong> and board transformations.</li><li>Uses <strong>one continuously updated network</strong>, no best-player selection.</li><li>Reuses <strong>one set of hyperparameters</strong> across all games.</li></ul><aside><p><strong>Best-player selection</strong> → In AlphaGo Zero:</p><ol><li>Train a new network for one iteration.</li><li>Play it against the <strong>previous best network</strong> for several round games.</li><li>If it wins <strong>more than 55%</strong>, it becomes the new best network.<ol><li>Otherwise, it is <strong>discarded</strong>.</li></ol></li><li>Only the <strong>best network</strong> is used for future self-play training.</li></ol></aside></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>