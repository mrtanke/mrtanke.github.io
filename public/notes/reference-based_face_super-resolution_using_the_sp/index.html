<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reference-Based Face Super-Resolution Using the Spatial Transformer | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: Reference-Based Face Super-Resolution Using the Spatial Transformer"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/reference-based_face_super-resolution_using_the_sp/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/reference-based_face_super-resolution_using_the_sp/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/reference-based_face_super-resolution_using_the_sp/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Reference-Based Face Super-Resolution Using the Spatial Transformer"><meta property="og:description" content="Paper-reading notes: Reference-Based Face Super-Resolution Using the Spatial Transformer"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-11-07T09:32:10+00:00"><meta property="article:modified_time" content="2025-11-07T09:32:10+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/reference-based_face_super-resolution_using_the_sp/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/reference-based_face_super-resolution_using_the_sp/image_1.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/reference-based_face_super-resolution_using_the_sp/image.png"><meta name=twitter:title content="Reference-Based Face Super-Resolution Using the Spatial Transformer"><meta name=twitter:description content="Paper-reading notes: Reference-Based Face Super-Resolution Using the Spatial Transformer"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Reference-Based Face Super-Resolution Using the Spatial Transformer","item":"https://my-blog-alpha-vert.vercel.app/notes/reference-based_face_super-resolution_using_the_sp/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reference-Based Face Super-Resolution Using the Spatial Transformer","name":"Reference-Based Face Super-Resolution Using the Spatial Transformer","description":"Paper-reading notes: Reference-Based Face Super-Resolution Using the Spatial Transformer","keywords":[],"articleBody":"Source code: https://github.com/varun-jois/FSRST\nIntroduction Face super-resolution aims to reconstruct high-resolution (HR) facial images from low-resolution (LR) inputs, enhancing fine details. This task is challenging because it is ill-posed: many possible HR outputs can correspond to the same LR image.\nTo reduce ambiguity, Reference-Based Super-Resolution (RefSR) introduces external HR reference images that share similar content (e.g., same person’s other photos). The model can then use these reference textures and shapes to guide reconstruction.\nHowever, RefSR introduces two main challenges:\nAlignment problem – matching facial structures between LR input and HR reference images. Information aggregation problem – determining how much and which parts of each reference to use. Traditional alignment methods (like deformable convolutions) are powerful but unstable and hard to train. To overcome this, the authors propose FSRST, which uses a Spatial Transformer Network (STN) for stable alignment and a distance-based weighted aggregation for effective information fusion.\nMethod The proposed Face Super-Resolution using Spatial Transformer (FSRST) is an end-to-end model with four components:\nFeature Extractor:\nExtracts features from both the LR input and HR references using residual blocks. Reference images are converted to grayscale and reshaped (space-to-depth) to match the LR resolution.\nSpatial Transformer Alignment (STA):\nReplaces unstable deformable convolutions with a Spatial Transformer module. It predicts an affine transformation that aligns each reference’s features with those of the LR image. This alignment is differentiable and stable, ensuring good correspondence between LR and HR feature spaces.\nDistance-Based Weighted Aggregation (DWA):\nAfter alignment, the model computes the L2-distance between each aligned reference feature and the LR feature. A softmax weighting gives higher importance to more similar references, while irrelevant ones are ignored. This allows the model to dynamically use helpful references or fall back to single-image SR when references are poor.\nOutput Constructor:\nCombines aggregated and LR features and passes them through multiple residual blocks and sub-pixel convolution for upsampling. The model predicts a residual image, added to a bicubic-upsampled LR image to produce the final HR output.\nConclusion The FSRST model introduces a stable and efficient alternative to deformable alignment for reference-based face super-resolution.\nIts Spatial Transformer alignment provides consistent and accurate correspondence, while the distance-based aggregation flexibly handles multiple references.\nExperiments on DFD, CelebAMask-HQ, and VoxCeleb2 datasets show that FSRST outperforms previous methods like TTSR, C2-Matching, MRefSR, and HIME — achieving higher PSNR/SSIM with fewer parameters.\nAlthough the STN-based alignment is not fully convolutional (fixed input size), the method is lightweight, effective, and adaptable for real-time applications such as video conferencing.\nFuture work aims to extend this model to video super-resolution and make the alignment module fully convolutional.\n","wordCount":"428","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/reference-based_face_super-resolution_using_the_sp/image.png","datePublished":"2025-11-07T09:32:10Z","dateModified":"2025-11-07T09:32:10Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/reference-based_face_super-resolution_using_the_sp/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Reference-Based Face Super-Resolution Using the Spatial Transformer</h1><div class=post-description>Paper-reading notes: Reference-Based Face Super-Resolution Using the Spatial Transformer</div><div class=post-meta><span title='2025-11-07 09:32:10 +0000 +0000'>November 7, 2025</span>&nbsp;·&nbsp;<span>428 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#method aria-label=Method>Method</a></li></ul></ul><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><p>Source code: <a href=https://github.com/varun-jois/FSRST>https://github.com/varun-jois/FSRST</a></p><h3 id=introduction><strong>Introduction</strong><a hidden class=anchor aria-hidden=true href=#introduction>#</a></h3><p>Face super-resolution aims to reconstruct high-resolution (HR) facial images from low-resolution (LR) inputs, enhancing <strong>fine details</strong>. This task is challenging because it is <strong>ill-posed</strong>: many possible HR outputs can correspond to the same LR image.</p><p>To reduce ambiguity, <strong>Reference-Based Super-Resolution (RefSR)</strong> introduces external HR reference images that share similar content (e.g., same person’s other photos). The model can then use these reference textures and shapes to guide reconstruction.</p><p>However, RefSR introduces two main challenges:</p><ol><li><strong>Alignment problem</strong> – matching facial structures between LR input and HR reference images.</li><li><strong>Information aggregation problem</strong> – determining how much and which parts of each reference to use.</li></ol><p>Traditional alignment methods (like deformable convolutions) are powerful but unstable and hard to train. To overcome this, the authors propose <strong>FSRST</strong>, which uses a <strong>Spatial Transformer Network (STN)</strong> for stable alignment and a <strong>distance-based weighted aggregation</strong> for effective information fusion.</p><h3 id=method><strong>Method</strong><a hidden class=anchor aria-hidden=true href=#method>#</a></h3><p>The proposed <strong>Face Super-Resolution using Spatial Transformer (FSRST)</strong> is an end-to-end model with four components:</p><p><img alt=image.png loading=lazy src=/notes/reference-based_face_super-resolution_using_the_sp/image.png></p><ol><li><p><strong>Feature Extractor:</strong></p><p>Extracts features from both the LR input and HR references using residual blocks. Reference images are converted to grayscale and reshaped (space-to-depth) to match the LR resolution.</p></li><li><p><strong>Spatial Transformer Alignment (STA):</strong></p><p>Replaces unstable deformable convolutions with a <strong>Spatial Transformer module</strong>. It predicts an affine transformation that aligns each reference’s features with those of the LR image. This alignment is differentiable and stable, ensuring good correspondence between LR and HR feature spaces.</p><p><img alt=image.png loading=lazy src=/notes/reference-based_face_super-resolution_using_the_sp/image_1.png></p></li><li><p><strong>Distance-Based Weighted Aggregation (DWA):</strong></p><p>After alignment, the model computes the L2-distance between each aligned reference feature and the LR feature. A <strong>softmax weighting</strong> gives higher importance to more similar references, while irrelevant ones are ignored. This allows the model to dynamically use helpful references or fall back to single-image SR when references are poor.</p></li><li><p><strong>Output Constructor:</strong></p><p>Combines aggregated and LR features and passes them through multiple residual blocks and sub-pixel convolution for upsampling. The model predicts a <strong>residual image</strong>, added to a bicubic-upsampled LR image to produce the final HR output.</p></li></ol><h1 id=conclusion><strong>Conclusion</strong><a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>The FSRST model introduces a <strong>stable and efficient alternative</strong> to deformable alignment for reference-based face super-resolution.</p><p>Its <strong>Spatial Transformer alignment</strong> provides consistent and accurate correspondence, while the <strong>distance-based aggregation</strong> flexibly handles multiple references.</p><p>Experiments on <strong>DFD</strong>, <strong>CelebAMask-HQ</strong>, and <strong>VoxCeleb2</strong> datasets show that FSRST outperforms previous methods like TTSR, C2-Matching, MRefSR, and HIME — achieving higher PSNR/SSIM with fewer parameters.</p><p>Although the STN-based alignment is not fully convolutional (fixed input size), the method is lightweight, effective, and adaptable for real-time applications such as video conferencing.</p><p>Future work aims to extend this model to <strong>video super-resolution</strong> and make the alignment module fully convolutional.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>