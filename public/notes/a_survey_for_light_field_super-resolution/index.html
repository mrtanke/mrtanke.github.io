<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A survey for light field super-resolution | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: A survey for light field super-resolution"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/"><meta property="og:site_name" content="Home"><meta property="og:title" content="A survey for light field super-resolution"><meta property="og:description" content="Paper-reading notes: A survey for light field super-resolution"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-11-14T07:41:11+00:00"><meta property="article:modified_time" content="2025-11-14T07:41:11+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/38bd0f5f-ebe8-44b8-b8bf-8bd68a63fbd8.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/image_1.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/image_10.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/image_11.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/image_2.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/38bd0f5f-ebe8-44b8-b8bf-8bd68a63fbd8.png"><meta name=twitter:title content="A survey for light field super-resolution"><meta name=twitter:description content="Paper-reading notes: A survey for light field super-resolution"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"A survey for light field super-resolution","item":"https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A survey for light field super-resolution","name":"A survey for light field super-resolution","description":"Paper-reading notes: A survey for light field super-resolution","keywords":[],"articleBody":" Three methods of Light Field Super-Resolution (LFSR):\nLight field spatial super-resolution (LFSSR) Light field angular super-resolution (LFASR) Light field spatial and angular super-resolution (LFSASR) Experiments for each method.\n1. Light field spatial super-resolution (LFSSR) Improve spatial resolution of each sub-aperture image (SAI).\n1.1. CNN-based method Use convolutions to learn spatial / angular correlations and fuse high-frequency details.\nResidual CNNs: Learn directional features and fuse sub-pixel details (e.g., resLF). Feature alignment: Optical-flow-based alignment, deformable conv alignment. 4D / separable CNNs: Use 4D conv or spatial–angular separable conv to jointly extract features. View fusion models: “All-to-One”, multi-view complementary information fusion. Attention-based CNNs: Channel/view attention, angular deformable alignment. resLF\nseperable Conv model\n“All-to-One” model\nview+channel attention model\n1.2. Transformer-based method Use attention to model long-range spatial-angular dependency.\nSpatial–angular transformer: Self-attention along EPI lines to capture parallax geometry. Volume and cross-view transformers: Model correlations across many viewpoints. Multi-scale angular transformer: Robust to disparity variations. EPI attention\nvolume transformer and cross-view transformer\nLF-DET\n2. Light field angular super-resolution (LFASR) Increase number of viewpoints (more SAIs) while preserving geometry.\n2.1. Depth-dependent method Estimate depth/disparity → warp existing views → blend new views.\nOptical-flow and superpixel-based warping Layered depth representations EPI-based geometry modeling Depth-guided warping with occlusion reasoning 2.2. Depth-independent method Avoid explicit depth; rely on signal priors or learning-based angular patterns.\nCNN-based angular detail restoration (on EPIs) Angular attention models to reconstruct views 3. Light field spatial-angular super-resolution (LFSASR) Simultaneously increase both spatial and angular resolution → full 4D reconstruction.\n3.1. Deep learning-based method Jointly model 4D light field structure (geometry + appearance).\n4D CNN encoder–decoder Pseudo-4D convolution combining EPI + spatial-angular blocks Disentangled models: separate spatial and angular subspaces EPI-based networks (CNN + LSTM) preserving geometry Self-supervised or domain-generalized models for wild light fields 3D encoder\nEPI-based networks\nExperiments LFSSR CNN-based: Convolutions, 4D/sep-conv, optical-flow alignment, attention fusion. Transformer-based: Global spatial–angular modeling, EPI attention, multi-scale. LFASR Depth-dependent: Estimate depth → warp → blend new views. Depth-independent: Fourier/shearlet priors, CNN restoration on EPI, angular attention. LFSASR Deep learning-based: 4D CNNs, disentangled spatial–angular modeling, EPI networks, high-order residual networks. ","wordCount":"341","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/38bd0f5f-ebe8-44b8-b8bf-8bd68a63fbd8.png","datePublished":"2025-11-14T07:41:11Z","dateModified":"2025-11-14T07:41:11Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/a_survey_for_light_field_super-resolution/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">A survey for light field super-resolution</h1><div class=post-description>Paper-reading notes: A survey for light field super-resolution</div><div class=post-meta><span title='2025-11-14 07:41:11 +0000 +0000'>November 14, 2025</span>&nbsp;·&nbsp;<span>341 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-light-field-spatial-super-resolution-lfssr aria-label="1. Light field spatial super-resolution (LFSSR)">1. Light field spatial super-resolution (LFSSR)</a><ul><li><a href=#11-cnn-based-method aria-label="1.1. CNN-based method">1.1. CNN-based method</a></li><li><a href=#12-transformer-based-method aria-label="1.2. Transformer-based method">1.2. Transformer-based method</a></li></ul></li><li><a href=#2-light-field-angular-super-resolution-lfasr aria-label="2. Light field angular super-resolution (LFASR)">2. Light field angular super-resolution (LFASR)</a><ul><li><a href=#21-depth-dependent-method aria-label="2.1. Depth-dependent method">2.1. Depth-dependent method</a></li><li><a href=#22-depth-independent-method aria-label="2.2. Depth-independent method">2.2. Depth-independent method</a></li></ul></li><li><a href=#3-light-field-spatial-angular-super-resolution-lfsasr aria-label="3. Light field spatial-angular super-resolution (LFSASR)">3. Light field spatial-angular super-resolution (LFSASR)</a><ul><li><a href=#31-deep-learning-based-method aria-label="3.1. Deep learning-based method">3.1. Deep learning-based method</a></li></ul></li><li><a href=#experiments aria-label=Experiments>Experiments</a><ul><ul><li><a href=#lfssr aria-label=LFSSR>LFSSR</a></li><li><a href=#lfasr aria-label=LFASR>LFASR</a></li><li><a href=#lfsasr aria-label=LFSASR>LFSASR</a></li></ul></ul></li></ul></div></details></div><div class=post-content><aside><p><strong>Three methods of Light Field Super-Resolution (LFSR):</strong></p><ol><li>Light field <strong>spatial</strong> super-resolution (LFSSR)</li><li>Light field <strong>angular</strong> super-resolution (LFASR)</li><li><strong>Light field spatial and angular super-resolution (LFSASR)</strong></li></ol><p><strong>Experiments</strong> for each method.</p></aside><h1 id=1-light-field-spatial-super-resolution-lfssr>1. Light field spatial super-resolution (LFSSR)<a hidden class=anchor aria-hidden=true href=#1-light-field-spatial-super-resolution-lfssr>#</a></h1><p>Improve <strong>spatial resolution</strong> of each sub-aperture image (<strong>SAI</strong>).</p><h2 id=11-cnn-based-method>1.1. CNN-based method<a hidden class=anchor aria-hidden=true href=#11-cnn-based-method>#</a></h2><p>Use convolutions to learn spatial / angular correlations and fuse high-frequency details.</p><ul><li><strong>Residual CNNs</strong>: Learn directional features and fuse sub-pixel details (e.g., resLF).</li><li><strong>Feature alignment</strong>: Optical-flow-based alignment, deformable conv alignment.</li><li><strong>4D / separable CNNs</strong>: Use 4D conv or spatial–angular separable conv to jointly extract features.</li><li><strong>View fusion models</strong>: “All-to-One”, multi-view complementary information fusion.</li><li><strong>Attention-based CNNs</strong>: Channel/view attention, angular deformable alignment.</li></ul><p><img alt=resLF loading=lazy src=/notes/a_survey_for_light_field_super-resolution/image.png></p><p>resLF</p><p><img alt="seperable Conv model" loading=lazy src=/notes/a_survey_for_light_field_super-resolution/image_1.png></p><p>seperable Conv model</p><p><img alt="“All-to-One” model" loading=lazy src=/notes/a_survey_for_light_field_super-resolution/image_2.png></p><p>“All-to-One” model</p><p><img alt="view+channel attention model" loading=lazy src=/notes/a_survey_for_light_field_super-resolution/image_3.png></p><p>view+channel attention model</p><h2 id=12-transformer-based-method>1.2. Transformer-based method<a hidden class=anchor aria-hidden=true href=#12-transformer-based-method>#</a></h2><p>Use attention to model long-range spatial-angular dependency.</p><ul><li><strong>Spatial–angular transformer</strong>: Self-attention along EPI lines to capture parallax geometry.</li><li><strong>Volume and cross-view transformers</strong>: Model correlations across many viewpoints.</li><li><strong>Multi-scale angular transformer</strong>: Robust to disparity variations.</li></ul><p><img alt="EPI attention" loading=lazy src=/notes/a_survey_for_light_field_super-resolution/image_4.png></p><p>EPI attention</p><p><img alt="volume transformer and cross-view transformer" loading=lazy src=/notes/a_survey_for_light_field_super-resolution/image_5.png></p><p>volume transformer and cross-view transformer</p><p><img alt="<strong>LF-DET</strong>" loading=lazy src=/notes/a_survey_for_light_field_super-resolution/image_6.png></p><p><strong>LF-DET</strong></p><h1 id=2-light-field-angular-super-resolution-lfasr>2. Light field angular super-resolution (LFASR)<a hidden class=anchor aria-hidden=true href=#2-light-field-angular-super-resolution-lfasr>#</a></h1><p>Increase <strong>number of viewpoints</strong> (more SAIs) while preserving geometry.</p><h2 id=21-depth-dependent-method>2.1. Depth-dependent method<a hidden class=anchor aria-hidden=true href=#21-depth-dependent-method>#</a></h2><p>Estimate depth/disparity → warp existing views → blend new views.</p><ul><li>Optical-flow and superpixel-based warping</li><li>Layered depth representations</li><li>EPI-based geometry modeling</li><li>Depth-guided warping with occlusion reasoning</li></ul><p><img alt=image.png loading=lazy src=/notes/a_survey_for_light_field_super-resolution/image_7.png></p><h2 id=22-depth-independent-method>2.2. Depth-independent method<a hidden class=anchor aria-hidden=true href=#22-depth-independent-method>#</a></h2><p>Avoid explicit depth; rely on signal priors or learning-based angular patterns.</p><ul><li>CNN-based angular detail restoration (on EPIs)</li><li>Angular attention models to reconstruct views</li></ul><h1 id=3-light-field-spatial-angular-super-resolution-lfsasr>3. Light field spatial-angular super-resolution (LFSASR)<a hidden class=anchor aria-hidden=true href=#3-light-field-spatial-angular-super-resolution-lfsasr>#</a></h1><p><strong>Simultaneously</strong> increase both spatial and angular resolution → full 4D reconstruction.</p><h2 id=31-deep-learning-based-method>3.1. Deep learning-based method<a hidden class=anchor aria-hidden=true href=#31-deep-learning-based-method>#</a></h2><p>Jointly model 4D light field structure (geometry + appearance).</p><ul><li><strong>4D CNN encoder–decoder</strong></li><li><strong>Pseudo-4D convolution</strong> combining EPI + spatial-angular blocks</li><li><strong>Disentangled models</strong>: separate spatial and angular subspaces</li><li><strong>EPI-based networks (CNN + LSTM)</strong> preserving geometry</li><li><strong>Self-supervised or domain-generalized models</strong> for wild light fields</li></ul><p><img alt="3D encoder" loading=lazy src=/notes/a_survey_for_light_field_super-resolution/image_8.png></p><p>3D encoder</p><p><img alt="<strong>EPI-based networks</strong>" loading=lazy src=/notes/a_survey_for_light_field_super-resolution/image_9.png></p><p><strong>EPI-based networks</strong></p><h1 id=experiments>Experiments<a hidden class=anchor aria-hidden=true href=#experiments>#</a></h1><p><img alt=image.png loading=lazy src=/notes/a_survey_for_light_field_super-resolution/38bd0f5f-ebe8-44b8-b8bf-8bd68a63fbd8.png></p><p><img alt=image.png loading=lazy src=/notes/a_survey_for_light_field_super-resolution/image_10.png></p><p><img alt=image.png loading=lazy src=/notes/a_survey_for_light_field_super-resolution/image_11.png></p><hr><aside><h3 id=lfssr><strong>LFSSR</strong><a hidden class=anchor aria-hidden=true href=#lfssr>#</a></h3><ul><li><strong>CNN-based</strong>: Convolutions, 4D/sep-conv, optical-flow alignment, attention fusion.</li><li><strong>Transformer-based</strong>: Global spatial–angular modeling, EPI attention, multi-scale.</li></ul><h3 id=lfasr><strong>LFASR</strong><a hidden class=anchor aria-hidden=true href=#lfasr>#</a></h3><ul><li><strong>Depth-dependent</strong>: Estimate depth → warp → blend new views.</li><li><strong>Depth-independent</strong>: Fourier/shearlet priors, CNN restoration on EPI, angular attention.</li></ul><h3 id=lfsasr><strong>LFSASR</strong><a hidden class=anchor aria-hidden=true href=#lfsasr>#</a></h3><ul><li><strong>Deep learning-based</strong>: 4D CNNs, disentangled spatial–angular modeling, EPI networks, high-order residual networks.</li></ul></aside></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>