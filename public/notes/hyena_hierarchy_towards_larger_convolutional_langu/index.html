<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Hyena Hierarchy: Towards Larger Convolutional Language Models | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: Hyena Hierarchy"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/hyena_hierarchy_towards_larger_convolutional_langu/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/hyena_hierarchy_towards_larger_convolutional_langu/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/hyena_hierarchy_towards_larger_convolutional_langu/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Hyena Hierarchy: Towards Larger Convolutional Language Models"><meta property="og:description" content="Paper-reading notes: Hyena Hierarchy"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-11-18T07:39:29+00:00"><meta property="article:modified_time" content="2025-11-18T07:39:29+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/hyena_hierarchy_towards_larger_convolutional_langu/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/hyena_hierarchy_towards_larger_convolutional_langu/image_1.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/hyena_hierarchy_towards_larger_convolutional_langu/image.png"><meta name=twitter:title content="Hyena Hierarchy: Towards Larger Convolutional Language Models"><meta name=twitter:description content="Paper-reading notes: Hyena Hierarchy"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Hyena Hierarchy: Towards Larger Convolutional Language Models","item":"https://my-blog-alpha-vert.vercel.app/notes/hyena_hierarchy_towards_larger_convolutional_langu/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Hyena Hierarchy: Towards Larger Convolutional Language Models","name":"Hyena Hierarchy: Towards Larger Convolutional Language Models","description":"Paper-reading notes: Hyena Hierarchy","keywords":[],"articleBody":" Each recurrence step is like a different attention head, but instead of running fully in parallel (multi-head), Hyena stacks them sequentially (multi-step).\nReview Transformer Attention in a Transformer takes a sequence of tokens and, for each token, creates a new representation by mixing information from all tokens.\nEach token first computes its own query, while every token also has a key and a value. For a token i, attention computes similarity scores between its query $Q_i$ and all keys $K_j$, applies softmax to turn these scores into weights, and then forms the output $y_i$ as a weighted sum of all values $V_j$.\n1. Introduction Traditional attention is expressive but computationally expensive.\nHyena asks:\nCan we reproduce the essential capabilities of attention: global mixing + data control —without building a full N×N matrix?\nThe answer is yes, using two cheap primitives:\nLong implicit convolutions (for global token mixing) Element-wise gates (for input-dependent weighting) Attention action Hyena replacement Mixed all tokens according to distance \u0026 structure (global mixing) Toeplitz convolution Weighted tokens based on QK / input (data control) Gating (diagonal matrix) Hyena stacks these primitives in a recurrence to approximate the expressiveness of attention at lower cost.\n2. Method The paper presents Hyena, an attention-free building block that replaces Transformers’ attention mechanism using a recurrence of gating and implicitly-parameterized long convolutions.\nFactorizing Attention Into Cheap Components: Self-attention does:\n$$ y = softmax(QK^\\top) V $$\nHyena replaces the giant attention matrix with a product of Toeplitz (convolution) and diagonal (gating) matrices:\n$$ H(u) = D_x^N S_h^N \\cdots D_x^2 S_h^2 D_x^1 S_h^1 $$\nWhere:\nSₕⁿ — Toeplitz matrices implementing long convolution Dₓⁿ — diagonal matrices implementing input-controlled gating Hyena forward pass: z1 = v # “value” z(n+1) = x(n) ⊙ (h(n) ∗ z(n)) # conv → gate y = z(N+1)\nRepeating (conv ➜ gate) N times gives Hyena deep expressive power, similar to attention heads.\n3. Novelty A. Implicit Long Convolutions Hyena creates very long filters using a small FFN with positional encoding, instead of storing huge kernels. These filters capture long-range dependencies and are applied efficiently with FFT in O(N log N).\n$$ h^n(t) = Window(t) \\cdot FFN(PosEnc(t)) $$\nB. Data-Controlled Gating Each step computes a gating vector from the input (via a linear layer). This makes the mixing input-dependent, similar to how QKᵀ gives dynamic weights in attention.\n$$ v = W_v u \\ x^n = Linear_n(u) \\ = W_n u $$\nC. Recurrence Depth as Multi-Head Analogue Hyena stacks many (convolution → gate) blocks. Each block learns a different interaction pattern, like an attention head, but sequential and far cheaper.\n$$ S_h¹ → D_x¹ → S_h² → D_x² → … → S_hᴺ → D_xᴺ $$\nD. Fast FFT-Based Computation All long convolutions are executed using FFT → multiply → inverse FFT, avoiding large matrices and enabling efficient processing of very long sequences.\n$$ h^n * z^n $$\nE. Structured Matrix Factorization View Hyena effectively approximates an attention matrix by breaking it into many Toeplitz (convolution) and diagonal (gating) factors. This yields an expressive, attention-like operator with lower cost.\n$$ A(q,k) \\approx D_q S_\\psi D_k S_\\varphi $$\n","wordCount":"516","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/hyena_hierarchy_towards_larger_convolutional_langu/image.png","datePublished":"2025-11-18T07:39:29Z","dateModified":"2025-11-18T07:39:29Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/hyena_hierarchy_towards_larger_convolutional_langu/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Hyena Hierarchy: Towards Larger Convolutional Language Models</h1><div class=post-description>Paper-reading notes: Hyena Hierarchy</div><div class=post-meta><span title='2025-11-18 07:39:29 +0000 +0000'>November 18, 2025</span>&nbsp;·&nbsp;<span>516 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#review-transformer aria-label="Review Transformer">Review Transformer</a></li><li><a href=#1-introduction aria-label="1. Introduction">1. Introduction</a></li><li><a href=#2-method aria-label="2. Method">2. Method</a><ul><ul><li><a href=#factorizing-attention-into-cheap-components aria-label="Factorizing Attention Into Cheap Components:">Factorizing Attention Into Cheap Components:</a></li><li><a href=#hyena-forward-pass aria-label="Hyena forward pass:">Hyena forward pass:</a></li></ul></ul></li><li><a href=#3-novelty aria-label="3. Novelty">3. Novelty</a><ul><ul><li><a href=#a-implicit-long-convolutions aria-label="A. Implicit Long Convolutions">A. Implicit Long Convolutions</a></li><li><a href=#b-data-controlled-gating aria-label="B. Data-Controlled Gating">B. Data-Controlled Gating</a></li><li><a href=#c-recurrence-depth-as-multi-head-analogue aria-label="C. Recurrence Depth as Multi-Head Analogue">C. Recurrence Depth as Multi-Head Analogue</a></li><li><a href=#d-fast-fft-based-computation aria-label="D. Fast FFT-Based Computation">D. Fast FFT-Based Computation</a></li><li><a href=#e-structured-matrix-factorization-view aria-label="E. Structured Matrix Factorization View">E. Structured Matrix Factorization View</a></li></ul></ul></li></ul></div></details></div><div class=post-content><aside><p>Each recurrence step is like a different attention head, but instead of running fully in parallel (multi-head), Hyena stacks them <strong>sequentially (multi-step)</strong>.</p></aside><h1 id=review-transformer>Review Transformer<a hidden class=anchor aria-hidden=true href=#review-transformer>#</a></h1><p>Attention in a Transformer takes a sequence of tokens and, for each token, creates a new representation by mixing information from <strong>all</strong> tokens.</p><p>Each token first computes its own query, while every token also has a key and a value. For a token i, attention computes similarity scores between its query $Q_i$ and all keys $K_j$, applies softmax to turn these scores into weights, and then forms the output $y_i$ as a weighted sum of all values $V_j$.</p><h1 id=1-introduction><strong>1. Introduction</strong><a hidden class=anchor aria-hidden=true href=#1-introduction>#</a></h1><p>Traditional attention is expressive but computationally expensive.</p><p>Hyena asks:</p><blockquote><p>Can we reproduce the essential capabilities of attention: <strong>global mixing</strong> + <strong>data control</strong> —without building a full N×N matrix?</p></blockquote><p>The answer is <strong>yes</strong>, using two cheap primitives:</p><ol><li><strong>Long implicit convolutions</strong> (for global token mixing)</li><li><strong>Element-wise gates</strong> (for input-dependent weighting)</li></ol><table><thead><tr><th>Attention action</th><th>Hyena replacement</th></tr></thead><tbody><tr><td>Mixed all tokens according to distance & structure (<strong>global mixing</strong>)</td><td>Toeplitz convolution</td></tr><tr><td>Weighted tokens based on QK / input (<strong>data control</strong>)</td><td>Gating (diagonal matrix)</td></tr></tbody></table><p><strong>Hyena</strong> stacks these primitives in a recurrence to approximate the expressiveness of attention at lower cost.</p><h1 id=2-method><strong>2. Method</strong><a hidden class=anchor aria-hidden=true href=#2-method>#</a></h1><p>The paper presents <strong>Hyena</strong>, an attention-free building block that replaces Transformers’ attention mechanism using <strong>a recurrence of gating</strong> and <strong>implicitly-parameterized long convolutions</strong>.</p><hr><h3 id=factorizing-attention-into-cheap-components><strong>Factorizing Attention Into Cheap Components:</strong><a hidden class=anchor aria-hidden=true href=#factorizing-attention-into-cheap-components>#</a></h3><p>Self-attention does:</p><p>$$
y = softmax(QK^\top) V
$$</p><p>Hyena replaces the giant attention matrix with a product of <strong>Toeplitz (convolution) and diagonal (gating) matrices</strong>:</p><p>$$
H(u) = D_x^N S_h^N \cdots D_x^2 S_h^2 D_x^1 S_h^1
$$</p><p><img alt=image.png loading=lazy src=/notes/hyena_hierarchy_towards_larger_convolutional_langu/image.png></p><p>Where:</p><ul><li><strong>Sₕⁿ</strong> — Toeplitz matrices implementing <strong>long convolution</strong></li><li><strong>Dₓⁿ</strong> — diagonal matrices implementing <strong>input-controlled gating</strong></li></ul><h3 id=hyena-forward-pass>Hyena forward pass:<a hidden class=anchor aria-hidden=true href=#hyena-forward-pass>#</a></h3><p><img alt=image.png loading=lazy src=/notes/hyena_hierarchy_towards_larger_convolutional_langu/image_1.png></p><aside><p>z1 = v # &ldquo;value&rdquo;
z(n+1) = x(n) ⊙ (h(n) ∗ z(n)) # conv → gate
y = z(N+1)</p></aside><p>Repeating (conv ➜ gate) N times gives Hyena deep expressive power, similar to attention heads.</p><hr><h1 id=3-novelty><strong>3. Novelty</strong><a hidden class=anchor aria-hidden=true href=#3-novelty>#</a></h1><h3 id=a-implicit-long-convolutions><strong>A. Implicit Long Convolutions</strong><a hidden class=anchor aria-hidden=true href=#a-implicit-long-convolutions>#</a></h3><p>Hyena creates very long filters using a small <strong>FFN</strong> with positional encoding, instead of storing huge kernels. These filters capture long-range dependencies and are applied efficiently with FFT in <strong>O(N log N)</strong>.</p><p>$$
h^n(t) = Window(t) \cdot FFN(PosEnc(t))
$$</p><h3 id=b-data-controlled-gating><strong>B. Data-Controlled Gating</strong><a hidden class=anchor aria-hidden=true href=#b-data-controlled-gating>#</a></h3><p>Each step computes a gating vector from the input (via a linear layer). This makes the mixing <strong>input-dependent</strong>, similar to how QKᵀ gives dynamic weights in attention.</p><p>$$
v = W_v u \ x^n = Linear_n(u) \ = W_n u
$$</p><h3 id=c-recurrence-depth-as-multi-head-analogue><strong>C. Recurrence Depth as Multi-Head Analogue</strong><a hidden class=anchor aria-hidden=true href=#c-recurrence-depth-as-multi-head-analogue>#</a></h3><p>Hyena stacks many <strong>(convolution → gate)</strong> blocks. Each block learns a different interaction pattern, like an attention head, but sequential and far cheaper.</p><p>$$
S_h¹ → D_x¹ → S_h² → D_x² → … → S_hᴺ → D_xᴺ
$$</p><h3 id=d-fast-fft-based-computation><strong>D. Fast FFT-Based Computation</strong><a hidden class=anchor aria-hidden=true href=#d-fast-fft-based-computation>#</a></h3><p>All long convolutions are executed using <strong>FFT → multiply → inverse FFT</strong>, avoiding large matrices and enabling efficient processing of very long sequences.</p><p>$$
h^n * z^n
$$</p><h3 id=e-structured-matrix-factorization-view><strong>E. Structured Matrix Factorization View</strong><a hidden class=anchor aria-hidden=true href=#e-structured-matrix-factorization-view>#</a></h3><p>Hyena effectively approximates an attention matrix by breaking it into many Toeplitz (convolution) and diagonal (gating) factors. This yields an expressive, attention-like operator with lower cost.</p><p>$$
A(q,k) \approx D_q S_\psi D_k S_\varphi
$$</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>