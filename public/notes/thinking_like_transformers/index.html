<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Thinking Like Transformers | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: RASP"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/thinking_like_transformers/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/thinking_like_transformers/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/thinking_like_transformers/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Thinking Like Transformers"><meta property="og:description" content="Paper-reading notes: RASP"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-12-07T15:14:48+00:00"><meta property="article:modified_time" content="2025-12-07T15:14:48+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/selfile.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/selfile.png"><meta name=twitter:title content="Thinking Like Transformers"><meta name=twitter:description content="Paper-reading notes: RASP"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Thinking Like Transformers","item":"https://my-blog-alpha-vert.vercel.app/notes/thinking_like_transformers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Thinking Like Transformers","name":"Thinking Like Transformers","description":"Paper-reading notes: RASP","keywords":[],"articleBody":"1. What is RASP? A small, symbolic language that models how Transformers compute.\nDescribes sequence operations using simple functions instead of neural weights. 2. Core Components 2.1 s-ops (sequence operators) Functions that take a sequence and return another sequence of the same length.\nExamples:\ntokens → original input\nindices → [0,1,2,…]\nlength → broadcast length\nelementwise ops: +, ==, %, conditionals\n→ These mimic MLP/FFN behavior.\n2.2 select (symbolic attention) select(q, k, predicate)\nProduces an n×n boolean matrix → attention pattern. Tells which positions “attend” to which. 2.3 aggregate (value combination) aggregate(selector, values)\nFor each position: gather values from selected positions and average them. Symbolic version of attention-value combination. 2.4 selector_width Counts how many positions were selected for each token. Used for counting / histogram tasks. 3. What RASP Can Express RASP programs can represent many Transformer-computable tasks:\nHistogram Double histogram Sequence reversal Matching parentheses Dyck languages Filtering, counting, boolean logic over sequences Shows that Transformers can perform structured, compositional operations.\n4. How RASP Models Transformers Elementwise s-ops → Transformer MLP select + aggregate → Self-attention No loops → fixed-depth computation, like real Transformers Models information flow limits: only attention can move information across tokens. 5. Examples Reverse sequence flip = select(indices, length - indices - 1, ==) reverse = aggregate(flip, tokens)\nHistogram same = select(tokens, tokens, ==) hist = selector_width(same)\n6. Compilation RASP programs can be compiled into real Transformer architectures:\nnumber of layers number of heads attention patterns Enables empirical testing of symbolic algorithms.\n7. Key Insights RASP provides a clean, understandable model of Transformer computation. It highlights what Transformers can and cannot compute. It acts like pseudocode for attention-based algorithms.\n","wordCount":"273","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/selfile.png","datePublished":"2025-12-07T15:14:48Z","dateModified":"2025-12-07T15:14:48Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/thinking_like_transformers/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Thinking Like Transformers</h1><div class=post-description>Paper-reading notes: RASP</div><div class=post-meta><span title='2025-12-07 15:14:48 +0000 +0000'>December 7, 2025</span>&nbsp;·&nbsp;<span>273 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-what-is-rasp aria-label="1. What is RASP?">1. What is RASP?</a></li><li><a href=#2-core-components aria-label="2. Core Components">2. Core Components</a><ul><li><a href=#21-s-ops-sequence-operators aria-label="2.1 s-ops (sequence operators)">2.1 s-ops (sequence operators)</a></li><li><a href=#22-select-symbolic-attention aria-label="2.2 select (symbolic attention)">2.2 select (symbolic attention)</a></li><li><a href=#23-aggregate-value-combination aria-label="2.3 aggregate (value combination)">2.3 aggregate (value combination)</a></li><li><a href=#24-selector_ aria-label="2.4 selector_width">2.4 selector_width</a></li></ul></li><li><a href=#3-what-rasp-can-express aria-label="3. What RASP Can Express">3. What RASP Can Express</a></li><li><a href=#4-how-rasp-models-transformers aria-label="4. How RASP Models Transformers">4. How RASP Models Transformers</a></li><li><a href=#5-examples aria-label="5. Examples">5. Examples</a><ul><li><a href=#reverse-sequence aria-label="Reverse sequence">Reverse sequence</a></li><li><a href=#histogram aria-label=Histogram>Histogram</a></li></ul></li><li><a href=#6-compilation aria-label="6. Compilation">6. Compilation</a></li><li><a href=#7-key-insights aria-label="7. Key Insights">7. Key Insights</a></li></ul></div></details></div><div class=post-content><h2 id=1-what-is-rasp><strong>1. What is RASP?</strong><a hidden class=anchor aria-hidden=true href=#1-what-is-rasp>#</a></h2><p>A small, symbolic language that models how Transformers compute.</p><ul><li>Describes <strong>sequence operations</strong> using simple functions instead of neural weights.</li></ul><h2 id=2-core-components><strong>2. Core Components</strong><a hidden class=anchor aria-hidden=true href=#2-core-components>#</a></h2><h3 id=21-s-ops-sequence-operators><strong>2.1 s-ops (sequence operators)</strong><a hidden class=anchor aria-hidden=true href=#21-s-ops-sequence-operators>#</a></h3><p>Functions that take a sequence and return another sequence of the same length.</p><p>Examples:</p><ul><li><p><code>tokens</code> → original input</p></li><li><p><code>indices</code> → [0,1,2,&mldr;]</p></li><li><p><code>length</code> → broadcast length</p></li><li><p>elementwise ops: +, ==, %, conditionals</p><p>→ These mimic <strong>MLP/FFN</strong> behavior.</p></li></ul><h3 id=22-select-symbolic-attention><strong>2.2 select (symbolic attention)</strong><a hidden class=anchor aria-hidden=true href=#22-select-symbolic-attention>#</a></h3><aside><p>select(q, k, predicate)</p></aside><ul><li>Produces an n×n boolean matrix → attention pattern.</li><li>Tells which positions “attend” to which.</li></ul><h3 id=23-aggregate-value-combination><strong>2.3 aggregate (value combination)</strong><a hidden class=anchor aria-hidden=true href=#23-aggregate-value-combination>#</a></h3><aside><p>aggregate(selector, values)</p></aside><ul><li>For each position: gather values from selected positions and average them.</li><li>Symbolic version of <strong>attention-value combination</strong>.</li></ul><h3 id=24-selector_><strong>2.4 selector_width</strong><a hidden class=anchor aria-hidden=true href=#24-selector_>#</a></h3><ul><li>Counts how many positions were selected for each token.</li><li>Used for <strong>counting / histogram</strong> tasks.</li></ul><h2 id=3-what-rasp-can-express><strong>3. What RASP Can Express</strong><a hidden class=anchor aria-hidden=true href=#3-what-rasp-can-express>#</a></h2><p>RASP programs can represent many Transformer-computable tasks:</p><ul><li>Histogram</li><li>Double histogram</li><li>Sequence reversal</li><li>Matching parentheses</li><li>Dyck languages</li><li>Filtering, counting, boolean logic over sequences</li></ul><p>Shows that Transformers can perform structured, compositional operations.</p><h2 id=4-how-rasp-models-transformers><strong>4. How RASP Models Transformers</strong><a hidden class=anchor aria-hidden=true href=#4-how-rasp-models-transformers>#</a></h2><ul><li><strong>Elementwise s-ops → Transformer MLP</strong></li><li><strong>select + aggregate → Self-attention</strong></li><li><strong>No loops → fixed-depth computation</strong>, like real Transformers</li><li>Models information flow limits: only attention can move information across tokens.</li></ul><h2 id=5-examples><strong>5. Examples</strong><a hidden class=anchor aria-hidden=true href=#5-examples>#</a></h2><h3 id=reverse-sequence><strong>Reverse sequence</strong><a hidden class=anchor aria-hidden=true href=#reverse-sequence>#</a></h3><aside><p>flip = select(indices, length - indices - 1, ==)
reverse = aggregate(flip, tokens)</p></aside><h3 id=histogram><strong>Histogram</strong><a hidden class=anchor aria-hidden=true href=#histogram>#</a></h3><aside><p>same = select(tokens, tokens, ==)
hist = selector_width(same)</p></aside><h2 id=6-compilation><strong>6. Compilation</strong><a hidden class=anchor aria-hidden=true href=#6-compilation>#</a></h2><p>RASP programs can be compiled into real Transformer architectures:</p><ul><li>number of layers</li><li>number of heads</li><li>attention patterns</li></ul><p>Enables empirical testing of symbolic algorithms.</p><h2 id=7-key-insights><strong>7. Key Insights</strong><a hidden class=anchor aria-hidden=true href=#7-key-insights>#</a></h2><p>RASP provides a <strong>clean, understandable model</strong> of Transformer computation. It highlights what Transformers can and cannot compute. It acts like <strong>pseudocode for attention-based algorithms</strong>.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>