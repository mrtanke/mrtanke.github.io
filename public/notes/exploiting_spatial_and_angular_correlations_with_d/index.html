<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Exploiting Spatial and Angular Correlations With Deep Efficient Transformers for Light Field Image Super-Resolution | Home</title><meta name=keywords content><meta name=description content="Paper-reading notes: LF-DET"><meta name=author content><link rel=canonical href=https://my-blog-alpha-vert.vercel.app/notes/exploiting_spatial_and_angular_correlations_with_d/><link crossorigin=anonymous href=/assets/css/stylesheet.9bb7abd44394c0e0d2cecd8ad4322626054cd3f5709a6d890d5a408efaf1fa90.css integrity="sha256-m7er1EOUwODSzs2K1DImJgVM0/Vwmm2JDVpAjvrx+pA=" rel="preload stylesheet" as=style><link rel=icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=16x16 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=icon type=image/png sizes=32x32 href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=apple-touch-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><link rel=mask-icon href=https://my-blog-alpha-vert.vercel.app/selfile.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://my-blog-alpha-vert.vercel.app/notes/exploiting_spatial_and_angular_correlations_with_d/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><style>ul#menu .tag-button{background:0 0;border:none;padding:0;margin:0;font-weight:500;color:inherit;cursor:pointer;text-decoration:none;opacity:.95;display:inline-block}ul#menu .tag-button:hover{text-decoration:underline}ul#menu .tag-button.active{text-decoration:underline;font-weight:600}body.is-home .post-entry{display:none}body.is-home .page-footer .pagination{display:none}</style><script>(function(){function e(){const e=location.pathname.replace(/\/+/g,"/");return e==="/"||e===""||e==="/index.html"}e()&&document.body.classList.add("is-home"),document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("menu");if(!e)return;if(e.querySelector(".tag-button"))return;const n=["Notes","Thoughts","Projects"],t={Notes:"/notes/",Thoughts:"/thoughts/",Projects:"/projects/"};n.forEach(n=>{const o=document.createElement("li"),s=document.createElement("a");s.className="tag-button",s.href=t[n],s.textContent=n,location.pathname.toLowerCase().startsWith(t[n])&&s.classList.add("active"),o.appendChild(s),e.appendChild(o)})})})()</script><meta property="og:url" content="https://my-blog-alpha-vert.vercel.app/notes/exploiting_spatial_and_angular_correlations_with_d/"><meta property="og:site_name" content="Home"><meta property="og:title" content="Exploiting Spatial and Angular Correlations With Deep Efficient Transformers for Light Field Image Super-Resolution"><meta property="og:description" content="Paper-reading notes: LF-DET"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-11-10T13:22:13+00:00"><meta property="article:modified_time" content="2025-11-10T13:22:13+00:00"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/exploiting_spatial_and_angular_correlations_with_d/image.png"><meta property="og:image" content="https://my-blog-alpha-vert.vercel.app/notes/exploiting_spatial_and_angular_correlations_with_d/image_1.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-alpha-vert.vercel.app/notes/exploiting_spatial_and_angular_correlations_with_d/image.png"><meta name=twitter:title content="Exploiting Spatial and Angular Correlations With Deep Efficient Transformers for Light Field Image Super-Resolution"><meta name=twitter:description content="Paper-reading notes: LF-DET"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://my-blog-alpha-vert.vercel.app/notes/"},{"@type":"ListItem","position":2,"name":"Exploiting Spatial and Angular Correlations With Deep Efficient Transformers for Light Field Image Super-Resolution","item":"https://my-blog-alpha-vert.vercel.app/notes/exploiting_spatial_and_angular_correlations_with_d/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Exploiting Spatial and Angular Correlations With Deep Efficient Transformers for Light Field Image Super-Resolution","name":"Exploiting Spatial and Angular Correlations With Deep Efficient Transformers for Light Field Image Super-Resolution","description":"Paper-reading notes: LF-DET","keywords":[],"articleBody":"Introduction An LF records both intensity information and directional information of all light rays. The currently popular representation for four-dimensional(4D) LF is the two-plane parametrization.\nSince the resolution of the device sensor is fixed, there is a trade-off between spatial and angular resolution. As a result, LF spatial super-resolution (LF-SSR) which aims to reconstruct high-resolution (HR) LF from its LR counterpart has attracted lots of attention.\nTo achieve high-quality LF-SSR performance, the key is to make full use of angular information which is not available in SISR.\nFor the CNNs, the local nature of convolutional filters raises a fundamental limitation in accessing global dependency. For the transformer, it provides global context modeling by self-attention mechanism, but the global interaction comes at a quadratic computational complexity cost. In addition, vision transformers are widely used to process 2D images rather than the complex 4D LFs. More efforts are needed to exploit an effective way to implement fully attentive features by transformers.\nThe paper proposes a new transformer-based network called LF-DET for light field spatial super-resolution. It first extracts local information from each sub-aperture image using convolution layers, then applies a spatial–angular separable transformer to model global context along both spatial and angular dimensions. The sub-sampling spatial modeling reduces computation cost, while the multi-scale angular modeling handles different disparity ranges effectively. Finally, hierarchical features from multiple transformer encoders are fused for high-quality reconstruction. LF-DET provides a flexible balance between model size, accuracy, and efficiency, and experiments show it achieves superior performance over existing methods.\nMethod Spatial–Angular Separable Transformer Encoder A standard transformer handles 1D token sequences, but a light field (LF) image is 4D $H \\times W \\times U \\times V$. Flattening it into 1D causes high computational and memory costs. The vanilla transformer cannot efficiently process such large data. To solve this, a spatial–angular separable transformer encoder is designed. It includes two main parts: sub-sampling spatial modeling and multi-scale angular modeling.\nA. Sub-Sampling Spatial Modeling High-resolution SAIs lead to heavy computation and memory use. To handle this, a sub-sampling convolution is used before the multi-head self-attention (MSA) to reduce token number.\nGiven LF features $F_a \\in R^{C \\times H \\times W}$, we reshape them to embeddings $x_a \\in R^{(HW) \\times C}$.\nThe queries, keys, and values are calculated as:\n$$ Q_a = x_a W_Q, \\quad K_a = x_{a_{sub}} W_K, \\quad V_a = x_{a_{sub}} W_V $$\nwhere $W_Q, W_K, W_V \\in R^{C \\times C}$.\nThe self-attention for the (p)-th subspace is:\n$$ SA_p(Q_{a,p}, K_{a,p}, V_{a,p}) = softmax \\left( \\frac{Q_{a,p}(K_{a,p})^T}{\\sqrt{C/P}} \\right)V_{a,p} $$\nAfter concatenating all subspaces, the result is:\n$$ \\tilde{x}_a = [SA_1, SA_2, \\ldots, SA_P]W_P + x_a $$\nwhere $W_P \\in R^{C \\times C}$. This sub-sampling strategy reduces computation by a ratio of $S^2$.\nSince attention ignores token order, a 3×3 convolution is added in FFN to insert positional encoding and local information:\n$$ x_a = MLP(GELU(DWConv(MLP(\\tilde{x}_a)))) + \\tilde{x}_a $$\nwhere DWConv is depth-wise convolution and GELU is the activation. This introduces spatial locality to the transformer.\nFinally, $x_a \\in R^{(HW) \\times C}$ represents the spatial transformer output, and $K$ such transformers are cascaded to enhance global spatial features.\nB. Multi-Scale Angular Modeling To model angular information, angular transformers process macro-pixels. Since objects with different disparities are misaligned across views, the network analyzes different macro-pixel scales.\nFor a downsampling factor $\\alpha$, the valid disparity range in a region of $M \\times M$ macro-pixels is $[-\\alpha M, \\alpha M]$. Larger $M$ captures larger disparities but increases cost and mismatch, so three scales (M=1,2,3) are used. Each $M \\times M$ macro-pixel group is reshaped into sequences $x_s^M \\in R^{(M^2UV) \\times C}$.\nThe attention mechanism is similar to the spatial transformer:\n$$ Q_s^M = x_s^M W_Q, \\quad K_s^M = x_s^M W_K, \\quad V_s^M = x_s^M W_V $$\n$$ SA_p(Q_{s,p}^M, K_{s,p}^M, V_{s,p}^M) = softmax \\left( \\frac{Q_{s,p}^M (K_{s,p}^M)^T}{\\sqrt{C/P}} \\right)V_{s,p}^M $$\n$$ \\tilde{x}_s^M = [SA_1, SA_2, \\ldots, SA_P]W_P + x_s^M $$\nAfter FFN with DWConv, the final angular feature is:\n$$ x_s^M = MLP(GELU(DWConv(MLP(\\tilde{x}_s^M)))) + \\tilde{x}s^M $$\nEach scale outputs a feature map $F_{MacPI}^M \\in R^{C \\times H_U \\times W_V}$ focusing on a specific disparity range.\nC. Spatial Attention Fusion The three angular feature maps are concatenated and passed through a 1×1 convolution and softmax to compute attention weights:\n$$ W = softmax(H_{1\\times1}[F_{MacPI}^1, F_{MacPI}^2, F_{MacPI}^3]) $$\nThe weights are split and applied to each feature map, producing the fused output:\n$$ \\overline{F}{MacPI} = \\sum{M=1}^3 W^M F_{MacPI}^M $$\nThis adaptive fusion enables the model to handle both small and large disparity regions effectively.\nNetwork Architecture LF-DET is a transformer-based network for light field super-resolution. The input light field $L^{LR} \\in R^{U \\times V \\times H \\times W}$ is converted from RGB to YCbCr, and only the Y channel is used. The goal is to reconstruct a high-resolution output $L^{HR} \\in R^{U \\times V \\times \\alpha H \\times \\alpha W}$, where $\\alpha$ is the upsampling factor. The Cb and Cr channels are upsampled by bicubic interpolation.\nThe network has an encoder–decoder structure. The encoder extracts local and global features, and the decoder fuses them through a Hierarchical Feature Aggregation (HFA) module. The final upsampling block generates the high-resolution light field.\nA. Local Feature Extraction Local context is important for super-resolution. The vanilla transformer cannot model local dependencies well. To fix this, LF-DET uses several 3×3 convolution layers with Leaky ReLU activations.\nEach SAI is processed by one convolution and three stacked convolutions, followed by a residual connection. This produces the initial local features $F_a^0 \\in R^{C \\times H \\times W}$. The same convolution weights are shared across all views.\nB. Global Feature Extraction Global spatial and angular information is modeled using spatial–angular separable transformer encoders. Each encoder first performs sub-sampling spatial modeling, then reshapes the data to the MacPI pattern for angular modeling.\nThe process for the (n)-th encoder is:\n$$ F_a^n = R_{LF_2}(H_{MAM}^n(R_{LF_1}(H_{SSM}^n(F_a^{n-1})))) $$\nThese encoders are stacked sequentially to extract deep and global correlations across views.\nC. Hierarchical Feature Aggregation \u0026 Upsampling The HFA module merges features from all encoder layers. Shallow and deep features are fused by element-wise addition and refined by three 3×3 convolution layers.\nThe overall fusion is:\n$$ \\overline{F_a} = [H_{FE}^1 \\sum_{i=1}^N F_a^i, H_{FE}^2 \\sum_{i=2}^N F_a^i, \\ldots, H_{FE}^N \\sum_{i=N}^N F_a^i] $$\nThen, the fused feature $\\overline{F}_a$ goes through an upsampling block with two convolutions, a pixel-shuffle layer, and Leaky ReLU activation. The pixel-shuffle enlarges the resolution from $H \\times W$ to $\\alpha H \\times \\alpha W$, and the final 3×3 convolution produces the output Y-channel image.\n","wordCount":"1071","inLanguage":"en","image":"https://my-blog-alpha-vert.vercel.app/notes/exploiting_spatial_and_angular_correlations_with_d/image.png","datePublished":"2025-11-10T13:22:13Z","dateModified":"2025-11-10T13:22:13Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://my-blog-alpha-vert.vercel.app/notes/exploiting_spatial_and_angular_correlations_with_d/"},"publisher":{"@type":"Organization","name":"Home","logo":{"@type":"ImageObject","url":"https://my-blog-alpha-vert.vercel.app/selfile.png"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://my-blog-alpha-vert.vercel.app/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/notes/><span class=active>Notes</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/thoughts/><span>Thoughts</span></a></li><li><a class=tag-button href=https://my-blog-alpha-vert.vercel.app/projects/><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://my-blog-alpha-vert.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Exploiting Spatial and Angular Correlations With Deep Efficient Transformers for Light Field Image Super-Resolution</h1><div class=post-description>Paper-reading notes: LF-DET</div><div class=post-meta><span title='2025-11-10 13:22:13 +0000 +0000'>November 10, 2025</span>&nbsp;·&nbsp;<span>1071 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#method aria-label=Method>Method</a><ul><li><a href=#spatialangular-separable-transformer-encoder aria-label="Spatial–Angular Separable Transformer Encoder">Spatial–Angular Separable Transformer Encoder</a><ul><li><a href=#a-sub-sampling-spatial-modeling aria-label="A. Sub-Sampling Spatial Modeling">A. Sub-Sampling Spatial Modeling</a></li><li><a href=#b-multi-scale-angular-modeling aria-label="B. Multi-Scale Angular Modeling">B. Multi-Scale Angular Modeling</a></li><li><a href=#c-spatial-attention-fusion aria-label="C. Spatial Attention Fusion">C. Spatial Attention Fusion</a></li></ul></li><li><a href=#network-architecture aria-label="Network Architecture">Network Architecture</a><ul><li><a href=#a-local-feature-extraction aria-label="A. Local Feature Extraction">A. Local Feature Extraction</a></li><li><a href=#b-global-feature-extraction aria-label="B. Global Feature Extraction">B. Global Feature Extraction</a></li><li><a href=#c-hierarchical-feature-aggregation--upsampling aria-label="C. Hierarchical Feature Aggregation & Upsampling">C. Hierarchical Feature Aggregation & Upsampling</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>An LF records both <strong>intensity information</strong> and <strong>directional information of all light rays</strong>. The currently popular representation for <strong>four-dimensional(4D)</strong> LF is the two-plane parametrization.</p><p>Since the resolution of the device sensor is fixed, there is a trade-off between spatial and angular resolution. As a result, <strong>LF spatial super-resolution (LF-SSR)</strong> which aims to reconstruct high-resolution (HR) LF from its LR counterpart has attracted lots of attention.</p><p>To achieve high-quality LF-SSR performance, the key is to make full use of <strong>angular information</strong> which is not available in SISR.</p><p>For the CNNs, the local nature of convolutional filters raises a fundamental limitation in accessing <strong>global dependency</strong>. For the transformer, it provides global context modeling by self-attention mechanism, but the global interaction comes at a quadratic computational complexity cost. In addition, vision transformers are widely used to process 2D images rather than the complex 4D LFs. More efforts are needed to exploit an effective way to implement fully attentive features by transformers.</p><p>The paper proposes a new transformer-based network called <strong>LF-DET</strong> for light field spatial super-resolution. It first extracts local information from each sub-aperture image using convolution layers, then applies a <strong>spatial–angular separable transformer</strong> to model global context along both spatial and angular dimensions. The <strong>sub-sampling spatial modeling</strong> reduces computation cost, while the <strong>multi-scale angular modeling</strong> handles different disparity ranges effectively. Finally, hierarchical features from multiple transformer encoders are fused for high-quality reconstruction. LF-DET provides a flexible balance between model size, accuracy, and efficiency, and experiments show it achieves superior performance over existing methods.</p><h1 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h1><h2 id=spatialangular-separable-transformer-encoder>Spatial–Angular Separable Transformer Encoder<a hidden class=anchor aria-hidden=true href=#spatialangular-separable-transformer-encoder>#</a></h2><p>A standard transformer handles 1D token sequences, but a light field (LF) image is 4D $H \times W \times U \times V$. Flattening it into 1D causes high computational and memory costs. The <strong>vanilla transformer</strong> cannot efficiently process such large data. To solve this, a <strong>spatial–angular separable transformer encoder</strong> is designed. It includes two main parts: <strong>sub-sampling spatial modeling</strong> and <strong>multi-scale angular modeling</strong>.</p><p><img alt=image.png loading=lazy src=/notes/exploiting_spatial_and_angular_correlations_with_d/image.png></p><h3 id=a-sub-sampling-spatial-modeling>A. Sub-Sampling Spatial Modeling<a hidden class=anchor aria-hidden=true href=#a-sub-sampling-spatial-modeling>#</a></h3><p>High-resolution SAIs lead to heavy computation and memory use. To handle this, a <strong>sub-sampling convolution</strong> is used before the multi-head self-attention (MSA) to reduce token number.</p><p>Given LF features $F_a \in R^{C \times H \times W}$, we reshape them to embeddings $x_a \in R^{(HW) \times C}$.</p><p>The queries, keys, and values are calculated as:</p><p>$$
Q_a = x_a W_Q, \quad K_a = x_{a_{sub}} W_K, \quad V_a = x_{a_{sub}} W_V
$$</p><p>where $W_Q, W_K, W_V \in R^{C \times C}$.</p><p>The self-attention for the (p)-th subspace is:</p><p>$$
SA_p(Q_{a,p}, K_{a,p}, V_{a,p}) = softmax \left( \frac{Q_{a,p}(K_{a,p})^T}{\sqrt{C/P}} \right)V_{a,p}
$$</p><p>After concatenating all subspaces, the result is:</p><p>$$
\tilde{x}_a = [SA_1, SA_2, \ldots, SA_P]W_P + x_a
$$</p><p>where $W_P \in R^{C \times C}$. This sub-sampling strategy reduces computation by a ratio of $S^2$.</p><p>Since attention ignores token order, a 3×3 convolution is added in FFN to insert positional encoding and local information:</p><p>$$
x_a = MLP(GELU(DWConv(MLP(\tilde{x}_a)))) + \tilde{x}_a
$$</p><p>where <strong>DWConv</strong> is depth-wise convolution and <strong>GELU</strong> is the activation. This introduces spatial locality to the transformer.</p><p>Finally, $x_a \in R^{(HW) \times C}$ represents the spatial transformer output, and $K$ such transformers are cascaded to enhance global spatial features.</p><h3 id=b-multi-scale-angular-modeling>B. Multi-Scale Angular Modeling<a hidden class=anchor aria-hidden=true href=#b-multi-scale-angular-modeling>#</a></h3><p>To model angular information, <strong>angular transformers</strong> process macro-pixels. Since objects with different disparities are misaligned across views, the network analyzes different macro-pixel scales.</p><p>For a downsampling factor $\alpha$, the valid disparity range in a region of $M \times M$ macro-pixels is $[-\alpha M, \alpha M]$. Larger $M$ captures larger disparities but increases cost and mismatch, so three scales (M=1,2,3) are used. Each $M \times M$ macro-pixel group is reshaped into sequences $x_s^M \in R^{(M^2UV) \times C}$.</p><p>The attention mechanism is similar to the spatial transformer:</p><p>$$
Q_s^M = x_s^M W_Q, \quad K_s^M = x_s^M W_K, \quad V_s^M = x_s^M W_V
$$</p><p>$$
SA_p(Q_{s,p}^M, K_{s,p}^M, V_{s,p}^M) = softmax \left( \frac{Q_{s,p}^M (K_{s,p}^M)^T}{\sqrt{C/P}} \right)V_{s,p}^M
$$</p><p>$$
\tilde{x}_s^M = [SA_1, SA_2, \ldots, SA_P]W_P + x_s^M
$$</p><p>After FFN with DWConv, the final angular feature is:</p><p>$$
x_s^M = MLP(GELU(DWConv(MLP(\tilde{x}_s^M)))) + \tilde{x}s^M
$$</p><p>Each scale outputs a feature map $F_{MacPI}^M \in R^{C \times H_U \times W_V}$ focusing on a specific disparity range.</p><h3 id=c-spatial-attention-fusion>C. Spatial Attention Fusion<a hidden class=anchor aria-hidden=true href=#c-spatial-attention-fusion>#</a></h3><p>The three angular feature maps are concatenated and passed through a 1×1 convolution and softmax to compute attention weights:</p><p>$$
W = softmax(H_{1\times1}[F_{MacPI}^1, F_{MacPI}^2, F_{MacPI}^3])
$$</p><p>The weights are split and applied to each feature map, producing the fused output:</p><p>$$
\overline{F}<em>{MacPI} = \sum</em>{M=1}^3 W^M F_{MacPI}^M
$$</p><p>This adaptive fusion enables the model to handle both small and large disparity regions effectively.</p><hr><h2 id=network-architecture>Network Architecture<a hidden class=anchor aria-hidden=true href=#network-architecture>#</a></h2><p>LF-DET is a transformer-based network for light field super-resolution. The input light field $L^{LR} \in R^{U \times V \times H \times W}$ is converted from RGB to YCbCr, and only the Y channel is used. The goal is to reconstruct a high-resolution output $L^{HR} \in R^{U \times V \times \alpha H \times \alpha W}$, where $\alpha$ is the upsampling factor. The Cb and Cr channels are upsampled by bicubic interpolation.</p><p>The network has an <strong>encoder–decoder</strong> structure. The encoder extracts local and global features, and the decoder fuses them through a <strong>Hierarchical Feature Aggregation (HFA)</strong> module. The final upsampling block generates the high-resolution light field.</p><p><img alt=image.png loading=lazy src=/notes/exploiting_spatial_and_angular_correlations_with_d/image_1.png></p><h3 id=a-local-feature-extraction>A. Local Feature Extraction<a hidden class=anchor aria-hidden=true href=#a-local-feature-extraction>#</a></h3><p>Local context is important for super-resolution. The vanilla transformer cannot model local dependencies well. To fix this, LF-DET uses several <strong>3×3 convolution layers</strong> with Leaky ReLU activations.</p><p>Each SAI is processed by one convolution and three stacked convolutions, followed by a residual connection. This produces the initial local features $F_a^0 \in R^{C \times H \times W}$. The same convolution weights are shared across all views.</p><h3 id=b-global-feature-extraction>B. Global Feature Extraction<a hidden class=anchor aria-hidden=true href=#b-global-feature-extraction>#</a></h3><p>Global spatial and angular information is modeled using <strong>spatial–angular separable transformer encoders</strong>. Each encoder first performs sub-sampling spatial modeling, then reshapes the data to the MacPI pattern for angular modeling.</p><p>The process for the (n)-th encoder is:</p><p>$$
F_a^n = R_{LF_2}(H_{MAM}^n(R_{LF_1}(H_{SSM}^n(F_a^{n-1}))))
$$</p><p>These encoders are stacked sequentially to extract deep and global correlations across views.</p><h3 id=c-hierarchical-feature-aggregation--upsampling>C. Hierarchical Feature Aggregation & Upsampling<a hidden class=anchor aria-hidden=true href=#c-hierarchical-feature-aggregation--upsampling>#</a></h3><p>The <strong>HFA module</strong> merges features from all encoder layers. Shallow and deep features are fused by element-wise addition and refined by three <strong>3×3 convolution layers</strong>.</p><p>The overall fusion is:</p><p>$$
\overline{F_a} = [H_{FE}^1 \sum_{i=1}^N F_a^i, H_{FE}^2 \sum_{i=2}^N F_a^i, \ldots, H_{FE}^N \sum_{i=N}^N F_a^i]
$$</p><p>Then, the fused feature $\overline{F}_a$ goes through an upsampling block with two convolutions, a pixel-shuffle layer, and Leaky ReLU activation. The pixel-shuffle enlarges the resolution from $H \times W$ to $\alpha H \times \alpha W$, and the final <strong>3×3 convolution</strong> produces the output Y-channel image.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>